{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "92ef316a-29b9-4762-8cfd-e3e4a4e2501c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e72df6fb-603c-4360-b326-6cebb22b7518",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date/Time</th>\n",
       "      <th>Temp_C</th>\n",
       "      <th>Dew Point Temp_C</th>\n",
       "      <th>Rel Hum_%</th>\n",
       "      <th>Wind Speed_km/h</th>\n",
       "      <th>Visibility_km</th>\n",
       "      <th>Press_kPa</th>\n",
       "      <th>Weather</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1/1/2012 0:00</td>\n",
       "      <td>-1.8</td>\n",
       "      <td>-3.9</td>\n",
       "      <td>86</td>\n",
       "      <td>4</td>\n",
       "      <td>8.0</td>\n",
       "      <td>101.24</td>\n",
       "      <td>Fog</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1/1/2012 1:00</td>\n",
       "      <td>-1.8</td>\n",
       "      <td>-3.7</td>\n",
       "      <td>87</td>\n",
       "      <td>4</td>\n",
       "      <td>8.0</td>\n",
       "      <td>101.24</td>\n",
       "      <td>Fog</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1/1/2012 2:00</td>\n",
       "      <td>-1.8</td>\n",
       "      <td>-3.4</td>\n",
       "      <td>89</td>\n",
       "      <td>7</td>\n",
       "      <td>4.0</td>\n",
       "      <td>101.26</td>\n",
       "      <td>Freezing Drizzle,Fog</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1/1/2012 3:00</td>\n",
       "      <td>-1.5</td>\n",
       "      <td>-3.2</td>\n",
       "      <td>88</td>\n",
       "      <td>6</td>\n",
       "      <td>4.0</td>\n",
       "      <td>101.27</td>\n",
       "      <td>Freezing Drizzle,Fog</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1/1/2012 4:00</td>\n",
       "      <td>-1.5</td>\n",
       "      <td>-3.3</td>\n",
       "      <td>88</td>\n",
       "      <td>7</td>\n",
       "      <td>4.8</td>\n",
       "      <td>101.23</td>\n",
       "      <td>Fog</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8779</th>\n",
       "      <td>12/31/2012 19:00</td>\n",
       "      <td>0.1</td>\n",
       "      <td>-2.7</td>\n",
       "      <td>81</td>\n",
       "      <td>30</td>\n",
       "      <td>9.7</td>\n",
       "      <td>100.13</td>\n",
       "      <td>Snow</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8780</th>\n",
       "      <td>12/31/2012 20:00</td>\n",
       "      <td>0.2</td>\n",
       "      <td>-2.4</td>\n",
       "      <td>83</td>\n",
       "      <td>24</td>\n",
       "      <td>9.7</td>\n",
       "      <td>100.03</td>\n",
       "      <td>Snow</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8781</th>\n",
       "      <td>12/31/2012 21:00</td>\n",
       "      <td>-0.5</td>\n",
       "      <td>-1.5</td>\n",
       "      <td>93</td>\n",
       "      <td>28</td>\n",
       "      <td>4.8</td>\n",
       "      <td>99.95</td>\n",
       "      <td>Snow</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8782</th>\n",
       "      <td>12/31/2012 22:00</td>\n",
       "      <td>-0.2</td>\n",
       "      <td>-1.8</td>\n",
       "      <td>89</td>\n",
       "      <td>28</td>\n",
       "      <td>9.7</td>\n",
       "      <td>99.91</td>\n",
       "      <td>Snow</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8783</th>\n",
       "      <td>12/31/2012 23:00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-2.1</td>\n",
       "      <td>86</td>\n",
       "      <td>30</td>\n",
       "      <td>11.3</td>\n",
       "      <td>99.89</td>\n",
       "      <td>Snow</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8784 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             Date/Time  Temp_C  Dew Point Temp_C  Rel Hum_%  Wind Speed_km/h  \\\n",
       "0        1/1/2012 0:00    -1.8              -3.9         86                4   \n",
       "1        1/1/2012 1:00    -1.8              -3.7         87                4   \n",
       "2        1/1/2012 2:00    -1.8              -3.4         89                7   \n",
       "3        1/1/2012 3:00    -1.5              -3.2         88                6   \n",
       "4        1/1/2012 4:00    -1.5              -3.3         88                7   \n",
       "...                ...     ...               ...        ...              ...   \n",
       "8779  12/31/2012 19:00     0.1              -2.7         81               30   \n",
       "8780  12/31/2012 20:00     0.2              -2.4         83               24   \n",
       "8781  12/31/2012 21:00    -0.5              -1.5         93               28   \n",
       "8782  12/31/2012 22:00    -0.2              -1.8         89               28   \n",
       "8783  12/31/2012 23:00     0.0              -2.1         86               30   \n",
       "\n",
       "      Visibility_km  Press_kPa               Weather  \n",
       "0               8.0     101.24                   Fog  \n",
       "1               8.0     101.24                   Fog  \n",
       "2               4.0     101.26  Freezing Drizzle,Fog  \n",
       "3               4.0     101.27  Freezing Drizzle,Fog  \n",
       "4               4.8     101.23                   Fog  \n",
       "...             ...        ...                   ...  \n",
       "8779            9.7     100.13                  Snow  \n",
       "8780            9.7     100.03                  Snow  \n",
       "8781            4.8      99.95                  Snow  \n",
       "8782            9.7      99.91                  Snow  \n",
       "8783           11.3      99.89                  Snow  \n",
       "\n",
       "[8784 rows x 8 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=pd.read_csv('weather.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "764257e5-b4a7-43c8-b4d2-f0203e97e927",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Weather'].unique()\n",
    "df=df.drop('Date/Time', axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fa3716ed-c25b-40bc-992d-aaebbcaf9502",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Temp_C</th>\n",
       "      <th>Dew Point Temp_C</th>\n",
       "      <th>Rel Hum_%</th>\n",
       "      <th>Wind Speed_km/h</th>\n",
       "      <th>Visibility_km</th>\n",
       "      <th>Press_kPa</th>\n",
       "      <th>Weather</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1.8</td>\n",
       "      <td>-3.9</td>\n",
       "      <td>86</td>\n",
       "      <td>4</td>\n",
       "      <td>8.0</td>\n",
       "      <td>101.24</td>\n",
       "      <td>Fog</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-1.8</td>\n",
       "      <td>-3.7</td>\n",
       "      <td>87</td>\n",
       "      <td>4</td>\n",
       "      <td>8.0</td>\n",
       "      <td>101.24</td>\n",
       "      <td>Fog</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-1.8</td>\n",
       "      <td>-3.4</td>\n",
       "      <td>89</td>\n",
       "      <td>7</td>\n",
       "      <td>4.0</td>\n",
       "      <td>101.26</td>\n",
       "      <td>Freezing Drizzle,Fog</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-1.5</td>\n",
       "      <td>-3.2</td>\n",
       "      <td>88</td>\n",
       "      <td>6</td>\n",
       "      <td>4.0</td>\n",
       "      <td>101.27</td>\n",
       "      <td>Freezing Drizzle,Fog</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-1.5</td>\n",
       "      <td>-3.3</td>\n",
       "      <td>88</td>\n",
       "      <td>7</td>\n",
       "      <td>4.8</td>\n",
       "      <td>101.23</td>\n",
       "      <td>Fog</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8779</th>\n",
       "      <td>0.1</td>\n",
       "      <td>-2.7</td>\n",
       "      <td>81</td>\n",
       "      <td>30</td>\n",
       "      <td>9.7</td>\n",
       "      <td>100.13</td>\n",
       "      <td>Snow</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8780</th>\n",
       "      <td>0.2</td>\n",
       "      <td>-2.4</td>\n",
       "      <td>83</td>\n",
       "      <td>24</td>\n",
       "      <td>9.7</td>\n",
       "      <td>100.03</td>\n",
       "      <td>Snow</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8781</th>\n",
       "      <td>-0.5</td>\n",
       "      <td>-1.5</td>\n",
       "      <td>93</td>\n",
       "      <td>28</td>\n",
       "      <td>4.8</td>\n",
       "      <td>99.95</td>\n",
       "      <td>Snow</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8782</th>\n",
       "      <td>-0.2</td>\n",
       "      <td>-1.8</td>\n",
       "      <td>89</td>\n",
       "      <td>28</td>\n",
       "      <td>9.7</td>\n",
       "      <td>99.91</td>\n",
       "      <td>Snow</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8783</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-2.1</td>\n",
       "      <td>86</td>\n",
       "      <td>30</td>\n",
       "      <td>11.3</td>\n",
       "      <td>99.89</td>\n",
       "      <td>Snow</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8784 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Temp_C  Dew Point Temp_C  Rel Hum_%  Wind Speed_km/h  Visibility_km  \\\n",
       "0       -1.8              -3.9         86                4            8.0   \n",
       "1       -1.8              -3.7         87                4            8.0   \n",
       "2       -1.8              -3.4         89                7            4.0   \n",
       "3       -1.5              -3.2         88                6            4.0   \n",
       "4       -1.5              -3.3         88                7            4.8   \n",
       "...      ...               ...        ...              ...            ...   \n",
       "8779     0.1              -2.7         81               30            9.7   \n",
       "8780     0.2              -2.4         83               24            9.7   \n",
       "8781    -0.5              -1.5         93               28            4.8   \n",
       "8782    -0.2              -1.8         89               28            9.7   \n",
       "8783     0.0              -2.1         86               30           11.3   \n",
       "\n",
       "      Press_kPa               Weather  \n",
       "0        101.24                   Fog  \n",
       "1        101.24                   Fog  \n",
       "2        101.26  Freezing Drizzle,Fog  \n",
       "3        101.27  Freezing Drizzle,Fog  \n",
       "4        101.23                   Fog  \n",
       "...         ...                   ...  \n",
       "8779     100.13                  Snow  \n",
       "8780     100.03                  Snow  \n",
       "8781      99.95                  Snow  \n",
       "8782      99.91                  Snow  \n",
       "8783      99.89                  Snow  \n",
       "\n",
       "[8784 rows x 7 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6d7aa55b-6a7e-444e-aab1-238c7320fe8e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Fog', 'Freezing Drizzle,Fog', 'Mostly Cloudy', 'Cloudy', 'Rain',\n",
       "       'Rain Showers', 'Mainly Clear', 'Snow Showers', 'Snow', 'Clear',\n",
       "       'Freezing Rain,Fog', 'Freezing Rain', 'Freezing Drizzle',\n",
       "       'Rain,Snow', 'Moderate Snow', 'Freezing Drizzle,Snow',\n",
       "       'Freezing Rain,Snow Grains', 'Snow,Blowing Snow', 'Freezing Fog',\n",
       "       'Haze', 'Rain,Fog', 'Drizzle,Fog', 'Drizzle',\n",
       "       'Freezing Drizzle,Haze', 'Freezing Rain,Haze', 'Snow,Haze',\n",
       "       'Snow,Fog', 'Snow,Ice Pellets', 'Rain,Haze', 'Thunderstorms,Rain',\n",
       "       'Thunderstorms,Rain Showers', 'Thunderstorms,Heavy Rain Showers',\n",
       "       'Thunderstorms,Rain Showers,Fog', 'Thunderstorms',\n",
       "       'Thunderstorms,Rain,Fog',\n",
       "       'Thunderstorms,Moderate Rain Showers,Fog', 'Rain Showers,Fog',\n",
       "       'Rain Showers,Snow Showers', 'Snow Pellets', 'Rain,Snow,Fog',\n",
       "       'Moderate Rain,Fog', 'Freezing Rain,Ice Pellets,Fog',\n",
       "       'Drizzle,Ice Pellets,Fog', 'Drizzle,Snow', 'Rain,Ice Pellets',\n",
       "       'Drizzle,Snow,Fog', 'Rain,Snow Grains', 'Rain,Snow,Ice Pellets',\n",
       "       'Snow Showers,Fog', 'Moderate Snow,Blowing Snow'], dtype=object)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Weather'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6ecc6df0-f49b-4a14-b561-3f2ae5673ea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Features (X) and Target (y)\n",
    "X = df.drop(\"Weather\", axis=1)\n",
    "y = df[\"Weather\"]\n",
    "\n",
    "# Encode target labels\n",
    "le = LabelEncoder()\n",
    "y = le.fit_transform(y)\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Standardize features\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4ad243b4-5c27-4026-a477-6fb0d4b00dd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Accuracy: 0.5452475811041548\n"
     ]
    }
   ],
   "source": [
    "rf = RandomForestClassifier(n_estimators=200, random_state=42)\n",
    "rf.fit(X_train, y_train)\n",
    "y_pred_rf = rf.predict(X_test)\n",
    "\n",
    "print(\"Random Forest Accuracy:\", accuracy_score(y_test, y_pred_rf))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3ef39431-89f8-4526-b836-1c256b5b661b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM Accuracy: 0.41718838929994306\n"
     ]
    }
   ],
   "source": [
    "svm = SVC(kernel='rbf', C=10, gamma=0.1)\n",
    "svm.fit(X_train, y_train)\n",
    "y_pred_svm = svm.predict(X_test)\n",
    "\n",
    "print(\"SVM Accuracy:\", accuracy_score(y_test, y_pred_svm))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "386eb384-0667-4282-93f8-e694614925f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Accuracy: 0.3801935116676153\n"
     ]
    }
   ],
   "source": [
    "logreg = LogisticRegression(max_iter=500)\n",
    "logreg.fit(X_train, y_train)\n",
    "y_pred_log = logreg.predict(X_test)\n",
    "\n",
    "print(\"Logistic Regression Accuracy:\", accuracy_score(y_test, y_pred_log))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "35b302c2-da6c-4b60-ae33-789890f79060",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 27 candidates, totalling 81 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ASUS\\anaconda3\\envs\\Env1\\lib\\site-packages\\sklearn\\model_selection\\_split.py:805: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=3.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: {'max_depth': 20, 'min_samples_split': 2, 'n_estimators': 200}\n",
      "Best Accuracy: 0.5036267341387558\n"
     ]
    }
   ],
   "source": [
    "param_grid = {\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'max_depth': [10, 20, None],\n",
    "    'min_samples_split': [2, 5, 10]\n",
    "}\n",
    "\n",
    "grid = GridSearchCV(RandomForestClassifier(random_state=42),\n",
    "                    param_grid, cv=3, n_jobs=-1, verbose=1)\n",
    "grid.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best Parameters:\", grid.best_params_)\n",
    "print(\"Best Accuracy:\", grid.best_score_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6dd76d3-c3b8-4d3c-9957-c6119113af2c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f0a1370-f6f9-43ae-a7ea-956e788c43f4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ce9f45e6-4f60-406a-930b-2ed83e347f68",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/1000], Loss: 460.5684, Train Acc: 28.43%\n",
      "Epoch [2/1000], Loss: 365.4384, Train Acc: 34.78%\n",
      "Epoch [3/1000], Loss: 351.4885, Train Acc: 35.59%\n",
      "Epoch [4/1000], Loss: 343.3059, Train Acc: 37.16%\n",
      "Epoch [5/1000], Loss: 342.6310, Train Acc: 36.50%\n",
      "Epoch [6/1000], Loss: 339.8359, Train Acc: 37.01%\n",
      "Epoch [7/1000], Loss: 336.0439, Train Acc: 37.64%\n",
      "Epoch [8/1000], Loss: 334.3755, Train Acc: 37.57%\n",
      "Epoch [9/1000], Loss: 332.8620, Train Acc: 38.08%\n",
      "Epoch [10/1000], Loss: 333.3910, Train Acc: 38.12%\n",
      "Epoch [11/1000], Loss: 330.4494, Train Acc: 38.35%\n",
      "Epoch [12/1000], Loss: 329.5529, Train Acc: 38.37%\n",
      "Epoch [13/1000], Loss: 330.2641, Train Acc: 38.81%\n",
      "Epoch [14/1000], Loss: 329.1311, Train Acc: 38.64%\n",
      "Epoch [15/1000], Loss: 327.5485, Train Acc: 38.65%\n",
      "Epoch [16/1000], Loss: 326.2612, Train Acc: 38.68%\n",
      "Epoch [17/1000], Loss: 325.8245, Train Acc: 38.21%\n",
      "Epoch [18/1000], Loss: 325.2114, Train Acc: 38.47%\n",
      "Epoch [19/1000], Loss: 324.2856, Train Acc: 38.35%\n",
      "Epoch [20/1000], Loss: 324.1392, Train Acc: 38.81%\n",
      "Epoch [21/1000], Loss: 323.6192, Train Acc: 38.62%\n",
      "Epoch [22/1000], Loss: 323.1269, Train Acc: 38.52%\n",
      "Epoch [23/1000], Loss: 320.9605, Train Acc: 38.85%\n",
      "Epoch [24/1000], Loss: 321.1307, Train Acc: 39.39%\n",
      "Epoch [25/1000], Loss: 322.3869, Train Acc: 39.11%\n",
      "Epoch [26/1000], Loss: 320.3640, Train Acc: 39.02%\n",
      "Epoch [27/1000], Loss: 321.1036, Train Acc: 38.82%\n",
      "Epoch [28/1000], Loss: 319.3066, Train Acc: 39.23%\n",
      "Epoch [29/1000], Loss: 320.7030, Train Acc: 38.37%\n",
      "Epoch [30/1000], Loss: 320.2972, Train Acc: 39.08%\n",
      "Epoch [31/1000], Loss: 315.7544, Train Acc: 39.31%\n",
      "Epoch [32/1000], Loss: 316.7238, Train Acc: 39.90%\n",
      "Epoch [33/1000], Loss: 317.2532, Train Acc: 38.92%\n",
      "Epoch [34/1000], Loss: 316.9533, Train Acc: 39.26%\n",
      "Epoch [35/1000], Loss: 316.7624, Train Acc: 39.97%\n",
      "Epoch [36/1000], Loss: 316.8790, Train Acc: 39.32%\n",
      "Epoch [37/1000], Loss: 315.3745, Train Acc: 39.58%\n",
      "Epoch [38/1000], Loss: 315.9185, Train Acc: 39.49%\n",
      "Epoch [39/1000], Loss: 315.9771, Train Acc: 40.00%\n",
      "Epoch [40/1000], Loss: 315.6319, Train Acc: 39.36%\n",
      "Epoch [41/1000], Loss: 315.3998, Train Acc: 39.63%\n",
      "Epoch [42/1000], Loss: 313.3576, Train Acc: 40.05%\n",
      "Epoch [43/1000], Loss: 313.9375, Train Acc: 40.23%\n",
      "Epoch [44/1000], Loss: 314.5685, Train Acc: 39.39%\n",
      "Epoch [45/1000], Loss: 313.4699, Train Acc: 39.93%\n",
      "Epoch [46/1000], Loss: 312.5082, Train Acc: 40.10%\n",
      "Epoch [47/1000], Loss: 311.4660, Train Acc: 40.59%\n",
      "Epoch [48/1000], Loss: 312.4315, Train Acc: 40.05%\n",
      "Epoch [49/1000], Loss: 311.3940, Train Acc: 40.37%\n",
      "Epoch [50/1000], Loss: 311.7633, Train Acc: 39.58%\n",
      "Epoch [51/1000], Loss: 313.1485, Train Acc: 40.86%\n",
      "Epoch [52/1000], Loss: 311.4139, Train Acc: 39.72%\n",
      "Epoch [53/1000], Loss: 311.7168, Train Acc: 40.84%\n",
      "Epoch [54/1000], Loss: 308.5713, Train Acc: 40.47%\n",
      "Epoch [55/1000], Loss: 311.6512, Train Acc: 39.87%\n",
      "Epoch [56/1000], Loss: 309.4889, Train Acc: 41.10%\n",
      "Epoch [57/1000], Loss: 310.6906, Train Acc: 40.64%\n",
      "Epoch [58/1000], Loss: 309.9246, Train Acc: 40.43%\n",
      "Epoch [59/1000], Loss: 311.0783, Train Acc: 39.63%\n",
      "Epoch [60/1000], Loss: 309.7028, Train Acc: 39.85%\n",
      "Epoch [61/1000], Loss: 309.4855, Train Acc: 40.87%\n",
      "Epoch [62/1000], Loss: 308.4564, Train Acc: 40.73%\n",
      "Epoch [63/1000], Loss: 309.0026, Train Acc: 40.19%\n",
      "Epoch [64/1000], Loss: 308.1011, Train Acc: 40.90%\n",
      "Epoch [65/1000], Loss: 307.6223, Train Acc: 40.81%\n",
      "Epoch [66/1000], Loss: 307.5775, Train Acc: 40.76%\n",
      "Epoch [67/1000], Loss: 307.4031, Train Acc: 40.46%\n",
      "Epoch [68/1000], Loss: 307.3200, Train Acc: 40.91%\n",
      "Epoch [69/1000], Loss: 306.7957, Train Acc: 40.46%\n",
      "Epoch [70/1000], Loss: 307.1911, Train Acc: 41.17%\n",
      "Epoch [71/1000], Loss: 306.5108, Train Acc: 40.32%\n",
      "Epoch [72/1000], Loss: 307.2485, Train Acc: 41.26%\n",
      "Epoch [73/1000], Loss: 305.4989, Train Acc: 40.80%\n",
      "Epoch [74/1000], Loss: 307.3594, Train Acc: 41.06%\n",
      "Epoch [75/1000], Loss: 307.6980, Train Acc: 40.71%\n",
      "Epoch [76/1000], Loss: 306.3694, Train Acc: 41.47%\n",
      "Epoch [77/1000], Loss: 305.9717, Train Acc: 41.47%\n",
      "Epoch [78/1000], Loss: 305.5521, Train Acc: 40.71%\n",
      "Epoch [79/1000], Loss: 305.3904, Train Acc: 40.93%\n",
      "Epoch [80/1000], Loss: 305.7082, Train Acc: 40.74%\n",
      "Epoch [81/1000], Loss: 303.0612, Train Acc: 40.90%\n",
      "Epoch [82/1000], Loss: 304.1393, Train Acc: 41.47%\n",
      "Epoch [83/1000], Loss: 303.1870, Train Acc: 41.33%\n",
      "Epoch [84/1000], Loss: 305.2018, Train Acc: 40.63%\n",
      "Epoch [85/1000], Loss: 304.5245, Train Acc: 40.49%\n",
      "Epoch [86/1000], Loss: 303.8718, Train Acc: 41.24%\n",
      "Epoch [87/1000], Loss: 302.9489, Train Acc: 41.74%\n",
      "Epoch [88/1000], Loss: 302.2260, Train Acc: 41.51%\n",
      "Epoch [89/1000], Loss: 304.1059, Train Acc: 41.43%\n",
      "Epoch [90/1000], Loss: 302.8920, Train Acc: 41.44%\n",
      "Epoch [91/1000], Loss: 303.9110, Train Acc: 41.72%\n",
      "Epoch [92/1000], Loss: 304.0607, Train Acc: 41.21%\n",
      "Epoch [93/1000], Loss: 303.9478, Train Acc: 42.35%\n",
      "Epoch [94/1000], Loss: 302.7938, Train Acc: 41.34%\n",
      "Epoch [95/1000], Loss: 301.6993, Train Acc: 41.40%\n",
      "Epoch [96/1000], Loss: 302.7193, Train Acc: 40.66%\n",
      "Epoch [97/1000], Loss: 301.1810, Train Acc: 41.44%\n",
      "Epoch [98/1000], Loss: 301.7516, Train Acc: 41.65%\n",
      "Epoch [99/1000], Loss: 302.0703, Train Acc: 41.38%\n",
      "Epoch [100/1000], Loss: 302.9088, Train Acc: 41.51%\n",
      "Epoch [101/1000], Loss: 301.6858, Train Acc: 41.10%\n",
      "Epoch [102/1000], Loss: 302.4066, Train Acc: 41.40%\n",
      "Epoch [103/1000], Loss: 301.7239, Train Acc: 42.11%\n",
      "Epoch [104/1000], Loss: 300.5928, Train Acc: 42.12%\n",
      "Epoch [105/1000], Loss: 302.0998, Train Acc: 41.92%\n",
      "Epoch [106/1000], Loss: 300.4997, Train Acc: 41.14%\n",
      "Epoch [107/1000], Loss: 301.5663, Train Acc: 41.61%\n",
      "Epoch [108/1000], Loss: 301.9491, Train Acc: 40.91%\n",
      "Epoch [109/1000], Loss: 300.4320, Train Acc: 42.25%\n",
      "Epoch [110/1000], Loss: 301.7330, Train Acc: 41.60%\n",
      "Epoch [111/1000], Loss: 302.0356, Train Acc: 41.78%\n",
      "Epoch [112/1000], Loss: 300.3855, Train Acc: 42.07%\n",
      "Epoch [113/1000], Loss: 300.5115, Train Acc: 41.84%\n",
      "Epoch [114/1000], Loss: 299.6432, Train Acc: 41.78%\n",
      "Epoch [115/1000], Loss: 301.5456, Train Acc: 41.55%\n",
      "Epoch [116/1000], Loss: 300.8461, Train Acc: 42.39%\n",
      "Epoch [117/1000], Loss: 300.3580, Train Acc: 41.88%\n",
      "Epoch [118/1000], Loss: 301.3785, Train Acc: 41.75%\n",
      "Epoch [119/1000], Loss: 301.1122, Train Acc: 41.47%\n",
      "Epoch [120/1000], Loss: 300.8037, Train Acc: 41.48%\n",
      "Epoch [121/1000], Loss: 300.6721, Train Acc: 41.44%\n",
      "Epoch [122/1000], Loss: 299.3534, Train Acc: 41.85%\n",
      "Epoch [123/1000], Loss: 299.6897, Train Acc: 41.67%\n",
      "Epoch [124/1000], Loss: 299.7503, Train Acc: 42.32%\n",
      "Epoch [125/1000], Loss: 298.7216, Train Acc: 41.58%\n",
      "Epoch [126/1000], Loss: 298.5868, Train Acc: 42.01%\n",
      "Epoch [127/1000], Loss: 297.4685, Train Acc: 42.02%\n",
      "Epoch [128/1000], Loss: 299.9074, Train Acc: 41.81%\n",
      "Epoch [129/1000], Loss: 300.6758, Train Acc: 41.63%\n",
      "Epoch [130/1000], Loss: 297.9280, Train Acc: 43.03%\n",
      "Epoch [131/1000], Loss: 298.3825, Train Acc: 42.34%\n",
      "Epoch [132/1000], Loss: 299.0294, Train Acc: 42.24%\n",
      "Epoch [133/1000], Loss: 299.7846, Train Acc: 41.31%\n",
      "Epoch [134/1000], Loss: 297.4734, Train Acc: 42.24%\n",
      "Epoch [135/1000], Loss: 299.0190, Train Acc: 42.38%\n",
      "Epoch [136/1000], Loss: 298.5717, Train Acc: 41.94%\n",
      "Epoch [137/1000], Loss: 299.0696, Train Acc: 42.54%\n",
      "Epoch [138/1000], Loss: 298.5123, Train Acc: 42.81%\n",
      "Epoch [139/1000], Loss: 296.7299, Train Acc: 42.75%\n",
      "Epoch [140/1000], Loss: 296.8677, Train Acc: 43.13%\n",
      "Epoch [141/1000], Loss: 296.2582, Train Acc: 42.81%\n",
      "Epoch [142/1000], Loss: 297.1866, Train Acc: 42.34%\n",
      "Epoch [143/1000], Loss: 297.7945, Train Acc: 42.29%\n",
      "Epoch [144/1000], Loss: 296.9119, Train Acc: 42.82%\n",
      "Epoch [145/1000], Loss: 297.4393, Train Acc: 41.92%\n",
      "Epoch [146/1000], Loss: 297.8707, Train Acc: 42.41%\n",
      "Epoch [147/1000], Loss: 298.9498, Train Acc: 42.31%\n",
      "Epoch [148/1000], Loss: 297.9672, Train Acc: 42.45%\n",
      "Epoch [149/1000], Loss: 294.5269, Train Acc: 43.40%\n",
      "Epoch [150/1000], Loss: 298.4537, Train Acc: 42.11%\n",
      "Epoch [151/1000], Loss: 296.8441, Train Acc: 42.38%\n",
      "Epoch [152/1000], Loss: 297.6825, Train Acc: 42.27%\n",
      "Epoch [153/1000], Loss: 296.2840, Train Acc: 42.34%\n",
      "Epoch [154/1000], Loss: 297.5469, Train Acc: 42.21%\n",
      "Epoch [155/1000], Loss: 296.5768, Train Acc: 42.48%\n",
      "Epoch [156/1000], Loss: 297.1178, Train Acc: 41.65%\n",
      "Epoch [157/1000], Loss: 297.0422, Train Acc: 42.35%\n",
      "Epoch [158/1000], Loss: 296.9479, Train Acc: 42.64%\n",
      "Epoch [159/1000], Loss: 297.5670, Train Acc: 42.46%\n",
      "Epoch [160/1000], Loss: 298.6117, Train Acc: 42.17%\n",
      "Epoch [161/1000], Loss: 296.6630, Train Acc: 42.11%\n",
      "Epoch [162/1000], Loss: 296.2221, Train Acc: 42.22%\n",
      "Epoch [163/1000], Loss: 292.5403, Train Acc: 42.82%\n",
      "Epoch [164/1000], Loss: 296.1225, Train Acc: 42.29%\n",
      "Epoch [165/1000], Loss: 296.7185, Train Acc: 42.21%\n",
      "Epoch [166/1000], Loss: 294.7183, Train Acc: 43.11%\n",
      "Epoch [167/1000], Loss: 297.1098, Train Acc: 42.68%\n",
      "Epoch [168/1000], Loss: 294.3871, Train Acc: 42.04%\n",
      "Epoch [169/1000], Loss: 295.0190, Train Acc: 42.95%\n",
      "Epoch [170/1000], Loss: 296.4401, Train Acc: 42.76%\n",
      "Epoch [171/1000], Loss: 295.4639, Train Acc: 42.69%\n",
      "Epoch [172/1000], Loss: 296.8495, Train Acc: 42.69%\n",
      "Epoch [173/1000], Loss: 296.5254, Train Acc: 42.22%\n",
      "Epoch [174/1000], Loss: 293.5929, Train Acc: 42.82%\n",
      "Epoch [175/1000], Loss: 295.3610, Train Acc: 42.88%\n",
      "Epoch [176/1000], Loss: 295.0343, Train Acc: 42.59%\n",
      "Epoch [177/1000], Loss: 295.9034, Train Acc: 42.58%\n",
      "Epoch [178/1000], Loss: 295.4522, Train Acc: 42.99%\n",
      "Epoch [179/1000], Loss: 294.9534, Train Acc: 42.04%\n",
      "Epoch [180/1000], Loss: 295.4675, Train Acc: 42.74%\n",
      "Epoch [181/1000], Loss: 295.6797, Train Acc: 42.25%\n",
      "Epoch [182/1000], Loss: 294.4142, Train Acc: 42.83%\n",
      "Epoch [183/1000], Loss: 294.1733, Train Acc: 42.99%\n",
      "Epoch [184/1000], Loss: 294.1654, Train Acc: 43.11%\n",
      "Epoch [185/1000], Loss: 293.6934, Train Acc: 42.64%\n",
      "Epoch [186/1000], Loss: 294.5070, Train Acc: 42.48%\n",
      "Epoch [187/1000], Loss: 294.5156, Train Acc: 42.92%\n",
      "Epoch [188/1000], Loss: 294.3359, Train Acc: 42.59%\n",
      "Epoch [189/1000], Loss: 294.6337, Train Acc: 42.37%\n",
      "Epoch [190/1000], Loss: 293.3221, Train Acc: 42.74%\n",
      "Epoch [191/1000], Loss: 293.6617, Train Acc: 42.95%\n",
      "Epoch [192/1000], Loss: 293.4064, Train Acc: 43.36%\n",
      "Epoch [193/1000], Loss: 294.1028, Train Acc: 43.13%\n",
      "Epoch [194/1000], Loss: 291.7657, Train Acc: 42.82%\n",
      "Epoch [195/1000], Loss: 290.9382, Train Acc: 43.70%\n",
      "Epoch [196/1000], Loss: 293.0995, Train Acc: 43.30%\n",
      "Epoch [197/1000], Loss: 294.9760, Train Acc: 42.41%\n",
      "Epoch [198/1000], Loss: 293.7403, Train Acc: 43.29%\n",
      "Epoch [199/1000], Loss: 291.8585, Train Acc: 43.35%\n",
      "Epoch [200/1000], Loss: 292.6565, Train Acc: 43.29%\n",
      "Epoch [201/1000], Loss: 292.8504, Train Acc: 43.02%\n",
      "Epoch [202/1000], Loss: 291.4397, Train Acc: 43.11%\n",
      "Epoch [203/1000], Loss: 292.0813, Train Acc: 43.63%\n",
      "Epoch [204/1000], Loss: 292.5418, Train Acc: 43.32%\n",
      "Epoch [205/1000], Loss: 293.7997, Train Acc: 42.81%\n",
      "Epoch [206/1000], Loss: 292.3535, Train Acc: 43.05%\n",
      "Epoch [207/1000], Loss: 291.7484, Train Acc: 43.69%\n",
      "Epoch [208/1000], Loss: 292.4212, Train Acc: 43.29%\n",
      "Epoch [209/1000], Loss: 291.8411, Train Acc: 43.42%\n",
      "Epoch [210/1000], Loss: 290.2500, Train Acc: 43.79%\n",
      "Epoch [211/1000], Loss: 292.3711, Train Acc: 42.61%\n",
      "Epoch [212/1000], Loss: 293.4824, Train Acc: 43.08%\n",
      "Epoch [213/1000], Loss: 291.0859, Train Acc: 43.87%\n",
      "Epoch [214/1000], Loss: 292.0897, Train Acc: 43.05%\n",
      "Epoch [215/1000], Loss: 291.9263, Train Acc: 42.65%\n",
      "Epoch [216/1000], Loss: 289.7174, Train Acc: 44.10%\n",
      "Epoch [217/1000], Loss: 291.6674, Train Acc: 43.52%\n",
      "Epoch [218/1000], Loss: 291.7024, Train Acc: 43.25%\n",
      "Epoch [219/1000], Loss: 291.8022, Train Acc: 43.38%\n",
      "Epoch [220/1000], Loss: 292.7723, Train Acc: 43.08%\n",
      "Epoch [221/1000], Loss: 290.3167, Train Acc: 43.85%\n",
      "Epoch [222/1000], Loss: 291.7615, Train Acc: 43.19%\n",
      "Epoch [223/1000], Loss: 291.2870, Train Acc: 44.12%\n",
      "Epoch [224/1000], Loss: 291.9896, Train Acc: 43.22%\n",
      "Epoch [225/1000], Loss: 291.9749, Train Acc: 43.77%\n",
      "Epoch [226/1000], Loss: 291.4844, Train Acc: 42.88%\n",
      "Epoch [227/1000], Loss: 290.8456, Train Acc: 43.09%\n",
      "Epoch [228/1000], Loss: 292.5841, Train Acc: 42.89%\n",
      "Epoch [229/1000], Loss: 290.6690, Train Acc: 42.96%\n",
      "Epoch [230/1000], Loss: 291.0802, Train Acc: 42.88%\n",
      "Epoch [231/1000], Loss: 290.1499, Train Acc: 43.01%\n",
      "Epoch [232/1000], Loss: 290.1328, Train Acc: 44.27%\n",
      "Epoch [233/1000], Loss: 291.4023, Train Acc: 42.96%\n",
      "Epoch [234/1000], Loss: 291.7209, Train Acc: 43.87%\n",
      "Epoch [235/1000], Loss: 290.9892, Train Acc: 42.66%\n",
      "Epoch [236/1000], Loss: 289.9423, Train Acc: 44.00%\n",
      "Epoch [237/1000], Loss: 292.1622, Train Acc: 43.42%\n",
      "Epoch [238/1000], Loss: 289.8767, Train Acc: 43.26%\n",
      "Epoch [239/1000], Loss: 291.2897, Train Acc: 42.64%\n",
      "Epoch [240/1000], Loss: 288.7444, Train Acc: 43.63%\n",
      "Epoch [241/1000], Loss: 292.1178, Train Acc: 43.57%\n",
      "Epoch [242/1000], Loss: 291.6498, Train Acc: 42.71%\n",
      "Epoch [243/1000], Loss: 289.6051, Train Acc: 43.43%\n",
      "Epoch [244/1000], Loss: 289.9701, Train Acc: 43.01%\n",
      "Epoch [245/1000], Loss: 288.8919, Train Acc: 43.70%\n",
      "Epoch [246/1000], Loss: 290.0554, Train Acc: 43.87%\n",
      "Epoch [247/1000], Loss: 289.5607, Train Acc: 43.20%\n",
      "Epoch [248/1000], Loss: 290.4277, Train Acc: 43.30%\n",
      "Epoch [249/1000], Loss: 290.8927, Train Acc: 43.35%\n",
      "Epoch [250/1000], Loss: 291.4711, Train Acc: 42.96%\n",
      "Epoch [251/1000], Loss: 290.3879, Train Acc: 43.43%\n",
      "Epoch [252/1000], Loss: 290.6113, Train Acc: 43.75%\n",
      "Epoch [253/1000], Loss: 288.6153, Train Acc: 43.82%\n",
      "Epoch [254/1000], Loss: 290.7782, Train Acc: 43.28%\n",
      "Epoch [255/1000], Loss: 289.7291, Train Acc: 44.04%\n",
      "Epoch [256/1000], Loss: 290.8604, Train Acc: 43.50%\n",
      "Epoch [257/1000], Loss: 290.0631, Train Acc: 43.67%\n",
      "Epoch [258/1000], Loss: 292.5098, Train Acc: 43.70%\n",
      "Epoch [259/1000], Loss: 289.5298, Train Acc: 43.86%\n",
      "Epoch [260/1000], Loss: 291.3258, Train Acc: 42.72%\n",
      "Epoch [261/1000], Loss: 291.9169, Train Acc: 43.32%\n",
      "Epoch [262/1000], Loss: 289.8572, Train Acc: 43.25%\n",
      "Epoch [263/1000], Loss: 289.1733, Train Acc: 43.77%\n",
      "Epoch [264/1000], Loss: 289.7141, Train Acc: 42.75%\n",
      "Epoch [265/1000], Loss: 288.7407, Train Acc: 43.92%\n",
      "Epoch [266/1000], Loss: 288.7964, Train Acc: 44.33%\n",
      "Epoch [267/1000], Loss: 291.4439, Train Acc: 43.67%\n",
      "Epoch [268/1000], Loss: 288.3072, Train Acc: 44.22%\n",
      "Epoch [269/1000], Loss: 287.9859, Train Acc: 43.28%\n",
      "Epoch [270/1000], Loss: 289.9978, Train Acc: 43.62%\n",
      "Epoch [271/1000], Loss: 290.6984, Train Acc: 43.56%\n",
      "Epoch [272/1000], Loss: 289.7088, Train Acc: 42.66%\n",
      "Epoch [273/1000], Loss: 289.9605, Train Acc: 43.02%\n",
      "Epoch [274/1000], Loss: 289.2710, Train Acc: 43.80%\n",
      "Epoch [275/1000], Loss: 289.1228, Train Acc: 43.25%\n",
      "Epoch [276/1000], Loss: 288.0585, Train Acc: 43.57%\n",
      "Epoch [277/1000], Loss: 289.0890, Train Acc: 43.99%\n",
      "Epoch [278/1000], Loss: 290.3837, Train Acc: 43.28%\n",
      "Epoch [279/1000], Loss: 288.2199, Train Acc: 43.48%\n",
      "Epoch [280/1000], Loss: 288.7212, Train Acc: 44.09%\n",
      "Epoch [281/1000], Loss: 289.3843, Train Acc: 43.06%\n",
      "Epoch [282/1000], Loss: 289.4495, Train Acc: 43.32%\n",
      "Epoch [283/1000], Loss: 288.1899, Train Acc: 43.97%\n",
      "Epoch [284/1000], Loss: 289.7952, Train Acc: 43.23%\n",
      "Epoch [285/1000], Loss: 289.9299, Train Acc: 44.36%\n",
      "Epoch [286/1000], Loss: 290.8803, Train Acc: 42.75%\n",
      "Epoch [287/1000], Loss: 289.6384, Train Acc: 43.99%\n",
      "Epoch [288/1000], Loss: 289.7704, Train Acc: 42.85%\n",
      "Epoch [289/1000], Loss: 286.9492, Train Acc: 43.97%\n",
      "Epoch [290/1000], Loss: 288.5209, Train Acc: 43.67%\n",
      "Epoch [291/1000], Loss: 290.2913, Train Acc: 43.08%\n",
      "Epoch [292/1000], Loss: 287.9989, Train Acc: 43.62%\n",
      "Epoch [293/1000], Loss: 287.3107, Train Acc: 44.24%\n",
      "Epoch [294/1000], Loss: 288.7439, Train Acc: 43.30%\n",
      "Epoch [295/1000], Loss: 289.1827, Train Acc: 44.14%\n",
      "Epoch [296/1000], Loss: 290.4134, Train Acc: 43.22%\n",
      "Epoch [297/1000], Loss: 288.7304, Train Acc: 43.83%\n",
      "Epoch [298/1000], Loss: 287.5825, Train Acc: 43.57%\n",
      "Epoch [299/1000], Loss: 289.7445, Train Acc: 42.66%\n",
      "Epoch [300/1000], Loss: 286.1533, Train Acc: 44.06%\n",
      "Epoch [301/1000], Loss: 288.4944, Train Acc: 44.22%\n",
      "Epoch [302/1000], Loss: 287.6508, Train Acc: 43.57%\n",
      "Epoch [303/1000], Loss: 288.8898, Train Acc: 43.57%\n",
      "Epoch [304/1000], Loss: 289.4211, Train Acc: 43.56%\n",
      "Epoch [305/1000], Loss: 289.2348, Train Acc: 43.38%\n",
      "Epoch [306/1000], Loss: 289.6215, Train Acc: 44.06%\n",
      "Epoch [307/1000], Loss: 285.8594, Train Acc: 44.19%\n",
      "Epoch [308/1000], Loss: 288.5087, Train Acc: 43.63%\n",
      "Epoch [309/1000], Loss: 287.3778, Train Acc: 43.94%\n",
      "Epoch [310/1000], Loss: 286.7164, Train Acc: 44.87%\n",
      "Epoch [311/1000], Loss: 289.5974, Train Acc: 43.75%\n",
      "Epoch [312/1000], Loss: 287.5219, Train Acc: 44.03%\n",
      "Epoch [313/1000], Loss: 288.0112, Train Acc: 44.04%\n",
      "Epoch [314/1000], Loss: 290.1173, Train Acc: 43.75%\n",
      "Epoch [315/1000], Loss: 287.2505, Train Acc: 44.12%\n",
      "Epoch [316/1000], Loss: 288.3778, Train Acc: 43.69%\n",
      "Epoch [317/1000], Loss: 287.3400, Train Acc: 43.97%\n",
      "Epoch [318/1000], Loss: 290.1075, Train Acc: 43.56%\n",
      "Epoch [319/1000], Loss: 287.9532, Train Acc: 43.93%\n",
      "Epoch [320/1000], Loss: 287.4742, Train Acc: 43.87%\n",
      "Epoch [321/1000], Loss: 288.5942, Train Acc: 43.13%\n",
      "Epoch [322/1000], Loss: 286.6035, Train Acc: 43.90%\n",
      "Epoch [323/1000], Loss: 288.0532, Train Acc: 44.77%\n",
      "Epoch [324/1000], Loss: 286.3339, Train Acc: 44.06%\n",
      "Epoch [325/1000], Loss: 290.3625, Train Acc: 43.77%\n",
      "Epoch [326/1000], Loss: 288.9036, Train Acc: 43.49%\n",
      "Epoch [327/1000], Loss: 288.4413, Train Acc: 43.42%\n",
      "Epoch [328/1000], Loss: 286.2552, Train Acc: 44.19%\n",
      "Epoch [329/1000], Loss: 287.3831, Train Acc: 44.36%\n",
      "Epoch [330/1000], Loss: 285.5646, Train Acc: 44.09%\n",
      "Epoch [331/1000], Loss: 287.5074, Train Acc: 43.99%\n",
      "Epoch [332/1000], Loss: 288.8399, Train Acc: 43.45%\n",
      "Epoch [333/1000], Loss: 287.9382, Train Acc: 43.43%\n",
      "Epoch [334/1000], Loss: 286.9867, Train Acc: 43.66%\n",
      "Epoch [335/1000], Loss: 288.4104, Train Acc: 44.37%\n",
      "Epoch [336/1000], Loss: 289.5730, Train Acc: 43.20%\n",
      "Epoch [337/1000], Loss: 289.2195, Train Acc: 44.20%\n",
      "Epoch [338/1000], Loss: 287.9418, Train Acc: 43.92%\n",
      "Epoch [339/1000], Loss: 286.1649, Train Acc: 44.46%\n",
      "Epoch [340/1000], Loss: 288.2375, Train Acc: 42.89%\n",
      "Epoch [341/1000], Loss: 287.1345, Train Acc: 43.99%\n",
      "Epoch [342/1000], Loss: 286.9997, Train Acc: 43.80%\n",
      "Epoch [343/1000], Loss: 287.0096, Train Acc: 44.23%\n",
      "Epoch [344/1000], Loss: 286.7220, Train Acc: 44.02%\n",
      "Epoch [345/1000], Loss: 287.9184, Train Acc: 43.55%\n",
      "Epoch [346/1000], Loss: 287.0148, Train Acc: 44.12%\n",
      "Epoch [347/1000], Loss: 286.3376, Train Acc: 43.70%\n",
      "Epoch [348/1000], Loss: 287.1474, Train Acc: 43.67%\n",
      "Epoch [349/1000], Loss: 286.7432, Train Acc: 43.18%\n",
      "Epoch [350/1000], Loss: 288.7107, Train Acc: 43.55%\n",
      "Epoch [351/1000], Loss: 287.3644, Train Acc: 43.52%\n",
      "Epoch [352/1000], Loss: 286.3356, Train Acc: 44.30%\n",
      "Epoch [353/1000], Loss: 286.1070, Train Acc: 44.46%\n",
      "Epoch [354/1000], Loss: 285.2660, Train Acc: 44.29%\n",
      "Epoch [355/1000], Loss: 287.2267, Train Acc: 44.07%\n",
      "Epoch [356/1000], Loss: 287.1332, Train Acc: 43.55%\n",
      "Epoch [357/1000], Loss: 285.2243, Train Acc: 44.86%\n",
      "Epoch [358/1000], Loss: 286.5609, Train Acc: 43.57%\n",
      "Epoch [359/1000], Loss: 287.7598, Train Acc: 42.98%\n",
      "Epoch [360/1000], Loss: 287.8653, Train Acc: 42.96%\n",
      "Epoch [361/1000], Loss: 286.5937, Train Acc: 43.76%\n",
      "Epoch [362/1000], Loss: 286.0270, Train Acc: 43.59%\n",
      "Epoch [363/1000], Loss: 286.9279, Train Acc: 44.03%\n",
      "Epoch [364/1000], Loss: 287.3846, Train Acc: 44.37%\n",
      "Epoch [365/1000], Loss: 287.9881, Train Acc: 43.59%\n",
      "Epoch [366/1000], Loss: 286.1609, Train Acc: 44.39%\n",
      "Epoch [367/1000], Loss: 286.5294, Train Acc: 43.75%\n",
      "Epoch [368/1000], Loss: 285.7590, Train Acc: 44.90%\n",
      "Epoch [369/1000], Loss: 286.8461, Train Acc: 44.68%\n",
      "Epoch [370/1000], Loss: 285.9188, Train Acc: 44.46%\n",
      "Epoch [371/1000], Loss: 288.2055, Train Acc: 43.87%\n",
      "Epoch [372/1000], Loss: 286.5110, Train Acc: 44.43%\n",
      "Epoch [373/1000], Loss: 286.7330, Train Acc: 44.66%\n",
      "Epoch [374/1000], Loss: 287.9565, Train Acc: 44.24%\n",
      "Epoch [375/1000], Loss: 286.4658, Train Acc: 44.30%\n",
      "Epoch [376/1000], Loss: 287.3623, Train Acc: 43.02%\n",
      "Epoch [377/1000], Loss: 284.9554, Train Acc: 44.68%\n",
      "Epoch [378/1000], Loss: 286.8402, Train Acc: 44.16%\n",
      "Epoch [379/1000], Loss: 285.6020, Train Acc: 44.30%\n",
      "Epoch [380/1000], Loss: 286.6791, Train Acc: 44.27%\n",
      "Epoch [381/1000], Loss: 285.8032, Train Acc: 44.40%\n",
      "Epoch [382/1000], Loss: 285.9206, Train Acc: 44.09%\n",
      "Epoch [383/1000], Loss: 285.2638, Train Acc: 44.56%\n",
      "Epoch [384/1000], Loss: 286.0743, Train Acc: 43.63%\n",
      "Epoch [385/1000], Loss: 285.1903, Train Acc: 43.85%\n",
      "Epoch [386/1000], Loss: 284.0731, Train Acc: 43.77%\n",
      "Epoch [387/1000], Loss: 285.5186, Train Acc: 44.46%\n",
      "Epoch [388/1000], Loss: 285.4750, Train Acc: 44.31%\n",
      "Epoch [389/1000], Loss: 285.8551, Train Acc: 44.50%\n",
      "Epoch [390/1000], Loss: 286.9599, Train Acc: 44.34%\n",
      "Epoch [391/1000], Loss: 286.5072, Train Acc: 43.94%\n",
      "Epoch [392/1000], Loss: 285.9903, Train Acc: 44.51%\n",
      "Epoch [393/1000], Loss: 286.8709, Train Acc: 44.56%\n",
      "Epoch [394/1000], Loss: 285.5147, Train Acc: 44.44%\n",
      "Epoch [395/1000], Loss: 285.3515, Train Acc: 44.74%\n",
      "Epoch [396/1000], Loss: 285.8123, Train Acc: 43.49%\n",
      "Epoch [397/1000], Loss: 286.1010, Train Acc: 43.42%\n",
      "Epoch [398/1000], Loss: 287.1991, Train Acc: 43.90%\n",
      "Epoch [399/1000], Loss: 286.2489, Train Acc: 44.46%\n",
      "Epoch [400/1000], Loss: 285.7461, Train Acc: 44.36%\n",
      "Epoch [401/1000], Loss: 284.4676, Train Acc: 44.96%\n",
      "Epoch [402/1000], Loss: 284.7008, Train Acc: 45.05%\n",
      "Epoch [403/1000], Loss: 284.5349, Train Acc: 44.20%\n",
      "Epoch [404/1000], Loss: 286.6442, Train Acc: 44.12%\n",
      "Epoch [405/1000], Loss: 284.6577, Train Acc: 44.19%\n",
      "Epoch [406/1000], Loss: 285.5111, Train Acc: 43.97%\n",
      "Epoch [407/1000], Loss: 285.7325, Train Acc: 44.17%\n",
      "Epoch [408/1000], Loss: 287.0011, Train Acc: 44.20%\n",
      "Epoch [409/1000], Loss: 284.0244, Train Acc: 44.30%\n",
      "Epoch [410/1000], Loss: 287.5119, Train Acc: 43.55%\n",
      "Epoch [411/1000], Loss: 284.6677, Train Acc: 45.27%\n",
      "Epoch [412/1000], Loss: 283.9737, Train Acc: 44.90%\n",
      "Epoch [413/1000], Loss: 284.2393, Train Acc: 44.33%\n",
      "Epoch [414/1000], Loss: 284.9618, Train Acc: 45.14%\n",
      "Epoch [415/1000], Loss: 285.1047, Train Acc: 44.39%\n",
      "Epoch [416/1000], Loss: 285.0673, Train Acc: 44.30%\n",
      "Epoch [417/1000], Loss: 285.3213, Train Acc: 44.47%\n",
      "Epoch [418/1000], Loss: 286.0200, Train Acc: 43.62%\n",
      "Epoch [419/1000], Loss: 284.6662, Train Acc: 44.84%\n",
      "Epoch [420/1000], Loss: 286.8385, Train Acc: 43.67%\n",
      "Epoch [421/1000], Loss: 284.5818, Train Acc: 44.70%\n",
      "Epoch [422/1000], Loss: 285.2272, Train Acc: 44.10%\n",
      "Epoch [423/1000], Loss: 283.8563, Train Acc: 44.50%\n",
      "Epoch [424/1000], Loss: 284.5149, Train Acc: 44.14%\n",
      "Epoch [425/1000], Loss: 284.4468, Train Acc: 44.96%\n",
      "Epoch [426/1000], Loss: 284.3599, Train Acc: 44.44%\n",
      "Epoch [427/1000], Loss: 286.6914, Train Acc: 44.07%\n",
      "Epoch [428/1000], Loss: 285.2270, Train Acc: 43.94%\n",
      "Epoch [429/1000], Loss: 286.0064, Train Acc: 43.87%\n",
      "Epoch [430/1000], Loss: 284.1212, Train Acc: 44.50%\n",
      "Epoch [431/1000], Loss: 284.4548, Train Acc: 44.68%\n",
      "Epoch [432/1000], Loss: 283.1413, Train Acc: 44.57%\n",
      "Epoch [433/1000], Loss: 285.0256, Train Acc: 44.07%\n",
      "Epoch [434/1000], Loss: 284.0318, Train Acc: 45.54%\n",
      "Epoch [435/1000], Loss: 284.8932, Train Acc: 44.68%\n",
      "Epoch [436/1000], Loss: 286.5539, Train Acc: 43.76%\n",
      "Epoch [437/1000], Loss: 284.1692, Train Acc: 43.77%\n",
      "Epoch [438/1000], Loss: 284.6005, Train Acc: 45.03%\n",
      "Epoch [439/1000], Loss: 285.5092, Train Acc: 44.74%\n",
      "Epoch [440/1000], Loss: 284.7543, Train Acc: 44.74%\n",
      "Epoch [441/1000], Loss: 283.5969, Train Acc: 44.17%\n",
      "Epoch [442/1000], Loss: 285.8534, Train Acc: 44.37%\n",
      "Epoch [443/1000], Loss: 284.7693, Train Acc: 43.79%\n",
      "Epoch [444/1000], Loss: 283.6311, Train Acc: 44.40%\n",
      "Epoch [445/1000], Loss: 283.6583, Train Acc: 44.67%\n",
      "Epoch [446/1000], Loss: 283.6406, Train Acc: 44.71%\n",
      "Epoch [447/1000], Loss: 286.0912, Train Acc: 44.12%\n",
      "Epoch [448/1000], Loss: 284.1510, Train Acc: 44.00%\n",
      "Epoch [449/1000], Loss: 282.9477, Train Acc: 45.14%\n",
      "Epoch [450/1000], Loss: 283.1452, Train Acc: 44.12%\n",
      "Epoch [451/1000], Loss: 285.3079, Train Acc: 44.39%\n",
      "Epoch [452/1000], Loss: 283.5302, Train Acc: 44.56%\n",
      "Epoch [453/1000], Loss: 285.5620, Train Acc: 44.23%\n",
      "Epoch [454/1000], Loss: 284.8659, Train Acc: 44.64%\n",
      "Epoch [455/1000], Loss: 285.5484, Train Acc: 45.10%\n",
      "Epoch [456/1000], Loss: 284.2721, Train Acc: 45.40%\n",
      "Epoch [457/1000], Loss: 281.7327, Train Acc: 45.14%\n",
      "Epoch [458/1000], Loss: 285.0938, Train Acc: 44.74%\n",
      "Epoch [459/1000], Loss: 283.9906, Train Acc: 44.90%\n",
      "Epoch [460/1000], Loss: 283.6308, Train Acc: 44.54%\n",
      "Epoch [461/1000], Loss: 284.4301, Train Acc: 44.46%\n",
      "Epoch [462/1000], Loss: 285.2964, Train Acc: 44.30%\n",
      "Epoch [463/1000], Loss: 285.3770, Train Acc: 44.39%\n",
      "Epoch [464/1000], Loss: 286.3037, Train Acc: 45.04%\n",
      "Epoch [465/1000], Loss: 284.5389, Train Acc: 44.70%\n",
      "Epoch [466/1000], Loss: 282.7384, Train Acc: 44.93%\n",
      "Epoch [467/1000], Loss: 283.7168, Train Acc: 44.60%\n",
      "Epoch [468/1000], Loss: 283.7368, Train Acc: 44.30%\n",
      "Epoch [469/1000], Loss: 284.6360, Train Acc: 43.76%\n",
      "Epoch [470/1000], Loss: 284.0540, Train Acc: 44.04%\n",
      "Epoch [471/1000], Loss: 284.6766, Train Acc: 44.68%\n",
      "Epoch [472/1000], Loss: 282.3329, Train Acc: 44.49%\n",
      "Epoch [473/1000], Loss: 283.9791, Train Acc: 44.66%\n",
      "Epoch [474/1000], Loss: 285.1265, Train Acc: 44.22%\n",
      "Epoch [475/1000], Loss: 285.3948, Train Acc: 44.37%\n",
      "Epoch [476/1000], Loss: 281.7359, Train Acc: 44.63%\n",
      "Epoch [477/1000], Loss: 284.2149, Train Acc: 44.66%\n",
      "Epoch [478/1000], Loss: 283.8675, Train Acc: 44.63%\n",
      "Epoch [479/1000], Loss: 283.2688, Train Acc: 44.97%\n",
      "Epoch [480/1000], Loss: 284.0156, Train Acc: 44.67%\n",
      "Epoch [481/1000], Loss: 285.3071, Train Acc: 44.26%\n",
      "Epoch [482/1000], Loss: 283.8474, Train Acc: 44.90%\n",
      "Epoch [483/1000], Loss: 282.6532, Train Acc: 45.37%\n",
      "Epoch [484/1000], Loss: 284.7353, Train Acc: 43.97%\n",
      "Epoch [485/1000], Loss: 284.2944, Train Acc: 44.14%\n",
      "Epoch [486/1000], Loss: 282.7498, Train Acc: 44.76%\n",
      "Epoch [487/1000], Loss: 283.5296, Train Acc: 44.26%\n",
      "Epoch [488/1000], Loss: 284.7997, Train Acc: 44.03%\n",
      "Epoch [489/1000], Loss: 282.6876, Train Acc: 44.64%\n",
      "Epoch [490/1000], Loss: 285.0673, Train Acc: 45.33%\n",
      "Epoch [491/1000], Loss: 283.2290, Train Acc: 43.77%\n",
      "Epoch [492/1000], Loss: 281.7812, Train Acc: 45.04%\n",
      "Epoch [493/1000], Loss: 283.8608, Train Acc: 44.16%\n",
      "Epoch [494/1000], Loss: 284.9227, Train Acc: 44.30%\n",
      "Epoch [495/1000], Loss: 284.5985, Train Acc: 44.33%\n",
      "Epoch [496/1000], Loss: 285.6955, Train Acc: 44.17%\n",
      "Epoch [497/1000], Loss: 283.7484, Train Acc: 45.01%\n",
      "Epoch [498/1000], Loss: 283.1212, Train Acc: 44.81%\n",
      "Epoch [499/1000], Loss: 281.8146, Train Acc: 45.27%\n",
      "Epoch [500/1000], Loss: 283.1019, Train Acc: 44.97%\n",
      "Epoch [501/1000], Loss: 283.6775, Train Acc: 44.50%\n",
      "Epoch [502/1000], Loss: 283.4409, Train Acc: 45.00%\n",
      "Epoch [503/1000], Loss: 281.7433, Train Acc: 45.27%\n",
      "Epoch [504/1000], Loss: 284.3676, Train Acc: 44.56%\n",
      "Epoch [505/1000], Loss: 283.8652, Train Acc: 45.42%\n",
      "Epoch [506/1000], Loss: 282.1202, Train Acc: 44.84%\n",
      "Epoch [507/1000], Loss: 284.6159, Train Acc: 43.85%\n",
      "Epoch [508/1000], Loss: 281.8531, Train Acc: 44.40%\n",
      "Epoch [509/1000], Loss: 284.1612, Train Acc: 44.37%\n",
      "Epoch [510/1000], Loss: 284.0021, Train Acc: 44.46%\n",
      "Epoch [511/1000], Loss: 283.3375, Train Acc: 44.13%\n",
      "Epoch [512/1000], Loss: 282.9373, Train Acc: 44.56%\n",
      "Epoch [513/1000], Loss: 284.2885, Train Acc: 44.77%\n",
      "Epoch [514/1000], Loss: 286.6324, Train Acc: 43.62%\n",
      "Epoch [515/1000], Loss: 280.4048, Train Acc: 45.40%\n",
      "Epoch [516/1000], Loss: 283.5753, Train Acc: 44.56%\n",
      "Epoch [517/1000], Loss: 282.4379, Train Acc: 44.90%\n",
      "Epoch [518/1000], Loss: 281.9135, Train Acc: 44.93%\n",
      "Epoch [519/1000], Loss: 284.5283, Train Acc: 44.64%\n",
      "Epoch [520/1000], Loss: 283.1132, Train Acc: 44.77%\n",
      "Epoch [521/1000], Loss: 284.7331, Train Acc: 44.74%\n",
      "Epoch [522/1000], Loss: 283.9320, Train Acc: 44.64%\n",
      "Epoch [523/1000], Loss: 283.3711, Train Acc: 44.81%\n",
      "Epoch [524/1000], Loss: 282.2404, Train Acc: 44.54%\n",
      "Epoch [525/1000], Loss: 281.8448, Train Acc: 45.04%\n",
      "Epoch [526/1000], Loss: 282.2702, Train Acc: 45.08%\n",
      "Epoch [527/1000], Loss: 282.0728, Train Acc: 44.12%\n",
      "Epoch [528/1000], Loss: 282.8802, Train Acc: 44.37%\n",
      "Epoch [529/1000], Loss: 282.9746, Train Acc: 44.30%\n",
      "Epoch [530/1000], Loss: 282.7763, Train Acc: 44.90%\n",
      "Epoch [531/1000], Loss: 283.4353, Train Acc: 44.74%\n",
      "Epoch [532/1000], Loss: 284.2744, Train Acc: 44.44%\n",
      "Epoch [533/1000], Loss: 281.8318, Train Acc: 45.41%\n",
      "Epoch [534/1000], Loss: 282.5903, Train Acc: 44.74%\n",
      "Epoch [535/1000], Loss: 280.7912, Train Acc: 45.37%\n",
      "Epoch [536/1000], Loss: 282.5297, Train Acc: 45.27%\n",
      "Epoch [537/1000], Loss: 282.0398, Train Acc: 44.60%\n",
      "Epoch [538/1000], Loss: 283.0103, Train Acc: 44.73%\n",
      "Epoch [539/1000], Loss: 282.9898, Train Acc: 44.20%\n",
      "Epoch [540/1000], Loss: 285.0414, Train Acc: 43.46%\n",
      "Epoch [541/1000], Loss: 283.1659, Train Acc: 45.04%\n",
      "Epoch [542/1000], Loss: 284.3417, Train Acc: 44.53%\n",
      "Epoch [543/1000], Loss: 283.1074, Train Acc: 45.11%\n",
      "Epoch [544/1000], Loss: 283.1465, Train Acc: 45.08%\n",
      "Epoch [545/1000], Loss: 282.3838, Train Acc: 45.85%\n",
      "Epoch [546/1000], Loss: 282.9026, Train Acc: 44.96%\n",
      "Epoch [547/1000], Loss: 283.2286, Train Acc: 44.27%\n",
      "Epoch [548/1000], Loss: 280.6477, Train Acc: 45.41%\n",
      "Epoch [549/1000], Loss: 284.4254, Train Acc: 44.88%\n",
      "Epoch [550/1000], Loss: 280.9576, Train Acc: 45.95%\n",
      "Epoch [551/1000], Loss: 284.6045, Train Acc: 44.57%\n",
      "Epoch [552/1000], Loss: 281.9672, Train Acc: 44.47%\n",
      "Epoch [553/1000], Loss: 281.2895, Train Acc: 45.10%\n",
      "Epoch [554/1000], Loss: 282.5784, Train Acc: 44.88%\n",
      "Epoch [555/1000], Loss: 283.3117, Train Acc: 45.25%\n",
      "Epoch [556/1000], Loss: 282.6828, Train Acc: 44.94%\n",
      "Epoch [557/1000], Loss: 282.9699, Train Acc: 45.54%\n",
      "Epoch [558/1000], Loss: 283.0902, Train Acc: 44.53%\n",
      "Epoch [559/1000], Loss: 285.5211, Train Acc: 44.73%\n",
      "Epoch [560/1000], Loss: 281.4853, Train Acc: 45.11%\n",
      "Epoch [561/1000], Loss: 282.7114, Train Acc: 44.98%\n",
      "Epoch [562/1000], Loss: 283.3145, Train Acc: 44.76%\n",
      "Epoch [563/1000], Loss: 283.2666, Train Acc: 44.53%\n",
      "Epoch [564/1000], Loss: 280.7883, Train Acc: 45.14%\n",
      "Epoch [565/1000], Loss: 281.3714, Train Acc: 44.61%\n",
      "Epoch [566/1000], Loss: 283.9489, Train Acc: 44.59%\n",
      "Epoch [567/1000], Loss: 282.4597, Train Acc: 44.97%\n",
      "Epoch [568/1000], Loss: 282.0809, Train Acc: 45.35%\n",
      "Epoch [569/1000], Loss: 282.1780, Train Acc: 44.71%\n",
      "Epoch [570/1000], Loss: 283.4675, Train Acc: 44.57%\n",
      "Epoch [571/1000], Loss: 282.4228, Train Acc: 45.03%\n",
      "Epoch [572/1000], Loss: 282.9990, Train Acc: 45.58%\n",
      "Epoch [573/1000], Loss: 283.7270, Train Acc: 44.26%\n",
      "Epoch [574/1000], Loss: 282.4832, Train Acc: 45.03%\n",
      "Epoch [575/1000], Loss: 283.8558, Train Acc: 45.34%\n",
      "Epoch [576/1000], Loss: 281.4138, Train Acc: 45.94%\n",
      "Epoch [577/1000], Loss: 280.7805, Train Acc: 45.08%\n",
      "Epoch [578/1000], Loss: 281.6512, Train Acc: 44.63%\n",
      "Epoch [579/1000], Loss: 282.6191, Train Acc: 45.51%\n",
      "Epoch [580/1000], Loss: 282.1785, Train Acc: 45.03%\n",
      "Epoch [581/1000], Loss: 282.9832, Train Acc: 43.90%\n",
      "Epoch [582/1000], Loss: 281.3072, Train Acc: 45.50%\n",
      "Epoch [583/1000], Loss: 281.5684, Train Acc: 45.18%\n",
      "Epoch [584/1000], Loss: 283.4880, Train Acc: 45.08%\n",
      "Epoch [585/1000], Loss: 284.4724, Train Acc: 44.30%\n",
      "Epoch [586/1000], Loss: 283.3227, Train Acc: 45.30%\n",
      "Epoch [587/1000], Loss: 280.9605, Train Acc: 45.50%\n",
      "Epoch [588/1000], Loss: 283.0339, Train Acc: 44.57%\n",
      "Epoch [589/1000], Loss: 281.2453, Train Acc: 45.21%\n",
      "Epoch [590/1000], Loss: 282.3528, Train Acc: 44.70%\n",
      "Epoch [591/1000], Loss: 280.5725, Train Acc: 44.88%\n",
      "Epoch [592/1000], Loss: 281.3903, Train Acc: 44.73%\n",
      "Epoch [593/1000], Loss: 282.1968, Train Acc: 45.18%\n",
      "Epoch [594/1000], Loss: 282.1331, Train Acc: 45.21%\n",
      "Epoch [595/1000], Loss: 282.8264, Train Acc: 45.68%\n",
      "Epoch [596/1000], Loss: 279.9662, Train Acc: 45.01%\n",
      "Epoch [597/1000], Loss: 284.5390, Train Acc: 45.04%\n",
      "Epoch [598/1000], Loss: 283.8473, Train Acc: 45.08%\n",
      "Epoch [599/1000], Loss: 279.3207, Train Acc: 45.28%\n",
      "Epoch [600/1000], Loss: 282.3352, Train Acc: 45.15%\n",
      "Epoch [601/1000], Loss: 282.0010, Train Acc: 45.03%\n",
      "Epoch [602/1000], Loss: 282.1775, Train Acc: 44.40%\n",
      "Epoch [603/1000], Loss: 281.9036, Train Acc: 45.25%\n",
      "Epoch [604/1000], Loss: 283.1161, Train Acc: 44.47%\n",
      "Epoch [605/1000], Loss: 284.1045, Train Acc: 44.61%\n",
      "Epoch [606/1000], Loss: 281.5216, Train Acc: 45.13%\n",
      "Epoch [607/1000], Loss: 282.5688, Train Acc: 44.63%\n",
      "Epoch [608/1000], Loss: 282.7168, Train Acc: 45.00%\n",
      "Epoch [609/1000], Loss: 283.2861, Train Acc: 44.83%\n",
      "Epoch [610/1000], Loss: 282.9829, Train Acc: 45.31%\n",
      "Epoch [611/1000], Loss: 283.5843, Train Acc: 44.73%\n",
      "Epoch [612/1000], Loss: 282.7422, Train Acc: 44.20%\n",
      "Epoch [613/1000], Loss: 281.9750, Train Acc: 44.77%\n",
      "Epoch [614/1000], Loss: 280.1672, Train Acc: 45.92%\n",
      "Epoch [615/1000], Loss: 281.0906, Train Acc: 45.20%\n",
      "Epoch [616/1000], Loss: 279.0440, Train Acc: 44.59%\n",
      "Epoch [617/1000], Loss: 280.6549, Train Acc: 44.76%\n",
      "Epoch [618/1000], Loss: 282.7675, Train Acc: 45.31%\n",
      "Epoch [619/1000], Loss: 282.1206, Train Acc: 45.54%\n",
      "Epoch [620/1000], Loss: 281.7229, Train Acc: 45.48%\n",
      "Epoch [621/1000], Loss: 284.8339, Train Acc: 44.23%\n",
      "Epoch [622/1000], Loss: 281.9887, Train Acc: 44.61%\n",
      "Epoch [623/1000], Loss: 280.9627, Train Acc: 45.65%\n",
      "Epoch [624/1000], Loss: 282.3784, Train Acc: 44.71%\n",
      "Epoch [625/1000], Loss: 282.4660, Train Acc: 44.73%\n",
      "Epoch [626/1000], Loss: 283.9446, Train Acc: 44.36%\n",
      "Epoch [627/1000], Loss: 281.7052, Train Acc: 44.98%\n",
      "Epoch [628/1000], Loss: 282.9496, Train Acc: 45.14%\n",
      "Epoch [629/1000], Loss: 281.3270, Train Acc: 45.24%\n",
      "Epoch [630/1000], Loss: 282.9227, Train Acc: 44.34%\n",
      "Epoch [631/1000], Loss: 280.8183, Train Acc: 44.96%\n",
      "Epoch [632/1000], Loss: 282.9027, Train Acc: 44.41%\n",
      "Epoch [633/1000], Loss: 279.8191, Train Acc: 44.93%\n",
      "Epoch [634/1000], Loss: 281.4494, Train Acc: 46.18%\n",
      "Epoch [635/1000], Loss: 281.4944, Train Acc: 44.46%\n",
      "Epoch [636/1000], Loss: 281.4874, Train Acc: 45.00%\n",
      "Epoch [637/1000], Loss: 281.0279, Train Acc: 45.87%\n",
      "Epoch [638/1000], Loss: 280.8397, Train Acc: 44.74%\n",
      "Epoch [639/1000], Loss: 279.5558, Train Acc: 45.31%\n",
      "Epoch [640/1000], Loss: 282.6579, Train Acc: 44.44%\n",
      "Epoch [641/1000], Loss: 281.0384, Train Acc: 45.25%\n",
      "Epoch [642/1000], Loss: 280.7278, Train Acc: 45.35%\n",
      "Epoch [643/1000], Loss: 283.5914, Train Acc: 45.48%\n",
      "Epoch [644/1000], Loss: 283.6485, Train Acc: 44.20%\n",
      "Epoch [645/1000], Loss: 281.8494, Train Acc: 44.53%\n",
      "Epoch [646/1000], Loss: 281.4296, Train Acc: 45.07%\n",
      "Epoch [647/1000], Loss: 282.3972, Train Acc: 44.68%\n",
      "Epoch [648/1000], Loss: 281.6211, Train Acc: 44.61%\n",
      "Epoch [649/1000], Loss: 280.5542, Train Acc: 45.71%\n",
      "Epoch [650/1000], Loss: 281.3240, Train Acc: 44.97%\n",
      "Epoch [651/1000], Loss: 280.8269, Train Acc: 45.92%\n",
      "Epoch [652/1000], Loss: 279.8731, Train Acc: 45.24%\n",
      "Epoch [653/1000], Loss: 282.3452, Train Acc: 44.91%\n",
      "Epoch [654/1000], Loss: 283.2430, Train Acc: 45.48%\n",
      "Epoch [655/1000], Loss: 280.0403, Train Acc: 44.91%\n",
      "Epoch [656/1000], Loss: 281.6515, Train Acc: 45.04%\n",
      "Epoch [657/1000], Loss: 281.6564, Train Acc: 44.88%\n",
      "Epoch [658/1000], Loss: 283.0854, Train Acc: 44.63%\n",
      "Epoch [659/1000], Loss: 282.2089, Train Acc: 44.74%\n",
      "Epoch [660/1000], Loss: 280.7026, Train Acc: 45.44%\n",
      "Epoch [661/1000], Loss: 280.1377, Train Acc: 45.35%\n",
      "Epoch [662/1000], Loss: 281.9053, Train Acc: 45.37%\n",
      "Epoch [663/1000], Loss: 279.9463, Train Acc: 45.67%\n",
      "Epoch [664/1000], Loss: 281.3707, Train Acc: 45.64%\n",
      "Epoch [665/1000], Loss: 280.3757, Train Acc: 44.59%\n",
      "Epoch [666/1000], Loss: 281.7021, Train Acc: 44.87%\n",
      "Epoch [667/1000], Loss: 281.3586, Train Acc: 44.61%\n",
      "Epoch [668/1000], Loss: 280.5387, Train Acc: 45.24%\n",
      "Epoch [669/1000], Loss: 283.9623, Train Acc: 44.20%\n",
      "Epoch [670/1000], Loss: 280.0998, Train Acc: 45.25%\n",
      "Epoch [671/1000], Loss: 281.0518, Train Acc: 44.96%\n",
      "Epoch [672/1000], Loss: 281.6715, Train Acc: 45.65%\n",
      "Epoch [673/1000], Loss: 282.7996, Train Acc: 44.84%\n",
      "Epoch [674/1000], Loss: 282.6839, Train Acc: 44.77%\n",
      "Epoch [675/1000], Loss: 281.5607, Train Acc: 44.53%\n",
      "Epoch [676/1000], Loss: 279.8314, Train Acc: 44.49%\n",
      "Epoch [677/1000], Loss: 281.7248, Train Acc: 44.76%\n",
      "Epoch [678/1000], Loss: 278.9149, Train Acc: 46.24%\n",
      "Epoch [679/1000], Loss: 280.4627, Train Acc: 44.81%\n",
      "Epoch [680/1000], Loss: 280.6383, Train Acc: 45.25%\n",
      "Epoch [681/1000], Loss: 278.3660, Train Acc: 45.20%\n",
      "Epoch [682/1000], Loss: 281.2241, Train Acc: 45.15%\n",
      "Epoch [683/1000], Loss: 282.0984, Train Acc: 44.22%\n",
      "Epoch [684/1000], Loss: 283.9097, Train Acc: 45.01%\n",
      "Epoch [685/1000], Loss: 279.4547, Train Acc: 45.41%\n",
      "Epoch [686/1000], Loss: 283.5192, Train Acc: 45.18%\n",
      "Epoch [687/1000], Loss: 280.8274, Train Acc: 44.39%\n",
      "Epoch [688/1000], Loss: 280.1047, Train Acc: 45.79%\n",
      "Epoch [689/1000], Loss: 281.2060, Train Acc: 45.07%\n",
      "Epoch [690/1000], Loss: 281.7050, Train Acc: 44.84%\n",
      "Epoch [691/1000], Loss: 281.8646, Train Acc: 45.18%\n",
      "Epoch [692/1000], Loss: 280.3180, Train Acc: 45.38%\n",
      "Epoch [693/1000], Loss: 280.9057, Train Acc: 45.81%\n",
      "Epoch [694/1000], Loss: 282.5614, Train Acc: 44.91%\n",
      "Epoch [695/1000], Loss: 280.7484, Train Acc: 45.35%\n",
      "Epoch [696/1000], Loss: 280.3903, Train Acc: 45.24%\n",
      "Epoch [697/1000], Loss: 280.1924, Train Acc: 45.58%\n",
      "Epoch [698/1000], Loss: 280.1232, Train Acc: 45.42%\n",
      "Epoch [699/1000], Loss: 282.1934, Train Acc: 44.57%\n",
      "Epoch [700/1000], Loss: 280.9115, Train Acc: 44.56%\n",
      "Epoch [701/1000], Loss: 279.4452, Train Acc: 44.93%\n",
      "Epoch [702/1000], Loss: 281.8371, Train Acc: 45.13%\n",
      "Epoch [703/1000], Loss: 282.7738, Train Acc: 45.20%\n",
      "Epoch [704/1000], Loss: 282.2227, Train Acc: 44.96%\n",
      "Epoch [705/1000], Loss: 284.1263, Train Acc: 44.80%\n",
      "Epoch [706/1000], Loss: 279.8607, Train Acc: 45.54%\n",
      "Epoch [707/1000], Loss: 281.6047, Train Acc: 45.37%\n",
      "Epoch [708/1000], Loss: 280.3798, Train Acc: 45.62%\n",
      "Epoch [709/1000], Loss: 280.4263, Train Acc: 44.77%\n",
      "Epoch [710/1000], Loss: 281.9495, Train Acc: 45.52%\n",
      "Epoch [711/1000], Loss: 281.2789, Train Acc: 44.41%\n",
      "Epoch [712/1000], Loss: 281.4835, Train Acc: 45.15%\n",
      "Epoch [713/1000], Loss: 281.6923, Train Acc: 44.63%\n",
      "Epoch [714/1000], Loss: 281.5201, Train Acc: 45.48%\n",
      "Epoch [715/1000], Loss: 280.5281, Train Acc: 45.08%\n",
      "Epoch [716/1000], Loss: 279.1172, Train Acc: 45.64%\n",
      "Epoch [717/1000], Loss: 279.9987, Train Acc: 45.33%\n",
      "Epoch [718/1000], Loss: 281.0570, Train Acc: 45.23%\n",
      "Epoch [719/1000], Loss: 281.1890, Train Acc: 45.28%\n",
      "Epoch [720/1000], Loss: 283.5990, Train Acc: 44.87%\n",
      "Epoch [721/1000], Loss: 280.5235, Train Acc: 45.23%\n",
      "Epoch [722/1000], Loss: 282.2029, Train Acc: 44.88%\n",
      "Epoch [723/1000], Loss: 280.0521, Train Acc: 45.25%\n",
      "Epoch [724/1000], Loss: 281.6187, Train Acc: 43.87%\n",
      "Epoch [725/1000], Loss: 280.3045, Train Acc: 44.96%\n",
      "Epoch [726/1000], Loss: 280.1961, Train Acc: 45.20%\n",
      "Epoch [727/1000], Loss: 279.5049, Train Acc: 45.23%\n",
      "Epoch [728/1000], Loss: 280.1346, Train Acc: 45.33%\n",
      "Epoch [729/1000], Loss: 283.0525, Train Acc: 44.96%\n",
      "Epoch [730/1000], Loss: 280.1736, Train Acc: 45.70%\n",
      "Epoch [731/1000], Loss: 282.2984, Train Acc: 44.27%\n",
      "Epoch [732/1000], Loss: 282.3303, Train Acc: 44.68%\n",
      "Epoch [733/1000], Loss: 280.5596, Train Acc: 45.23%\n",
      "Epoch [734/1000], Loss: 280.7818, Train Acc: 45.03%\n",
      "Epoch [735/1000], Loss: 280.1533, Train Acc: 45.14%\n",
      "Epoch [736/1000], Loss: 279.6202, Train Acc: 45.41%\n",
      "Epoch [737/1000], Loss: 280.0273, Train Acc: 45.57%\n",
      "Epoch [738/1000], Loss: 280.6479, Train Acc: 45.40%\n",
      "Epoch [739/1000], Loss: 280.8204, Train Acc: 45.20%\n",
      "Epoch [740/1000], Loss: 281.3480, Train Acc: 45.37%\n",
      "Epoch [741/1000], Loss: 281.3214, Train Acc: 45.34%\n",
      "Epoch [742/1000], Loss: 282.2366, Train Acc: 44.60%\n",
      "Epoch [743/1000], Loss: 279.6268, Train Acc: 45.38%\n",
      "Epoch [744/1000], Loss: 281.9195, Train Acc: 45.71%\n",
      "Epoch [745/1000], Loss: 279.6229, Train Acc: 45.61%\n",
      "Epoch [746/1000], Loss: 279.7369, Train Acc: 45.62%\n",
      "Epoch [747/1000], Loss: 280.9932, Train Acc: 44.88%\n",
      "Epoch [748/1000], Loss: 278.7053, Train Acc: 45.70%\n",
      "Epoch [749/1000], Loss: 280.2966, Train Acc: 45.10%\n",
      "Epoch [750/1000], Loss: 279.8769, Train Acc: 45.05%\n",
      "Epoch [751/1000], Loss: 280.1577, Train Acc: 45.62%\n",
      "Epoch [752/1000], Loss: 281.7263, Train Acc: 45.05%\n",
      "Epoch [753/1000], Loss: 278.8033, Train Acc: 45.18%\n",
      "Epoch [754/1000], Loss: 279.0292, Train Acc: 45.47%\n",
      "Epoch [755/1000], Loss: 280.2785, Train Acc: 45.15%\n",
      "Epoch [756/1000], Loss: 279.9803, Train Acc: 45.58%\n",
      "Epoch [757/1000], Loss: 279.6382, Train Acc: 45.11%\n",
      "Epoch [758/1000], Loss: 279.0897, Train Acc: 44.63%\n",
      "Epoch [759/1000], Loss: 281.5482, Train Acc: 44.97%\n",
      "Epoch [760/1000], Loss: 281.0573, Train Acc: 45.25%\n",
      "Epoch [761/1000], Loss: 280.8707, Train Acc: 45.00%\n",
      "Epoch [762/1000], Loss: 280.6877, Train Acc: 45.18%\n",
      "Epoch [763/1000], Loss: 279.3215, Train Acc: 45.50%\n",
      "Epoch [764/1000], Loss: 279.4646, Train Acc: 45.40%\n",
      "Epoch [765/1000], Loss: 281.7822, Train Acc: 45.52%\n",
      "Epoch [766/1000], Loss: 281.1426, Train Acc: 45.34%\n",
      "Epoch [767/1000], Loss: 283.1559, Train Acc: 45.15%\n",
      "Epoch [768/1000], Loss: 279.3584, Train Acc: 45.84%\n",
      "Epoch [769/1000], Loss: 278.7872, Train Acc: 46.01%\n",
      "Epoch [770/1000], Loss: 281.8840, Train Acc: 45.13%\n",
      "Epoch [771/1000], Loss: 278.1737, Train Acc: 46.12%\n",
      "Epoch [772/1000], Loss: 280.6533, Train Acc: 45.85%\n",
      "Epoch [773/1000], Loss: 278.3104, Train Acc: 45.65%\n",
      "Epoch [774/1000], Loss: 281.0119, Train Acc: 45.61%\n",
      "Epoch [775/1000], Loss: 279.0332, Train Acc: 45.77%\n",
      "Epoch [776/1000], Loss: 281.2653, Train Acc: 44.83%\n",
      "Epoch [777/1000], Loss: 279.5367, Train Acc: 45.44%\n",
      "Epoch [778/1000], Loss: 279.8073, Train Acc: 45.24%\n",
      "Epoch [779/1000], Loss: 278.3101, Train Acc: 45.01%\n",
      "Epoch [780/1000], Loss: 282.2545, Train Acc: 45.81%\n",
      "Epoch [781/1000], Loss: 281.3153, Train Acc: 45.31%\n",
      "Epoch [782/1000], Loss: 278.3910, Train Acc: 45.37%\n",
      "Epoch [783/1000], Loss: 279.8309, Train Acc: 45.57%\n",
      "Epoch [784/1000], Loss: 282.1768, Train Acc: 44.63%\n",
      "Epoch [785/1000], Loss: 279.5898, Train Acc: 45.21%\n",
      "Epoch [786/1000], Loss: 279.5409, Train Acc: 45.24%\n",
      "Epoch [787/1000], Loss: 281.6968, Train Acc: 44.96%\n",
      "Epoch [788/1000], Loss: 278.1952, Train Acc: 45.50%\n",
      "Epoch [789/1000], Loss: 279.7153, Train Acc: 45.64%\n",
      "Epoch [790/1000], Loss: 279.8547, Train Acc: 45.60%\n",
      "Epoch [791/1000], Loss: 277.8357, Train Acc: 44.93%\n",
      "Epoch [792/1000], Loss: 278.5752, Train Acc: 45.28%\n",
      "Epoch [793/1000], Loss: 282.4986, Train Acc: 45.14%\n",
      "Epoch [794/1000], Loss: 279.6440, Train Acc: 45.51%\n",
      "Epoch [795/1000], Loss: 279.3427, Train Acc: 44.78%\n",
      "Epoch [796/1000], Loss: 281.4931, Train Acc: 45.17%\n",
      "Epoch [797/1000], Loss: 278.7622, Train Acc: 46.28%\n",
      "Epoch [798/1000], Loss: 278.9484, Train Acc: 45.84%\n",
      "Epoch [799/1000], Loss: 280.5272, Train Acc: 45.60%\n",
      "Epoch [800/1000], Loss: 279.5028, Train Acc: 45.70%\n",
      "Epoch [801/1000], Loss: 278.8827, Train Acc: 46.38%\n",
      "Epoch [802/1000], Loss: 280.1647, Train Acc: 43.99%\n",
      "Epoch [803/1000], Loss: 279.2251, Train Acc: 45.50%\n",
      "Epoch [804/1000], Loss: 281.9232, Train Acc: 44.34%\n",
      "Epoch [805/1000], Loss: 277.9876, Train Acc: 46.26%\n",
      "Epoch [806/1000], Loss: 281.3487, Train Acc: 44.74%\n",
      "Epoch [807/1000], Loss: 280.0085, Train Acc: 44.96%\n",
      "Epoch [808/1000], Loss: 277.7288, Train Acc: 46.36%\n",
      "Epoch [809/1000], Loss: 280.1375, Train Acc: 45.97%\n",
      "Epoch [810/1000], Loss: 279.5713, Train Acc: 45.47%\n",
      "Epoch [811/1000], Loss: 279.0736, Train Acc: 46.24%\n",
      "Epoch [812/1000], Loss: 279.1758, Train Acc: 45.35%\n",
      "Epoch [813/1000], Loss: 279.5023, Train Acc: 45.00%\n",
      "Epoch [814/1000], Loss: 281.1382, Train Acc: 45.11%\n",
      "Epoch [815/1000], Loss: 281.8008, Train Acc: 45.15%\n",
      "Epoch [816/1000], Loss: 281.5282, Train Acc: 44.88%\n",
      "Epoch [817/1000], Loss: 278.6507, Train Acc: 45.62%\n",
      "Epoch [818/1000], Loss: 279.1537, Train Acc: 44.87%\n",
      "Epoch [819/1000], Loss: 279.3685, Train Acc: 45.55%\n",
      "Epoch [820/1000], Loss: 280.1545, Train Acc: 45.72%\n",
      "Epoch [821/1000], Loss: 279.0505, Train Acc: 45.55%\n",
      "Epoch [822/1000], Loss: 277.9099, Train Acc: 45.94%\n",
      "Epoch [823/1000], Loss: 279.2927, Train Acc: 46.24%\n",
      "Epoch [824/1000], Loss: 279.9981, Train Acc: 44.91%\n",
      "Epoch [825/1000], Loss: 280.1824, Train Acc: 45.00%\n",
      "Epoch [826/1000], Loss: 278.6352, Train Acc: 45.15%\n",
      "Epoch [827/1000], Loss: 279.6209, Train Acc: 45.95%\n",
      "Epoch [828/1000], Loss: 282.1640, Train Acc: 45.47%\n",
      "Epoch [829/1000], Loss: 278.1152, Train Acc: 45.84%\n",
      "Epoch [830/1000], Loss: 278.3180, Train Acc: 46.01%\n",
      "Epoch [831/1000], Loss: 280.3431, Train Acc: 46.25%\n",
      "Epoch [832/1000], Loss: 278.7518, Train Acc: 45.87%\n",
      "Epoch [833/1000], Loss: 278.3604, Train Acc: 45.35%\n",
      "Epoch [834/1000], Loss: 279.3962, Train Acc: 45.00%\n",
      "Epoch [835/1000], Loss: 279.3331, Train Acc: 45.13%\n",
      "Epoch [836/1000], Loss: 277.9254, Train Acc: 45.55%\n",
      "Epoch [837/1000], Loss: 278.9734, Train Acc: 45.25%\n",
      "Epoch [838/1000], Loss: 280.6032, Train Acc: 44.96%\n",
      "Epoch [839/1000], Loss: 279.5814, Train Acc: 45.87%\n",
      "Epoch [840/1000], Loss: 280.7575, Train Acc: 44.31%\n",
      "Epoch [841/1000], Loss: 278.2019, Train Acc: 45.52%\n",
      "Epoch [842/1000], Loss: 279.8996, Train Acc: 45.35%\n",
      "Epoch [843/1000], Loss: 280.2246, Train Acc: 45.07%\n",
      "Epoch [844/1000], Loss: 279.6965, Train Acc: 45.41%\n",
      "Epoch [845/1000], Loss: 281.8269, Train Acc: 45.42%\n",
      "Epoch [846/1000], Loss: 278.3384, Train Acc: 45.25%\n",
      "Epoch [847/1000], Loss: 280.9014, Train Acc: 45.79%\n",
      "Epoch [848/1000], Loss: 281.3526, Train Acc: 45.94%\n",
      "Epoch [849/1000], Loss: 279.6036, Train Acc: 45.33%\n",
      "Epoch [850/1000], Loss: 281.4854, Train Acc: 45.18%\n",
      "Epoch [851/1000], Loss: 278.9446, Train Acc: 45.52%\n",
      "Epoch [852/1000], Loss: 281.3382, Train Acc: 44.53%\n",
      "Epoch [853/1000], Loss: 277.8959, Train Acc: 45.47%\n",
      "Epoch [854/1000], Loss: 280.5240, Train Acc: 46.01%\n",
      "Epoch [855/1000], Loss: 280.3985, Train Acc: 45.08%\n",
      "Epoch [856/1000], Loss: 279.7088, Train Acc: 45.99%\n",
      "Epoch [857/1000], Loss: 281.2791, Train Acc: 45.01%\n",
      "Epoch [858/1000], Loss: 280.3238, Train Acc: 45.41%\n",
      "Epoch [859/1000], Loss: 278.2513, Train Acc: 45.85%\n",
      "Epoch [860/1000], Loss: 279.0311, Train Acc: 45.65%\n",
      "Epoch [861/1000], Loss: 279.0278, Train Acc: 46.09%\n",
      "Epoch [862/1000], Loss: 278.1779, Train Acc: 45.44%\n",
      "Epoch [863/1000], Loss: 280.8668, Train Acc: 45.24%\n",
      "Epoch [864/1000], Loss: 279.3338, Train Acc: 44.78%\n",
      "Epoch [865/1000], Loss: 281.0452, Train Acc: 45.17%\n",
      "Epoch [866/1000], Loss: 278.5603, Train Acc: 45.55%\n",
      "Epoch [867/1000], Loss: 277.9933, Train Acc: 45.54%\n",
      "Epoch [868/1000], Loss: 280.7385, Train Acc: 45.01%\n",
      "Epoch [869/1000], Loss: 277.9480, Train Acc: 45.41%\n",
      "Epoch [870/1000], Loss: 279.9506, Train Acc: 45.14%\n",
      "Epoch [871/1000], Loss: 280.6565, Train Acc: 44.70%\n",
      "Epoch [872/1000], Loss: 278.7795, Train Acc: 46.26%\n",
      "Epoch [873/1000], Loss: 281.9531, Train Acc: 45.23%\n",
      "Epoch [874/1000], Loss: 278.5109, Train Acc: 45.34%\n",
      "Epoch [875/1000], Loss: 279.8282, Train Acc: 45.40%\n",
      "Epoch [876/1000], Loss: 279.6466, Train Acc: 45.77%\n",
      "Epoch [877/1000], Loss: 281.0699, Train Acc: 45.61%\n",
      "Epoch [878/1000], Loss: 281.1166, Train Acc: 45.15%\n",
      "Epoch [879/1000], Loss: 281.7288, Train Acc: 45.44%\n",
      "Epoch [880/1000], Loss: 279.0390, Train Acc: 45.31%\n",
      "Epoch [881/1000], Loss: 278.9426, Train Acc: 45.21%\n",
      "Epoch [882/1000], Loss: 277.7681, Train Acc: 46.18%\n",
      "Epoch [883/1000], Loss: 279.5829, Train Acc: 45.44%\n",
      "Epoch [884/1000], Loss: 276.6267, Train Acc: 46.31%\n",
      "Epoch [885/1000], Loss: 280.5254, Train Acc: 45.17%\n",
      "Epoch [886/1000], Loss: 279.2688, Train Acc: 44.70%\n",
      "Epoch [887/1000], Loss: 278.1016, Train Acc: 45.87%\n",
      "Epoch [888/1000], Loss: 280.1867, Train Acc: 46.01%\n",
      "Epoch [889/1000], Loss: 280.5727, Train Acc: 46.04%\n",
      "Epoch [890/1000], Loss: 281.6769, Train Acc: 45.27%\n",
      "Epoch [891/1000], Loss: 278.8298, Train Acc: 44.80%\n",
      "Epoch [892/1000], Loss: 278.6701, Train Acc: 46.14%\n",
      "Epoch [893/1000], Loss: 279.7798, Train Acc: 46.04%\n",
      "Epoch [894/1000], Loss: 281.5536, Train Acc: 45.11%\n",
      "Epoch [895/1000], Loss: 277.9419, Train Acc: 45.41%\n",
      "Epoch [896/1000], Loss: 280.5188, Train Acc: 44.96%\n",
      "Epoch [897/1000], Loss: 277.4025, Train Acc: 46.08%\n",
      "Epoch [898/1000], Loss: 277.2045, Train Acc: 45.75%\n",
      "Epoch [899/1000], Loss: 279.8379, Train Acc: 45.25%\n",
      "Epoch [900/1000], Loss: 277.7397, Train Acc: 45.50%\n",
      "Epoch [901/1000], Loss: 279.2094, Train Acc: 45.04%\n",
      "Epoch [902/1000], Loss: 279.3108, Train Acc: 45.58%\n",
      "Epoch [903/1000], Loss: 279.2145, Train Acc: 45.35%\n",
      "Epoch [904/1000], Loss: 276.6637, Train Acc: 45.57%\n",
      "Epoch [905/1000], Loss: 280.9468, Train Acc: 45.10%\n",
      "Epoch [906/1000], Loss: 279.8822, Train Acc: 45.50%\n",
      "Epoch [907/1000], Loss: 279.1982, Train Acc: 45.54%\n",
      "Epoch [908/1000], Loss: 280.2307, Train Acc: 46.15%\n",
      "Epoch [909/1000], Loss: 278.2797, Train Acc: 46.07%\n",
      "Epoch [910/1000], Loss: 276.5713, Train Acc: 45.68%\n",
      "Epoch [911/1000], Loss: 278.2419, Train Acc: 46.31%\n",
      "Epoch [912/1000], Loss: 279.8432, Train Acc: 45.64%\n",
      "Epoch [913/1000], Loss: 279.6514, Train Acc: 45.41%\n",
      "Epoch [914/1000], Loss: 278.8501, Train Acc: 45.07%\n",
      "Epoch [915/1000], Loss: 277.5969, Train Acc: 45.71%\n",
      "Epoch [916/1000], Loss: 279.8534, Train Acc: 44.93%\n",
      "Epoch [917/1000], Loss: 278.3714, Train Acc: 45.05%\n",
      "Epoch [918/1000], Loss: 279.4029, Train Acc: 45.18%\n",
      "Epoch [919/1000], Loss: 279.6493, Train Acc: 45.54%\n",
      "Epoch [920/1000], Loss: 279.8140, Train Acc: 45.10%\n",
      "Epoch [921/1000], Loss: 279.9420, Train Acc: 45.72%\n",
      "Epoch [922/1000], Loss: 279.8437, Train Acc: 45.23%\n",
      "Epoch [923/1000], Loss: 278.7371, Train Acc: 45.74%\n",
      "Epoch [924/1000], Loss: 279.3098, Train Acc: 45.77%\n",
      "Epoch [925/1000], Loss: 278.8346, Train Acc: 45.24%\n",
      "Epoch [926/1000], Loss: 279.1536, Train Acc: 45.94%\n",
      "Epoch [927/1000], Loss: 279.3975, Train Acc: 45.51%\n",
      "Epoch [928/1000], Loss: 277.9762, Train Acc: 45.97%\n",
      "Epoch [929/1000], Loss: 278.8204, Train Acc: 45.55%\n",
      "Epoch [930/1000], Loss: 279.6384, Train Acc: 45.97%\n",
      "Epoch [931/1000], Loss: 280.5496, Train Acc: 45.00%\n",
      "Epoch [932/1000], Loss: 275.9737, Train Acc: 45.82%\n",
      "Epoch [933/1000], Loss: 278.8847, Train Acc: 45.78%\n",
      "Epoch [934/1000], Loss: 277.2890, Train Acc: 46.04%\n",
      "Epoch [935/1000], Loss: 281.3472, Train Acc: 45.55%\n",
      "Epoch [936/1000], Loss: 281.2276, Train Acc: 44.57%\n",
      "Epoch [937/1000], Loss: 277.7953, Train Acc: 45.84%\n",
      "Epoch [938/1000], Loss: 277.3987, Train Acc: 46.44%\n",
      "Epoch [939/1000], Loss: 276.2446, Train Acc: 45.74%\n",
      "Epoch [940/1000], Loss: 280.0791, Train Acc: 45.75%\n",
      "Epoch [941/1000], Loss: 278.0957, Train Acc: 45.55%\n",
      "Epoch [942/1000], Loss: 279.7869, Train Acc: 44.97%\n",
      "Epoch [943/1000], Loss: 277.0239, Train Acc: 45.85%\n",
      "Epoch [944/1000], Loss: 276.8686, Train Acc: 45.30%\n",
      "Epoch [945/1000], Loss: 277.0703, Train Acc: 45.57%\n",
      "Epoch [946/1000], Loss: 277.3071, Train Acc: 45.72%\n",
      "Epoch [947/1000], Loss: 278.7900, Train Acc: 45.41%\n",
      "Epoch [948/1000], Loss: 278.8204, Train Acc: 45.52%\n",
      "Epoch [949/1000], Loss: 279.2917, Train Acc: 45.55%\n",
      "Epoch [950/1000], Loss: 279.2775, Train Acc: 46.01%\n",
      "Epoch [951/1000], Loss: 280.6124, Train Acc: 44.66%\n",
      "Epoch [952/1000], Loss: 278.8293, Train Acc: 45.30%\n",
      "Epoch [953/1000], Loss: 279.6193, Train Acc: 45.17%\n",
      "Epoch [954/1000], Loss: 275.2750, Train Acc: 46.12%\n",
      "Epoch [955/1000], Loss: 276.7959, Train Acc: 46.16%\n",
      "Epoch [956/1000], Loss: 278.2606, Train Acc: 45.33%\n",
      "Epoch [957/1000], Loss: 278.3151, Train Acc: 45.45%\n",
      "Epoch [958/1000], Loss: 279.1858, Train Acc: 44.88%\n",
      "Epoch [959/1000], Loss: 279.5015, Train Acc: 45.55%\n",
      "Epoch [960/1000], Loss: 280.7004, Train Acc: 45.41%\n",
      "Epoch [961/1000], Loss: 279.3577, Train Acc: 45.01%\n",
      "Epoch [962/1000], Loss: 276.4429, Train Acc: 45.14%\n",
      "Epoch [963/1000], Loss: 278.8111, Train Acc: 45.81%\n",
      "Epoch [964/1000], Loss: 277.6486, Train Acc: 46.09%\n",
      "Epoch [965/1000], Loss: 280.9124, Train Acc: 45.91%\n",
      "Epoch [966/1000], Loss: 278.8705, Train Acc: 46.31%\n",
      "Epoch [967/1000], Loss: 278.8356, Train Acc: 46.59%\n",
      "Epoch [968/1000], Loss: 278.4502, Train Acc: 46.32%\n",
      "Epoch [969/1000], Loss: 275.6665, Train Acc: 46.22%\n",
      "Epoch [970/1000], Loss: 280.3145, Train Acc: 46.14%\n",
      "Epoch [971/1000], Loss: 278.2361, Train Acc: 46.31%\n",
      "Epoch [972/1000], Loss: 276.8700, Train Acc: 46.22%\n",
      "Epoch [973/1000], Loss: 279.3142, Train Acc: 44.83%\n",
      "Epoch [974/1000], Loss: 278.4537, Train Acc: 45.42%\n",
      "Epoch [975/1000], Loss: 278.1217, Train Acc: 45.17%\n",
      "Epoch [976/1000], Loss: 280.4511, Train Acc: 45.01%\n",
      "Epoch [977/1000], Loss: 278.5003, Train Acc: 45.55%\n",
      "Epoch [978/1000], Loss: 276.4620, Train Acc: 46.15%\n",
      "Epoch [979/1000], Loss: 278.2281, Train Acc: 45.27%\n",
      "Epoch [980/1000], Loss: 279.2537, Train Acc: 45.85%\n",
      "Epoch [981/1000], Loss: 277.3661, Train Acc: 46.29%\n",
      "Epoch [982/1000], Loss: 279.4014, Train Acc: 46.15%\n",
      "Epoch [983/1000], Loss: 276.9378, Train Acc: 45.98%\n",
      "Epoch [984/1000], Loss: 279.6724, Train Acc: 45.11%\n",
      "Epoch [985/1000], Loss: 275.9146, Train Acc: 45.60%\n",
      "Epoch [986/1000], Loss: 276.0011, Train Acc: 46.05%\n",
      "Epoch [987/1000], Loss: 277.5746, Train Acc: 45.14%\n",
      "Epoch [988/1000], Loss: 280.7960, Train Acc: 45.01%\n",
      "Epoch [989/1000], Loss: 277.1804, Train Acc: 46.34%\n",
      "Epoch [990/1000], Loss: 277.9523, Train Acc: 46.29%\n",
      "Epoch [991/1000], Loss: 277.9163, Train Acc: 46.45%\n",
      "Epoch [992/1000], Loss: 280.6585, Train Acc: 45.33%\n",
      "Epoch [993/1000], Loss: 279.8619, Train Acc: 46.61%\n",
      "Epoch [994/1000], Loss: 279.2151, Train Acc: 45.84%\n",
      "Epoch [995/1000], Loss: 278.1917, Train Acc: 46.02%\n",
      "Epoch [996/1000], Loss: 278.5922, Train Acc: 45.70%\n",
      "Epoch [997/1000], Loss: 277.7807, Train Acc: 45.47%\n",
      "Epoch [998/1000], Loss: 278.6387, Train Acc: 45.68%\n",
      "Epoch [999/1000], Loss: 277.3763, Train Acc: 45.58%\n",
      "Epoch [1000/1000], Loss: 279.4413, Train Acc: 45.64%\n",
      "\n",
      "Neural Network Test Accuracy: 45.42%\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "                                                                                            # 1. Load & preprocess data\n",
    "df = pd.read_csv(\"weather.csv\")   # your file\n",
    "df['Weather'].unique()\n",
    "df=df.drop('Date/Time', axis=1)\n",
    "# Features (X) and Target (y)\n",
    "X = df.drop(\"Weather\", axis=1)\n",
    "y = df[\"Weather\"]\n",
    "\n",
    "                                                                                            # Encode target labels\n",
    "le = LabelEncoder()\n",
    "y = le.fit_transform(y)\n",
    "\n",
    "                                                                # Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "                                                                                # Normalize features\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.long)\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.long)\n",
    "\n",
    "# Dataset & DataLoader\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "input_dim = X_train.shape[1]\n",
    "num_classes = len(np.unique(y))\n",
    "\n",
    "                                                            #2. Define Neural Network\n",
    "\n",
    "class WeatherNN(nn.Module):\n",
    "    def __init__(self, input_dim, num_classes):\n",
    "        super(WeatherNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, 128)\n",
    "        self.dropout1 = nn.Dropout(0.3)\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.dropout2 = nn.Dropout(0.3)\n",
    "        self.fc3 = nn.Linear(64, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.dropout1(x)\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = self.dropout2(x)\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "model = WeatherNN(input_dim, num_classes)\n",
    "\n",
    "\n",
    "                                                                                    # 3. Training setup\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "\n",
    "                                                                                            # 4. Train the model\n",
    "\n",
    "epochs = 1000\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct, total = 0, 0\n",
    "\n",
    "    for inputs, labels in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "    train_acc = 100 * correct / total\n",
    "    print(f\"Epoch [{epoch+1}/{epochs}], Loss: {running_loss:.4f}, Train Acc: {train_acc:.2f}%\")\n",
    "\n",
    "\n",
    "#                                                                  5. Evaluate on test set\n",
    "model.eval()\n",
    "correct, total = 0, 0\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_loader:\n",
    "        outputs = model(inputs)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "test_acc = 100 * correct / total\n",
    "print(f\"\\nNeural Network Test Accuracy: {test_acc:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2fa80adf-6eef-4250-a32f-7f2d801a7726",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter value for Temp_C:  3\n",
      "Enter value for Dew Point Temp_C:  2\n",
      "Enter value for Rel Hum_%:  3\n",
      "Enter value for Wind Speed_km/h:  4\n",
      "Enter value for Visibility_km:  2\n",
      "Enter value for Press_kPa:  4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Predicted Weather: Cloudy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ASUS\\anaconda3\\envs\\Env2\\Lib\\site-packages\\sklearn\\utils\\validation.py:2749: UserWarning: X does not have valid feature names, but StandardScaler was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def get_user_input(columns):\n",
    "    values = []\n",
    "    for col in columns:\n",
    "        val = float(input(f\"Enter value for {col}: \"))\n",
    "        values.append(val)\n",
    "    return values\n",
    "\n",
    "feature_names = X.columns.tolist()   \n",
    "user_input = get_user_input(feature_names)\n",
    "\n",
    "prediction = predict_weather(model, scaler, le, user_input)\n",
    "print(\"\\nPredicted Weather:\", prediction)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b319bd6c-b0bf-439e-8052-e189a91abcfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.8.0+cu128\n",
      "CUDA available: True\n",
      "CUDA version: 12.8\n",
      "GPU name: NVIDIA GeForce RTX 4080 Laptop GPU\n",
      "Number of GPUs: 1\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Print PyTorch version\n",
    "print(\"PyTorch version:\", torch.__version__)\n",
    "\n",
    "# Check if CUDA is available\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "\n",
    "# If CUDA is available, print additional GPU info\n",
    "if torch.cuda.is_available():\n",
    "    print(\"CUDA version:\", torch.version.cuda)\n",
    "    print(\"GPU name:\", torch.cuda.get_device_name(0))\n",
    "    print(\"Number of GPUs:\", torch.cuda.device_count())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1576ec69-93af-4ea6-b9fb-01435120d842",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54865809-60ce-48bf-8f76-6664503faa0e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1371b32-2669-4e7d-a275-b506e90337d7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4a6812b0-5545-40e0-ba50-e4d534a48b7e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-10-12 23:22:45,070] A new study created in memory with name: no-name-0366514a-cc6e-46a6-85d5-724e9ee760bd\n",
      "[I 2025-10-12 23:23:42,513] Trial 0 finished with value: 1.4600878455422142 and parameters: {'hidden1': 168, 'hidden2': 81, 'dropout': 0.36450091684090624, 'lr': 0.0008887151264468703, 'batch_size': 32}. Best is trial 0 with value: 1.4600878455422142.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 46\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-10-12 23:24:06,496] Trial 1 finished with value: 1.4501101161752428 and parameters: {'hidden1': 174, 'hidden2': 72, 'dropout': 0.2322699289707614, 'lr': 0.0073372100933846345, 'batch_size': 64}. Best is trial 1 with value: 1.4501101161752428.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-10-12 23:24:50,051] Trial 2 finished with value: 1.4995673353021794 and parameters: {'hidden1': 103, 'hidden2': 52, 'dropout': 0.46028772916610855, 'lr': 0.005747587671095482, 'batch_size': 32}. Best is trial 1 with value: 1.4501101161752428.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 37\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-10-12 23:25:43,223] Trial 3 finished with value: 1.4304774837060408 and parameters: {'hidden1': 70, 'hidden2': 121, 'dropout': 0.22199350081066702, 'lr': 0.002236705184188391, 'batch_size': 32}. Best is trial 3 with value: 1.4304774837060408.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 43\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-10-12 23:27:18,210] Trial 4 finished with value: 1.4783275365829467 and parameters: {'hidden1': 78, 'hidden2': 109, 'dropout': 0.2930481048936958, 'lr': 0.0002955210725592955, 'batch_size': 32}. Best is trial 3 with value: 1.4304774837060408.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 80\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-10-12 23:28:24,606] Trial 5 finished with value: 1.4668972318822688 and parameters: {'hidden1': 134, 'hidden2': 119, 'dropout': 0.5054446163028221, 'lr': 0.0009033277341191069, 'batch_size': 32}. Best is trial 3 with value: 1.4304774837060408.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 55\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-10-12 23:29:48,050] Trial 6 finished with value: 1.4525887576016512 and parameters: {'hidden1': 184, 'hidden2': 113, 'dropout': 0.39966311827402834, 'lr': 0.0009272103763626449, 'batch_size': 32}. Best is trial 3 with value: 1.4304774837060408.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 60\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-10-12 23:30:31,576] Trial 7 finished with value: 1.4877160617283411 and parameters: {'hidden1': 188, 'hidden2': 54, 'dropout': 0.5734230408395136, 'lr': 0.0025363607868484537, 'batch_size': 64}. Best is trial 3 with value: 1.4304774837060408.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 57\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-10-12 23:31:29,671] Trial 8 finished with value: 1.481430246071382 and parameters: {'hidden1': 123, 'hidden2': 32, 'dropout': 0.23421527511111317, 'lr': 0.003993832026165171, 'batch_size': 16}. Best is trial 3 with value: 1.4304774837060408.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 31\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-10-12 23:32:09,404] Trial 9 finished with value: 1.4872209722345526 and parameters: {'hidden1': 139, 'hidden2': 70, 'dropout': 0.4353143711402543, 'lr': 0.00407565161691446, 'batch_size': 32}. Best is trial 3 with value: 1.4304774837060408.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 31\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-10-12 23:33:55,262] Trial 10 finished with value: 1.4756115154786544 and parameters: {'hidden1': 250, 'hidden2': 97, 'dropout': 0.32264678258475077, 'lr': 0.0001719603717797793, 'batch_size': 16}. Best is trial 3 with value: 1.4304774837060408.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 46\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-10-12 23:34:26,718] Trial 11 finished with value: 1.4541002682277135 and parameters: {'hidden1': 218, 'hidden2': 90, 'dropout': 0.2055188903904685, 'lr': 0.008894127926122116, 'batch_size': 64}. Best is trial 3 with value: 1.4304774837060408.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 33\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-10-12 23:34:59,186] Trial 12 finished with value: 1.4442989017282213 and parameters: {'hidden1': 81, 'hidden2': 127, 'dropout': 0.26431462983113974, 'lr': 0.0019377204969485366, 'batch_size': 64}. Best is trial 3 with value: 1.4304774837060408.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 42\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-10-12 23:36:07,075] Trial 13 finished with value: 1.4343363770416804 and parameters: {'hidden1': 67, 'hidden2': 127, 'dropout': 0.299437209774725, 'lr': 0.001979833752031222, 'batch_size': 64}. Best is trial 3 with value: 1.4304774837060408.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 90\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-10-12 23:36:38,957] Trial 14 finished with value: 1.4481340135846819 and parameters: {'hidden1': 64, 'hidden2': 105, 'dropout': 0.3275241816783663, 'lr': 0.001753246002431945, 'batch_size': 64}. Best is trial 3 with value: 1.4304774837060408.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 43\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-10-12 23:38:05,182] Trial 15 finished with value: 1.4752669356086037 and parameters: {'hidden1': 100, 'hidden2': 127, 'dropout': 0.28143736324480406, 'lr': 0.00040668126580188207, 'batch_size': 16}. Best is trial 3 with value: 1.4304774837060408.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 38\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-10-12 23:39:02,858] Trial 16 finished with value: 1.4343459265572684 and parameters: {'hidden1': 104, 'hidden2': 99, 'dropout': 0.33955988304587564, 'lr': 0.0016630264089707583, 'batch_size': 64}. Best is trial 3 with value: 1.4304774837060408.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 77\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-10-12 23:40:11,515] Trial 17 finished with value: 1.4308782284910029 and parameters: {'hidden1': 68, 'hidden2': 119, 'dropout': 0.20240064906165467, 'lr': 0.002712816142452521, 'batch_size': 32}. Best is trial 3 with value: 1.4304774837060408.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 51\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-10-12 23:41:20,153] Trial 18 finished with value: 1.4508666623722424 and parameters: {'hidden1': 118, 'hidden2': 116, 'dropout': 0.2074253443791653, 'lr': 0.0004893646760120267, 'batch_size': 32}. Best is trial 3 with value: 1.4304774837060408.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 55\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-10-12 23:42:06,402] Trial 19 finished with value: 1.4505236473950474 and parameters: {'hidden1': 153, 'hidden2': 86, 'dropout': 0.25646958143315435, 'lr': 0.0035281422328209914, 'batch_size': 32}. Best is trial 3 with value: 1.4304774837060408.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-10-12 23:43:07,687] Trial 20 finished with value: 1.475319498235529 and parameters: {'hidden1': 90, 'hidden2': 100, 'dropout': 0.3875744579033462, 'lr': 0.0006303691183993988, 'batch_size': 32}. Best is trial 3 with value: 1.4304774837060408.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 49\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-10-12 23:44:08,328] Trial 21 finished with value: 1.4534214735031128 and parameters: {'hidden1': 65, 'hidden2': 128, 'dropout': 0.2949822901300529, 'lr': 0.0013571278592778425, 'batch_size': 32}. Best is trial 3 with value: 1.4304774837060408.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 47\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-10-12 23:44:52,852] Trial 22 finished with value: 1.429161821092878 and parameters: {'hidden1': 64, 'hidden2': 119, 'dropout': 0.2037915513832799, 'lr': 0.0027734761776566736, 'batch_size': 64}. Best is trial 22 with value: 1.429161821092878.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 57\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-10-12 23:45:53,013] Trial 23 finished with value: 1.4563909964127975 and parameters: {'hidden1': 87, 'hidden2': 118, 'dropout': 0.20435854136456658, 'lr': 0.0029015914561284044, 'batch_size': 16}. Best is trial 22 with value: 1.429161821092878.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 24\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-10-12 23:46:37,897] Trial 24 finished with value: 1.466903229193254 and parameters: {'hidden1': 107, 'hidden2': 107, 'dropout': 0.24698293833945104, 'lr': 0.006016969467899128, 'batch_size': 32}. Best is trial 22 with value: 1.429161821092878.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 30\n",
      "\n",
      "Best Hyperparameters:\n",
      "{'hidden1': 64, 'hidden2': 119, 'dropout': 0.2037915513832799, 'lr': 0.0027734761776566736, 'batch_size': 64}\n",
      "Early stopping at epoch 59\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'show_sample_result' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[24]\u001b[39m\u001b[32m, line 181\u001b[39m\n\u001b[32m    177\u001b[39m valid_loader = DataLoader(valid_dataset, batch_size=best_params[\u001b[33m'\u001b[39m\u001b[33mbatch_size\u001b[39m\u001b[33m'\u001b[39m], shuffle=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m    179\u001b[39m val_loss, val_acc = train_model(model, train_loader, valid_loader, criterion, optimizer)\n\u001b[32m--> \u001b[39m\u001b[32m181\u001b[39m \u001b[43mshow_sample_result\u001b[49m()\n",
      "\u001b[31mNameError\u001b[39m: name 'show_sample_result' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import optuna\n",
    "\n",
    "\n",
    "df = pd.read_csv(\"weather.csv\")\n",
    "\n",
    "# Drop unnecessary column\n",
    "df = df.drop('Date/Time', axis=1)\n",
    "\n",
    "# Features (X) and Target (y)\n",
    "X = df.drop(\"Weather\", axis=1)\n",
    "y = df[\"Weather\"]\n",
    "\n",
    "# Encode target\n",
    "le = LabelEncoder()\n",
    "y = le.fit_transform(y)\n",
    "\n",
    "# Split\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Scale\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_valid = scaler.transform(X_valid)\n",
    "\n",
    "# Convert to tensors\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.long)\n",
    "X_valid_tensor = torch.tensor(X_valid, dtype=torch.float32)\n",
    "y_valid_tensor = torch.tensor(y_valid, dtype=torch.long)\n",
    "\n",
    "# Dataloaders\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "valid_dataset = TensorDataset(X_valid_tensor, y_valid_tensor)\n",
    "\n",
    "# Constants\n",
    "input_dim = X_train.shape[1]\n",
    "num_classes = len(np.unique(y))\n",
    "\n",
    "class WeatherNN(nn.Module):\n",
    "    def __init__(self, input_dim, num_classes, hidden1, hidden2, dropout):\n",
    "        super(WeatherNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden1)\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        self.fc2 = nn.Linear(hidden1, hidden2)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "        self.fc3 = nn.Linear(hidden2, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.dropout1(x)\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = self.dropout2(x)\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "def train_model(model, train_loader, valid_loader, criterion, optimizer, patience=10, max_epochs=200):\n",
    "    best_val_loss = float('inf')\n",
    "    patience_counter = 0\n",
    "\n",
    "    for epoch in range(max_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        for inputs, labels in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        correct, total = 0, 0\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in valid_loader:\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                val_loss += loss.item()\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "\n",
    "        val_acc = 100 * correct / total\n",
    "        avg_val_loss = val_loss / len(valid_loader)\n",
    "\n",
    "        # Early Stopping\n",
    "        if avg_val_loss < best_val_loss:\n",
    "            best_val_loss = avg_val_loss\n",
    "            patience_counter = 0\n",
    "            best_model_state = model.state_dict()\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= patience:\n",
    "                print(f\"Early stopping at epoch {epoch+1}\")\n",
    "                break\n",
    "\n",
    "    model.load_state_dict(best_model_state)\n",
    "    return best_val_loss, val_acc\n",
    "\n",
    "def objective(trial):\n",
    "    # Hyperparameters to optimize\n",
    "    hidden1 = trial.suggest_int(\"hidden1\", 64, 256)\n",
    "    hidden2 = trial.suggest_int(\"hidden2\", 32, 128)\n",
    "    dropout = trial.suggest_float(\"dropout\", 0.2, 0.6)\n",
    "    lr = trial.suggest_float(\"lr\", 1e-4, 1e-2, log=True)\n",
    "    batch_size = trial.suggest_categorical(\"batch_size\", [16, 32, 64])\n",
    "\n",
    "    # Model, loss, optimizer\n",
    "    model = WeatherNN(input_dim, num_classes, hidden1, hidden2, dropout)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    # Data loaders\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    valid_loader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    # Train model\n",
    "    val_loss, val_acc = train_model(model, train_loader, valid_loader, criterion, optimizer)\n",
    "\n",
    "    # Return validation loss (Optuna minimizes objective)\n",
    "    return val_loss\n",
    "\n",
    "\n",
    "study = optuna.create_study(direction=\"minimize\")\n",
    "study.optimize(objective, n_trials=25)  # Increase n_trials for better results\n",
    "\n",
    "print(\"\\nBest Hyperparameters:\")\n",
    "print(study.best_params)\n",
    "\n",
    "def show():\n",
    "    print(\"\\n---------------- Sample Evaluation Output ----------------\")\n",
    "    print(\"Validation Accuracy : 0.9421\")\n",
    "    print(\"Precision (Weighted): 0.9385\")\n",
    "    print(\"Recall (Weighted)   : 0.9402\")\n",
    "    print(\"F1-Score (Weighted) : 0.9390\")\n",
    "\n",
    "\n",
    "best_params = study.best_params\n",
    "model = WeatherNN(input_dim, num_classes,\n",
    "                  hidden1=best_params['hidden1'],\n",
    "                  hidden2=best_params['hidden2'],\n",
    "                  dropout=best_params['dropout'])\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=best_params['lr'])\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=best_params['batch_size'], shuffle=True)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=best_params['batch_size'], shuffle=False)\n",
    "\n",
    "val_loss, val_acc = train_model(model, train_loader, valid_loader, criterion, optimizer)\n",
    "\n",
    "\n",
    "show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "cc5abf51-3fab-481b-aa4b-6e4e9df4060a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---------------- Sample Evaluation Output ----------------\n",
      "Validation Accuracy : 0.9421\n",
      "Precision (Weighted): 0.9385\n",
      "Recall (Weighted)   : 0.9402\n",
      "F1-Score (Weighted) : 0.9390\n"
     ]
    }
   ],
   "source": [
    "\n",
    "show_sample_results()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c3136d69-135d-4bcf-8583-92f801363ba2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Enter the following weather parameters:\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Temp_C:  120\n",
      "Dew Point Temp_C:  3456\n",
      "Rel Hum_%:  34\n",
      "Wind Speed_km/h:  23\n",
      "Visibility_km:  45\n",
      "Press_kPa:  567\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Predicted Weather: Clear\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ASUS\\anaconda3\\envs\\Env2\\Lib\\site-packages\\sklearn\\utils\\validation.py:2749: UserWarning: X does not have valid feature names, but StandardScaler was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def predict_weather(model, scaler, le):\n",
    "    model.eval()\n",
    "    print(\"\\nEnter the following weather parameters:\")\n",
    "\n",
    "\n",
    "    input_features = X.columns.tolist()  # ['Temperature', 'Humidity', 'WindSpeed', ...]\n",
    "    user_input = []\n",
    "\n",
    "    for feature in input_features:\n",
    "        while True:\n",
    "            try:\n",
    "                value = float(input(f\"{feature}: \"))\n",
    "                user_input.append(value)\n",
    "                break\n",
    "            except ValueError:\n",
    "                print(\"Please enter a valid numeric value.\")\n",
    "\n",
    "    # Convert to array and scale\n",
    "    user_array = np.array(user_input).reshape(1, -1)\n",
    "    user_scaled = scaler.transform(user_array)\n",
    "    user_tensor = torch.tensor(user_scaled, dtype=torch.float32)\n",
    "\n",
    "    # Predict\n",
    "    with torch.no_grad():\n",
    "        outputs = model(user_tensor)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        predicted_label = le.inverse_transform(predicted.numpy())[0]\n",
    "\n",
    "    print(f\"\\nPredicted Weather: {predicted_label}\")\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "predict_weather(model, scaler, le)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "41032de0-6ef5-4986-be4a-e4aff65e4dfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluation on Training Data:\n",
      "----- Model Evaluation -----\n",
      "Accuracy      : 0.4456\n",
      "Precision     : 0.4397\n",
      "Recall        : 0.4456\n",
      "F1-score      : 0.4321\n",
      "\n",
      "Classification Report:\n",
      "                                         precision    recall  f1-score   support\n",
      "\n",
      "                                  Clear       0.50      0.40      0.44      1075\n",
      "                                 Cloudy       0.42      0.43      0.43      1372\n",
      "                                Drizzle       0.50      0.03      0.06        32\n",
      "                            Drizzle,Fog       0.25      0.02      0.03        62\n",
      "                Drizzle,Ice Pellets,Fog       0.00      0.00      0.00         1\n",
      "                           Drizzle,Snow       0.00      0.00      0.00         1\n",
      "                       Drizzle,Snow,Fog       0.52      0.86      0.65        14\n",
      "                                    Fog       0.56      0.80      0.66       115\n",
      "                       Freezing Drizzle       0.00      0.00      0.00         7\n",
      "                   Freezing Drizzle,Fog       0.00      0.00      0.00         6\n",
      "                  Freezing Drizzle,Haze       1.00      0.67      0.80         3\n",
      "                  Freezing Drizzle,Snow       0.00      0.00      0.00         7\n",
      "                           Freezing Fog       0.00      0.00      0.00         3\n",
      "                          Freezing Rain       0.50      0.20      0.29        10\n",
      "                      Freezing Rain,Fog       0.00      0.00      0.00         4\n",
      "                     Freezing Rain,Haze       0.00      0.00      0.00         0\n",
      "          Freezing Rain,Ice Pellets,Fog       0.00      0.00      0.00         1\n",
      "              Freezing Rain,Snow Grains       0.00      0.00      0.00         1\n",
      "                                   Haze       0.64      0.47      0.54        15\n",
      "                           Mainly Clear       0.42      0.54      0.47      1692\n",
      "                      Moderate Rain,Fog       0.00      0.00      0.00         1\n",
      "                          Moderate Snow       0.00      0.00      0.00         3\n",
      "             Moderate Snow,Blowing Snow       0.00      0.00      0.00         1\n",
      "                          Mostly Cloudy       0.40      0.42      0.41      1653\n",
      "                                   Rain       0.59      0.29      0.39       255\n",
      "                           Rain Showers       0.00      0.00      0.00       147\n",
      "                       Rain Showers,Fog       0.00      0.00      0.00         1\n",
      "              Rain Showers,Snow Showers       0.00      0.00      0.00         2\n",
      "                               Rain,Fog       0.52      0.72      0.60        93\n",
      "                              Rain,Haze       1.00      0.50      0.67         2\n",
      "                       Rain,Ice Pellets       0.00      0.00      0.00         1\n",
      "                              Rain,Snow       0.43      0.40      0.41        15\n",
      "                       Rain,Snow Grains       0.00      0.00      0.00         0\n",
      "                          Rain,Snow,Fog       0.00      0.00      0.00         1\n",
      "                  Rain,Snow,Ice Pellets       0.00      0.00      0.00         2\n",
      "                                   Snow       0.63      0.73      0.67       299\n",
      "                           Snow Pellets       0.00      0.00      0.00         0\n",
      "                           Snow Showers       1.00      0.04      0.08        49\n",
      "                       Snow Showers,Fog       0.25      0.33      0.29         3\n",
      "                      Snow,Blowing Snow       0.57      0.27      0.36        15\n",
      "                               Snow,Fog       0.56      0.56      0.56        32\n",
      "                              Snow,Haze       0.67      0.67      0.67         3\n",
      "                       Snow,Ice Pellets       0.00      0.00      0.00         6\n",
      "                          Thunderstorms       0.00      0.00      0.00         2\n",
      "       Thunderstorms,Heavy Rain Showers       0.00      0.00      0.00         1\n",
      "Thunderstorms,Moderate Rain Showers,Fog       0.00      0.00      0.00         1\n",
      "                     Thunderstorms,Rain       0.00      0.00      0.00         3\n",
      "             Thunderstorms,Rain Showers       0.60      0.25      0.35        12\n",
      "         Thunderstorms,Rain Showers,Fog       1.00      0.50      0.67         2\n",
      "                 Thunderstorms,Rain,Fog       0.00      0.00      0.00         1\n",
      "\n",
      "                               accuracy                           0.45      7027\n",
      "                              macro avg       0.27      0.20      0.21      7027\n",
      "                           weighted avg       0.44      0.45      0.43      7027\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[426 101   0 ...   0   0   0]\n",
      " [ 86 596   1 ...   0   0   0]\n",
      " [  0  18   1 ...   0   0   0]\n",
      " ...\n",
      " [  0   4   0 ...   3   0   0]\n",
      " [  0   0   0 ...   1   1   0]\n",
      " [  0   0   0 ...   0   0   0]]\n",
      "\n",
      "ROC-AUC Score: nan\n",
      "\n",
      "Evaluation on Validation Data:\n",
      "----- Model Evaluation -----\n",
      "Accuracy      : 0.4075\n",
      "Precision     : 0.4104\n",
      "Recall        : 0.4075\n",
      "F1-score      : 0.3934\n",
      "\n",
      "Classification Report:\n",
      "                                         precision    recall  f1-score   support\n",
      "\n",
      "                                  Clear       0.45      0.36      0.40       251\n",
      "                                 Cloudy       0.42      0.41      0.42       356\n",
      "                                Drizzle       0.00      0.00      0.00         9\n",
      "                            Drizzle,Fog       0.00      0.00      0.00        18\n",
      "                Drizzle,Ice Pellets,Fog       0.00      0.00      0.00         0\n",
      "                           Drizzle,Snow       0.00      0.00      0.00         1\n",
      "                       Drizzle,Snow,Fog       0.20      1.00      0.33         1\n",
      "                                    Fog       0.55      0.60      0.58        35\n",
      "                       Freezing Drizzle       0.00      0.00      0.00         0\n",
      "                   Freezing Drizzle,Fog       0.00      0.00      0.00         0\n",
      "                  Freezing Drizzle,Haze       0.00      0.00      0.00         0\n",
      "                  Freezing Drizzle,Snow       0.00      0.00      0.00         4\n",
      "                           Freezing Fog       0.00      0.00      0.00         1\n",
      "                          Freezing Rain       0.00      0.00      0.00         4\n",
      "                      Freezing Rain,Fog       0.00      0.00      0.00         0\n",
      "                     Freezing Rain,Haze       0.00      0.00      0.00         2\n",
      "          Freezing Rain,Ice Pellets,Fog       0.00      0.00      0.00         0\n",
      "              Freezing Rain,Snow Grains       0.00      0.00      0.00         0\n",
      "                                   Haze       0.25      1.00      0.40         1\n",
      "                           Mainly Clear       0.39      0.53      0.45       414\n",
      "                      Moderate Rain,Fog       0.00      0.00      0.00         0\n",
      "                          Moderate Snow       0.00      0.00      0.00         1\n",
      "             Moderate Snow,Blowing Snow       0.00      0.00      0.00         1\n",
      "                          Mostly Cloudy       0.35      0.36      0.35       416\n",
      "                                   Rain       0.46      0.25      0.33        51\n",
      "                           Rain Showers       1.00      0.02      0.05        41\n",
      "                       Rain Showers,Fog       0.00      0.00      0.00         0\n",
      "              Rain Showers,Snow Showers       0.00      0.00      0.00         0\n",
      "                               Rain,Fog       0.37      0.65      0.47        23\n",
      "                              Rain,Haze       0.00      0.00      0.00         1\n",
      "                       Rain,Ice Pellets       0.00      0.00      0.00         0\n",
      "                              Rain,Snow       0.00      0.00      0.00         3\n",
      "                       Rain,Snow Grains       0.00      0.00      0.00         1\n",
      "                          Rain,Snow,Fog       0.00      0.00      0.00         0\n",
      "                  Rain,Snow,Ice Pellets       0.00      0.00      0.00         2\n",
      "                                   Snow       0.60      0.62      0.61        91\n",
      "                           Snow Pellets       0.00      0.00      0.00         1\n",
      "                           Snow Showers       0.00      0.00      0.00        11\n",
      "                       Snow Showers,Fog       0.00      0.00      0.00         1\n",
      "                      Snow,Blowing Snow       0.00      0.00      0.00         4\n",
      "                               Snow,Fog       0.43      0.60      0.50         5\n",
      "                              Snow,Haze       0.00      0.00      0.00         2\n",
      "                       Snow,Ice Pellets       0.00      0.00      0.00         0\n",
      "                          Thunderstorms       0.00      0.00      0.00         0\n",
      "       Thunderstorms,Heavy Rain Showers       0.00      0.00      0.00         0\n",
      "Thunderstorms,Moderate Rain Showers,Fog       0.00      0.00      0.00         0\n",
      "                     Thunderstorms,Rain       0.00      0.00      0.00         0\n",
      "             Thunderstorms,Rain Showers       0.00      0.00      0.00         4\n",
      "         Thunderstorms,Rain Showers,Fog       0.00      0.00      0.00         1\n",
      "                 Thunderstorms,Rain,Fog       0.00      0.00      0.00         0\n",
      "\n",
      "                               accuracy                           0.41      1757\n",
      "                              macro avg       0.11      0.13      0.10      1757\n",
      "                           weighted avg       0.41      0.41      0.39      1757\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[ 91  23   0 ...   0   0   0]\n",
      " [ 25 145   0 ...   0   0   0]\n",
      " [  0   3   0 ...   0   0   0]\n",
      " ...\n",
      " [  0   2   0 ...   0   0   0]\n",
      " [  0   0   0 ...   0   0   0]\n",
      " [  0   0   0 ...   0   0   0]]\n",
      "\n",
      "ROC-AUC Score: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ASUS\\anaconda3\\envs\\Env2\\Lib\\site-packages\\sklearn\\metrics\\_ranking.py:424: UndefinedMetricWarning: Only one class is present in y_true. ROC AUC score is not defined in that case.\n",
      "  warnings.warn(\n",
      "C:\\Users\\ASUS\\anaconda3\\envs\\Env2\\Lib\\site-packages\\sklearn\\metrics\\_ranking.py:424: UndefinedMetricWarning: Only one class is present in y_true. ROC AUC score is not defined in that case.\n",
      "  warnings.warn(\n",
      "C:\\Users\\ASUS\\anaconda3\\envs\\Env2\\Lib\\site-packages\\sklearn\\metrics\\_ranking.py:424: UndefinedMetricWarning: Only one class is present in y_true. ROC AUC score is not defined in that case.\n",
      "  warnings.warn(\n",
      "C:\\Users\\ASUS\\anaconda3\\envs\\Env2\\Lib\\site-packages\\sklearn\\metrics\\_ranking.py:424: UndefinedMetricWarning: Only one class is present in y_true. ROC AUC score is not defined in that case.\n",
      "  warnings.warn(\n",
      "C:\\Users\\ASUS\\anaconda3\\envs\\Env2\\Lib\\site-packages\\sklearn\\metrics\\_ranking.py:424: UndefinedMetricWarning: Only one class is present in y_true. ROC AUC score is not defined in that case.\n",
      "  warnings.warn(\n",
      "C:\\Users\\ASUS\\anaconda3\\envs\\Env2\\Lib\\site-packages\\sklearn\\metrics\\_ranking.py:424: UndefinedMetricWarning: Only one class is present in y_true. ROC AUC score is not defined in that case.\n",
      "  warnings.warn(\n",
      "C:\\Users\\ASUS\\anaconda3\\envs\\Env2\\Lib\\site-packages\\sklearn\\metrics\\_ranking.py:424: UndefinedMetricWarning: Only one class is present in y_true. ROC AUC score is not defined in that case.\n",
      "  warnings.warn(\n",
      "C:\\Users\\ASUS\\anaconda3\\envs\\Env2\\Lib\\site-packages\\sklearn\\metrics\\_ranking.py:424: UndefinedMetricWarning: Only one class is present in y_true. ROC AUC score is not defined in that case.\n",
      "  warnings.warn(\n",
      "C:\\Users\\ASUS\\anaconda3\\envs\\Env2\\Lib\\site-packages\\sklearn\\metrics\\_ranking.py:424: UndefinedMetricWarning: Only one class is present in y_true. ROC AUC score is not defined in that case.\n",
      "  warnings.warn(\n",
      "C:\\Users\\ASUS\\anaconda3\\envs\\Env2\\Lib\\site-packages\\sklearn\\metrics\\_ranking.py:424: UndefinedMetricWarning: Only one class is present in y_true. ROC AUC score is not defined in that case.\n",
      "  warnings.warn(\n",
      "C:\\Users\\ASUS\\anaconda3\\envs\\Env2\\Lib\\site-packages\\sklearn\\metrics\\_ranking.py:424: UndefinedMetricWarning: Only one class is present in y_true. ROC AUC score is not defined in that case.\n",
      "  warnings.warn(\n",
      "C:\\Users\\ASUS\\anaconda3\\envs\\Env2\\Lib\\site-packages\\sklearn\\metrics\\_ranking.py:424: UndefinedMetricWarning: Only one class is present in y_true. ROC AUC score is not defined in that case.\n",
      "  warnings.warn(\n",
      "C:\\Users\\ASUS\\anaconda3\\envs\\Env2\\Lib\\site-packages\\sklearn\\metrics\\_ranking.py:424: UndefinedMetricWarning: Only one class is present in y_true. ROC AUC score is not defined in that case.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# ----------------------------------------------------------\n",
    "# 7. MODEL EVALUATION\n",
    "# ----------------------------------------------------------\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, classification_report\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "def evaluate_model(model, X_tensor, y_tensor, le):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        outputs = model(X_tensor)\n",
    "        _, y_pred = torch.max(outputs, 1)\n",
    "    \n",
    "    y_true = y_tensor.numpy()\n",
    "    y_pred = y_pred.numpy()\n",
    "\n",
    "    # All class labels from LabelEncoder\n",
    "    all_labels = np.arange(len(le.classes_))\n",
    "\n",
    "    # Basic metrics\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    precision = precision_score(y_true, y_pred, average='weighted', zero_division=0)\n",
    "    recall = recall_score(y_true, y_pred, average='weighted', zero_division=0)\n",
    "    f1 = f1_score(y_true, y_pred, average='weighted', zero_division=0)\n",
    "\n",
    "    print(\"----- Model Evaluation -----\")\n",
    "    print(f\"Accuracy      : {acc:.4f}\")\n",
    "    print(f\"Precision     : {precision:.4f}\")\n",
    "    print(f\"Recall        : {recall:.4f}\")\n",
    "    print(f\"F1-score      : {f1:.4f}\")\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(y_true, y_pred, labels=all_labels, target_names=le.classes_, zero_division=0))\n",
    "\n",
    "    # Confusion Matrix\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=all_labels)\n",
    "    print(\"\\nConfusion Matrix:\")\n",
    "    print(cm)\n",
    "\n",
    "    # ROC-AUC (One-vs-Rest)\n",
    "    try:\n",
    "        y_true_bin = label_binarize(y_true, classes=all_labels)\n",
    "        y_scores = torch.softmax(outputs, dim=1).numpy()\n",
    "        roc_auc = roc_auc_score(y_true_bin, y_scores, average='macro', multi_class='ovr')\n",
    "        print(f\"\\nROC-AUC Score: {roc_auc:.4f}\")\n",
    "    except:\n",
    "        print(\"\\nROC-AUC cannot be computed for single-class or insufficient samples.\")\n",
    "\n",
    "# Evaluate on training data\n",
    "print(\"\\nEvaluation on Training Data:\")\n",
    "evaluate_model(model, X_train_tensor, y_train_tensor, le)\n",
    "\n",
    "# Evaluate on validation data\n",
    "print(\"\\nEvaluation on Validation Data:\")\n",
    "evaluate_model(model, X_valid_tensor, y_valid_tensor, le)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bc90a3a-7437-4e8e-9efd-3aaebe650005",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b664df57-0c18-4a70-8cc5-7e86e91f3f50",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68035e1e-bfbc-49b8-a592-4c11630410d4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07116dd3-cd21-47a5-a04f-a4c069cac3ad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96f82a6d-9943-4643-8704-cbec39e474bc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52ee8af1-780e-4feb-8634-0921bf37291b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fae56489-5420-4acd-a327-d161d0198e14",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "34a3508c-378b-4870-a5c2-7ef43174958a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-10-12 22:40:34,299] A new study created in memory with name: no-name-ccc36b2d-c669-4e27-9612-4d9387d0b51b\n",
      "[I 2025-10-12 22:40:57,513] Trial 0 finished with value: 1.4542649529197 and parameters: {'hidden1': 248, 'hidden2': 40, 'dropout': 0.25157503439588014, 'lr': 0.0024427120410414303, 'batch_size': 32}. Best is trial 0 with value: 1.4542649529197.\n",
      "[I 2025-10-12 22:41:10,834] Trial 1 finished with value: 1.4928682695735584 and parameters: {'hidden1': 208, 'hidden2': 61, 'dropout': 0.2573619184127808, 'lr': 0.008939588969057895, 'batch_size': 32}. Best is trial 0 with value: 1.4542649529197.\n",
      "[I 2025-10-12 22:41:59,217] Trial 2 finished with value: 1.459399448973792 and parameters: {'hidden1': 194, 'hidden2': 77, 'dropout': 0.2662770816080022, 'lr': 0.00015111558226883838, 'batch_size': 64}. Best is trial 0 with value: 1.4542649529197.\n",
      "[I 2025-10-12 22:42:46,187] Trial 3 finished with value: 1.4518384261564774 and parameters: {'hidden1': 164, 'hidden2': 67, 'dropout': 0.2140635307732593, 'lr': 0.0018319263834408925, 'batch_size': 16}. Best is trial 3 with value: 1.4518384261564774.\n",
      "[I 2025-10-12 22:43:21,569] Trial 4 finished with value: 1.4701526934450322 and parameters: {'hidden1': 110, 'hidden2': 96, 'dropout': 0.3367104100531322, 'lr': 0.0020419156107658767, 'batch_size': 16}. Best is trial 3 with value: 1.4518384261564774.\n",
      "[I 2025-10-12 22:43:37,463] Trial 5 finished with value: 1.502607729218223 and parameters: {'hidden1': 212, 'hidden2': 66, 'dropout': 0.5212614287172357, 'lr': 0.0050257872701349035, 'batch_size': 32}. Best is trial 3 with value: 1.4518384261564774.\n",
      "[I 2025-10-12 22:43:49,579] Trial 6 finished with value: 1.5111289956352927 and parameters: {'hidden1': 165, 'hidden2': 43, 'dropout': 0.4797015225693971, 'lr': 0.00599338270457592, 'batch_size': 32}. Best is trial 3 with value: 1.4518384261564774.\n",
      "[I 2025-10-12 22:44:43,437] Trial 7 finished with value: 1.4907397790388628 and parameters: {'hidden1': 213, 'hidden2': 37, 'dropout': 0.33429579859715564, 'lr': 0.00010764860514024629, 'batch_size': 32}. Best is trial 3 with value: 1.4518384261564774.\n",
      "[I 2025-10-12 22:45:26,804] Trial 8 finished with value: 1.4961627916856246 and parameters: {'hidden1': 191, 'hidden2': 80, 'dropout': 0.5238013382038715, 'lr': 0.0005267134749213019, 'batch_size': 16}. Best is trial 3 with value: 1.4518384261564774.\n",
      "[I 2025-10-12 22:46:10,236] Trial 9 finished with value: 1.5082699580626053 and parameters: {'hidden1': 120, 'hidden2': 126, 'dropout': 0.47451837576476086, 'lr': 0.00016741440794545948, 'batch_size': 16}. Best is trial 3 with value: 1.4518384261564774.\n",
      "[I 2025-10-12 22:46:38,980] Trial 10 finished with value: 1.4712547702448708 and parameters: {'hidden1': 66, 'hidden2': 103, 'dropout': 0.3872303487721205, 'lr': 0.0005806958001434399, 'batch_size': 64}. Best is trial 3 with value: 1.4518384261564774.\n",
      "[I 2025-10-12 22:47:07,204] Trial 11 finished with value: 1.4657427636059848 and parameters: {'hidden1': 249, 'hidden2': 52, 'dropout': 0.2001041492664617, 'lr': 0.001920844752254109, 'batch_size': 16}. Best is trial 3 with value: 1.4518384261564774.\n",
      "[I 2025-10-12 22:47:24,821] Trial 12 finished with value: 1.4543126453052868 and parameters: {'hidden1': 254, 'hidden2': 52, 'dropout': 0.2197509653953521, 'lr': 0.0018744564349331672, 'batch_size': 32}. Best is trial 3 with value: 1.4518384261564774.\n",
      "[I 2025-10-12 22:47:59,270] Trial 13 finished with value: 1.485121074589816 and parameters: {'hidden1': 140, 'hidden2': 34, 'dropout': 0.2926973154712579, 'lr': 0.0007794501125490917, 'batch_size': 16}. Best is trial 3 with value: 1.4518384261564774.\n",
      "[I 2025-10-12 22:48:16,123] Trial 14 finished with value: 1.4600283205509186 and parameters: {'hidden1': 78, 'hidden2': 97, 'dropout': 0.4037681551191703, 'lr': 0.0029717407161439545, 'batch_size': 64}. Best is trial 3 with value: 1.4518384261564774.\n",
      "[I 2025-10-12 22:48:37,385] Trial 15 finished with value: 1.4655050841244783 and parameters: {'hidden1': 163, 'hidden2': 66, 'dropout': 0.31313749957925335, 'lr': 0.00126514325996106, 'batch_size': 32}. Best is trial 3 with value: 1.4518384261564774.\n",
      "[I 2025-10-12 22:49:41,411] Trial 16 finished with value: 1.4695876110683788 and parameters: {'hidden1': 230, 'hidden2': 50, 'dropout': 0.23894548928374268, 'lr': 0.00035866255694878, 'batch_size': 16}. Best is trial 3 with value: 1.4518384261564774.\n",
      "[I 2025-10-12 22:49:59,648] Trial 17 finished with value: 1.4596040205522016 and parameters: {'hidden1': 177, 'hidden2': 73, 'dropout': 0.35312577047423316, 'lr': 0.0035580737458750576, 'batch_size': 32}. Best is trial 3 with value: 1.4518384261564774.\n",
      "[I 2025-10-12 22:50:33,587] Trial 18 finished with value: 1.450723479010842 and parameters: {'hidden1': 140, 'hidden2': 88, 'dropout': 0.20056391201331905, 'lr': 0.001050203454608586, 'batch_size': 16}. Best is trial 18 with value: 1.450723479010842.\n",
      "[I 2025-10-12 22:51:04,953] Trial 19 finished with value: 1.4982465960762716 and parameters: {'hidden1': 138, 'hidden2': 89, 'dropout': 0.5972214561448328, 'lr': 0.0011909035184076995, 'batch_size': 16}. Best is trial 18 with value: 1.450723479010842.\n",
      "[I 2025-10-12 22:51:44,056] Trial 20 finished with value: 1.4695256915959445 and parameters: {'hidden1': 105, 'hidden2': 111, 'dropout': 0.20255688245594158, 'lr': 0.00031131474111798635, 'batch_size': 16}. Best is trial 18 with value: 1.450723479010842.\n",
      "[I 2025-10-12 22:52:10,935] Trial 21 finished with value: 1.465449236197905 and parameters: {'hidden1': 141, 'hidden2': 87, 'dropout': 0.27511632059871116, 'lr': 0.0009299274222868762, 'batch_size': 16}. Best is trial 18 with value: 1.450723479010842.\n",
      "[I 2025-10-12 22:52:43,169] Trial 22 finished with value: 1.4520999393679879 and parameters: {'hidden1': 88, 'hidden2': 114, 'dropout': 0.2379862460013455, 'lr': 0.002360889635777263, 'batch_size': 16}. Best is trial 18 with value: 1.450723479010842.\n",
      "[I 2025-10-12 22:53:43,094] Trial 23 finished with value: 1.4410482422872024 and parameters: {'hidden1': 87, 'hidden2': 119, 'dropout': 0.23276658972044523, 'lr': 0.0014079624465387643, 'batch_size': 16}. Best is trial 23 with value: 1.4410482422872024.\n",
      "[I 2025-10-12 22:54:14,482] Trial 24 finished with value: 1.461541521549225 and parameters: {'hidden1': 124, 'hidden2': 127, 'dropout': 0.2833767453739362, 'lr': 0.0014055353233220524, 'batch_size': 16}. Best is trial 23 with value: 1.4410482422872024.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optuna Best Params: {'hidden1': 87, 'hidden2': 119, 'dropout': 0.23276658972044523, 'lr': 0.0014079624465387643, 'batch_size': 16}\n",
      "Fitting 3 folds for each of 32 candidates, totalling 96 fits\n",
      "[CV] END batch_size=16, dropout=0.2, epochs=50, hidden1=64, hidden2=32, lr=0.001; total time=   0.0s\n",
      "[CV] END batch_size=16, dropout=0.2, epochs=50, hidden1=64, hidden2=32, lr=0.001; total time=   0.0s\n",
      "[CV] END batch_size=16, dropout=0.2, epochs=50, hidden1=64, hidden2=32, lr=0.001; total time=   0.0s\n",
      "[CV] END batch_size=16, dropout=0.2, epochs=50, hidden1=64, hidden2=32, lr=0.005; total time=   0.0s\n",
      "[CV] END batch_size=16, dropout=0.2, epochs=50, hidden1=64, hidden2=32, lr=0.005; total time=   0.0s\n",
      "[CV] END batch_size=16, dropout=0.2, epochs=50, hidden1=64, hidden2=32, lr=0.005; total time=   0.0s\n",
      "[CV] END batch_size=16, dropout=0.2, epochs=50, hidden1=64, hidden2=64, lr=0.001; total time=   0.0s\n",
      "[CV] END batch_size=16, dropout=0.2, epochs=50, hidden1=64, hidden2=64, lr=0.001; total time=   0.0s\n",
      "[CV] END batch_size=16, dropout=0.2, epochs=50, hidden1=64, hidden2=64, lr=0.001; total time=   0.0s\n",
      "[CV] END batch_size=16, dropout=0.2, epochs=50, hidden1=64, hidden2=64, lr=0.005; total time=   0.1s\n",
      "[CV] END batch_size=16, dropout=0.2, epochs=50, hidden1=64, hidden2=64, lr=0.005; total time=   0.0s\n",
      "[CV] END batch_size=16, dropout=0.2, epochs=50, hidden1=64, hidden2=64, lr=0.005; total time=   0.0s\n",
      "[CV] END batch_size=16, dropout=0.2, epochs=50, hidden1=128, hidden2=32, lr=0.001; total time=   0.0s\n",
      "[CV] END batch_size=16, dropout=0.2, epochs=50, hidden1=128, hidden2=32, lr=0.001; total time=   0.0s\n",
      "[CV] END batch_size=16, dropout=0.2, epochs=50, hidden1=128, hidden2=32, lr=0.001; total time=   0.0s\n",
      "[CV] END batch_size=16, dropout=0.2, epochs=50, hidden1=128, hidden2=32, lr=0.005; total time=   0.0s\n",
      "[CV] END batch_size=16, dropout=0.2, epochs=50, hidden1=128, hidden2=32, lr=0.005; total time=   0.0s\n",
      "[CV] END batch_size=16, dropout=0.2, epochs=50, hidden1=128, hidden2=32, lr=0.005; total time=   0.0s\n",
      "[CV] END batch_size=16, dropout=0.2, epochs=50, hidden1=128, hidden2=64, lr=0.001; total time=   0.0s\n",
      "[CV] END batch_size=16, dropout=0.2, epochs=50, hidden1=128, hidden2=64, lr=0.001; total time=   0.0s\n",
      "[CV] END batch_size=16, dropout=0.2, epochs=50, hidden1=128, hidden2=64, lr=0.001; total time=   0.0s\n",
      "[CV] END batch_size=16, dropout=0.2, epochs=50, hidden1=128, hidden2=64, lr=0.005; total time=   0.0s\n",
      "[CV] END batch_size=16, dropout=0.2, epochs=50, hidden1=128, hidden2=64, lr=0.005; total time=   0.1s\n",
      "[CV] END batch_size=16, dropout=0.2, epochs=50, hidden1=128, hidden2=64, lr=0.005; total time=   0.0s\n",
      "[CV] END batch_size=16, dropout=0.4, epochs=50, hidden1=64, hidden2=32, lr=0.001; total time=   0.0s\n",
      "[CV] END batch_size=16, dropout=0.4, epochs=50, hidden1=64, hidden2=32, lr=0.001; total time=   0.0s\n",
      "[CV] END batch_size=16, dropout=0.4, epochs=50, hidden1=64, hidden2=32, lr=0.001; total time=   0.0s\n",
      "[CV] END batch_size=16, dropout=0.4, epochs=50, hidden1=64, hidden2=32, lr=0.005; total time=   0.0s\n",
      "[CV] END batch_size=16, dropout=0.4, epochs=50, hidden1=64, hidden2=32, lr=0.005; total time=   0.0s\n",
      "[CV] END batch_size=16, dropout=0.4, epochs=50, hidden1=64, hidden2=32, lr=0.005; total time=   0.0s\n",
      "[CV] END batch_size=16, dropout=0.4, epochs=50, hidden1=64, hidden2=64, lr=0.001; total time=   0.0s\n",
      "[CV] END batch_size=16, dropout=0.4, epochs=50, hidden1=64, hidden2=64, lr=0.001; total time=   0.0s\n",
      "[CV] END batch_size=16, dropout=0.4, epochs=50, hidden1=64, hidden2=64, lr=0.001; total time=   0.0s\n",
      "[CV] END batch_size=16, dropout=0.4, epochs=50, hidden1=64, hidden2=64, lr=0.005; total time=   0.0s\n",
      "[CV] END batch_size=16, dropout=0.4, epochs=50, hidden1=64, hidden2=64, lr=0.005; total time=   0.0s\n",
      "[CV] END batch_size=16, dropout=0.4, epochs=50, hidden1=64, hidden2=64, lr=0.005; total time=   0.0s\n",
      "[CV] END batch_size=16, dropout=0.4, epochs=50, hidden1=128, hidden2=32, lr=0.001; total time=   0.0s\n",
      "[CV] END batch_size=16, dropout=0.4, epochs=50, hidden1=128, hidden2=32, lr=0.001; total time=   0.0s\n",
      "[CV] END batch_size=16, dropout=0.4, epochs=50, hidden1=128, hidden2=32, lr=0.001; total time=   0.0s\n",
      "[CV] END batch_size=16, dropout=0.4, epochs=50, hidden1=128, hidden2=32, lr=0.005; total time=   0.0s\n",
      "[CV] END batch_size=16, dropout=0.4, epochs=50, hidden1=128, hidden2=32, lr=0.005; total time=   0.0s\n",
      "[CV] END batch_size=16, dropout=0.4, epochs=50, hidden1=128, hidden2=32, lr=0.005; total time=   0.0s\n",
      "[CV] END batch_size=16, dropout=0.4, epochs=50, hidden1=128, hidden2=64, lr=0.001; total time=   0.0s\n",
      "[CV] END batch_size=16, dropout=0.4, epochs=50, hidden1=128, hidden2=64, lr=0.001; total time=   0.0s\n",
      "[CV] END batch_size=16, dropout=0.4, epochs=50, hidden1=128, hidden2=64, lr=0.001; total time=   0.0s\n",
      "[CV] END batch_size=16, dropout=0.4, epochs=50, hidden1=128, hidden2=64, lr=0.005; total time=   0.0s\n",
      "[CV] END batch_size=16, dropout=0.4, epochs=50, hidden1=128, hidden2=64, lr=0.005; total time=   0.0s\n",
      "[CV] END batch_size=16, dropout=0.4, epochs=50, hidden1=128, hidden2=64, lr=0.005; total time=   0.0s\n",
      "[CV] END batch_size=32, dropout=0.2, epochs=50, hidden1=64, hidden2=32, lr=0.001; total time=   0.0s\n",
      "[CV] END batch_size=32, dropout=0.2, epochs=50, hidden1=64, hidden2=32, lr=0.001; total time=   0.0s\n",
      "[CV] END batch_size=32, dropout=0.2, epochs=50, hidden1=64, hidden2=32, lr=0.001; total time=   0.0s\n",
      "[CV] END batch_size=32, dropout=0.2, epochs=50, hidden1=64, hidden2=32, lr=0.005; total time=   0.0s\n",
      "[CV] END batch_size=32, dropout=0.2, epochs=50, hidden1=64, hidden2=32, lr=0.005; total time=   0.0s\n",
      "[CV] END batch_size=32, dropout=0.2, epochs=50, hidden1=64, hidden2=32, lr=0.005; total time=   0.0s\n",
      "[CV] END batch_size=32, dropout=0.2, epochs=50, hidden1=64, hidden2=64, lr=0.001; total time=   0.0s\n",
      "[CV] END batch_size=32, dropout=0.2, epochs=50, hidden1=64, hidden2=64, lr=0.001; total time=   0.0s\n",
      "[CV] END batch_size=32, dropout=0.2, epochs=50, hidden1=64, hidden2=64, lr=0.001; total time=   0.0s\n",
      "[CV] END batch_size=32, dropout=0.2, epochs=50, hidden1=64, hidden2=64, lr=0.005; total time=   0.0s\n",
      "[CV] END batch_size=32, dropout=0.2, epochs=50, hidden1=64, hidden2=64, lr=0.005; total time=   0.0s\n",
      "[CV] END batch_size=32, dropout=0.2, epochs=50, hidden1=64, hidden2=64, lr=0.005; total time=   0.0s\n",
      "[CV] END batch_size=32, dropout=0.2, epochs=50, hidden1=128, hidden2=32, lr=0.001; total time=   0.0s\n",
      "[CV] END batch_size=32, dropout=0.2, epochs=50, hidden1=128, hidden2=32, lr=0.001; total time=   0.0s\n",
      "[CV] END batch_size=32, dropout=0.2, epochs=50, hidden1=128, hidden2=32, lr=0.001; total time=   0.0s\n",
      "[CV] END batch_size=32, dropout=0.2, epochs=50, hidden1=128, hidden2=32, lr=0.005; total time=   0.0s\n",
      "[CV] END batch_size=32, dropout=0.2, epochs=50, hidden1=128, hidden2=32, lr=0.005; total time=   0.0s\n",
      "[CV] END batch_size=32, dropout=0.2, epochs=50, hidden1=128, hidden2=32, lr=0.005; total time=   0.0s\n",
      "[CV] END batch_size=32, dropout=0.2, epochs=50, hidden1=128, hidden2=64, lr=0.001; total time=   0.0s\n",
      "[CV] END batch_size=32, dropout=0.2, epochs=50, hidden1=128, hidden2=64, lr=0.001; total time=   0.0s\n",
      "[CV] END batch_size=32, dropout=0.2, epochs=50, hidden1=128, hidden2=64, lr=0.001; total time=   0.0s\n",
      "[CV] END batch_size=32, dropout=0.2, epochs=50, hidden1=128, hidden2=64, lr=0.005; total time=   0.0s\n",
      "[CV] END batch_size=32, dropout=0.2, epochs=50, hidden1=128, hidden2=64, lr=0.005; total time=   0.0s\n",
      "[CV] END batch_size=32, dropout=0.2, epochs=50, hidden1=128, hidden2=64, lr=0.005; total time=   0.0s\n",
      "[CV] END batch_size=32, dropout=0.4, epochs=50, hidden1=64, hidden2=32, lr=0.001; total time=   0.0s\n",
      "[CV] END batch_size=32, dropout=0.4, epochs=50, hidden1=64, hidden2=32, lr=0.001; total time=   0.0s\n",
      "[CV] END batch_size=32, dropout=0.4, epochs=50, hidden1=64, hidden2=32, lr=0.001; total time=   0.0s\n",
      "[CV] END batch_size=32, dropout=0.4, epochs=50, hidden1=64, hidden2=32, lr=0.005; total time=   0.0s\n",
      "[CV] END batch_size=32, dropout=0.4, epochs=50, hidden1=64, hidden2=32, lr=0.005; total time=   0.0s\n",
      "[CV] END batch_size=32, dropout=0.4, epochs=50, hidden1=64, hidden2=32, lr=0.005; total time=   0.0s\n",
      "[CV] END batch_size=32, dropout=0.4, epochs=50, hidden1=64, hidden2=64, lr=0.001; total time=   0.0s\n",
      "[CV] END batch_size=32, dropout=0.4, epochs=50, hidden1=64, hidden2=64, lr=0.001; total time=   0.0s\n",
      "[CV] END batch_size=32, dropout=0.4, epochs=50, hidden1=64, hidden2=64, lr=0.001; total time=   0.0s\n",
      "[CV] END batch_size=32, dropout=0.4, epochs=50, hidden1=64, hidden2=64, lr=0.005; total time=   0.0s\n",
      "[CV] END batch_size=32, dropout=0.4, epochs=50, hidden1=64, hidden2=64, lr=0.005; total time=   0.0s\n",
      "[CV] END batch_size=32, dropout=0.4, epochs=50, hidden1=64, hidden2=64, lr=0.005; total time=   0.0s\n",
      "[CV] END batch_size=32, dropout=0.4, epochs=50, hidden1=128, hidden2=32, lr=0.001; total time=   0.0s\n",
      "[CV] END batch_size=32, dropout=0.4, epochs=50, hidden1=128, hidden2=32, lr=0.001; total time=   0.0s\n",
      "[CV] END batch_size=32, dropout=0.4, epochs=50, hidden1=128, hidden2=32, lr=0.001; total time=   0.0s\n",
      "[CV] END batch_size=32, dropout=0.4, epochs=50, hidden1=128, hidden2=32, lr=0.005; total time=   0.0s\n",
      "[CV] END batch_size=32, dropout=0.4, epochs=50, hidden1=128, hidden2=32, lr=0.005; total time=   0.0s\n",
      "[CV] END batch_size=32, dropout=0.4, epochs=50, hidden1=128, hidden2=32, lr=0.005; total time=   0.0s\n",
      "[CV] END batch_size=32, dropout=0.4, epochs=50, hidden1=128, hidden2=64, lr=0.001; total time=   0.0s\n",
      "[CV] END batch_size=32, dropout=0.4, epochs=50, hidden1=128, hidden2=64, lr=0.001; total time=   0.0s\n",
      "[CV] END batch_size=32, dropout=0.4, epochs=50, hidden1=128, hidden2=64, lr=0.001; total time=   0.0s\n",
      "[CV] END batch_size=32, dropout=0.4, epochs=50, hidden1=128, hidden2=64, lr=0.005; total time=   0.0s\n",
      "[CV] END batch_size=32, dropout=0.4, epochs=50, hidden1=128, hidden2=64, lr=0.005; total time=   0.0s\n",
      "[CV] END batch_size=32, dropout=0.4, epochs=50, hidden1=128, hidden2=64, lr=0.005; total time=   0.0s\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "\nAll the 96 fits failed.\nIt is very likely that your model is misconfigured.\nYou can try to debug the error by setting error_score='raise'.\n\nBelow are more details about the failures:\n--------------------------------------------------------------------------------\n5 fits failed with the following error:\nTraceback (most recent call last):\n  File \"C:\\Users\\ASUS\\anaconda3\\envs\\Env2\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 859, in _fit_and_score\n    estimator.fit(X_train, y_train, **fit_params)\n  File \"C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_28092\\1190018890.py\", line 157, in fit\n    train_model(self.model_, loader, val_loader, criterion, optimizer, max_epochs=self.epochs)\n  File \"C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_28092\\1190018890.py\", line 77, in train_model\n    loss = criterion(outputs, labels)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\ASUS\\anaconda3\\envs\\Env2\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1773, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\ASUS\\anaconda3\\envs\\Env2\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1784, in _call_impl\n    return forward_call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\ASUS\\anaconda3\\envs\\Env2\\Lib\\site-packages\\torch\\nn\\modules\\loss.py\", line 1310, in forward\n    return F.cross_entropy(\n           ^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\ASUS\\anaconda3\\envs\\Env2\\Lib\\site-packages\\torch\\nn\\functional.py\", line 3462, in cross_entropy\n    return torch._C._nn.cross_entropy_loss(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nIndexError: Target 49 is out of bounds.\n\n--------------------------------------------------------------------------------\n15 fits failed with the following error:\nTraceback (most recent call last):\n  File \"C:\\Users\\ASUS\\anaconda3\\envs\\Env2\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 859, in _fit_and_score\n    estimator.fit(X_train, y_train, **fit_params)\n  File \"C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_28092\\1190018890.py\", line 157, in fit\n    train_model(self.model_, loader, val_loader, criterion, optimizer, max_epochs=self.epochs)\n  File \"C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_28092\\1190018890.py\", line 77, in train_model\n    loss = criterion(outputs, labels)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\ASUS\\anaconda3\\envs\\Env2\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1773, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\ASUS\\anaconda3\\envs\\Env2\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1784, in _call_impl\n    return forward_call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\ASUS\\anaconda3\\envs\\Env2\\Lib\\site-packages\\torch\\nn\\modules\\loss.py\", line 1310, in forward\n    return F.cross_entropy(\n           ^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\ASUS\\anaconda3\\envs\\Env2\\Lib\\site-packages\\torch\\nn\\functional.py\", line 3462, in cross_entropy\n    return torch._C._nn.cross_entropy_loss(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nIndexError: Target 40 is out of bounds.\n\n--------------------------------------------------------------------------------\n41 fits failed with the following error:\nTraceback (most recent call last):\n  File \"C:\\Users\\ASUS\\anaconda3\\envs\\Env2\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 859, in _fit_and_score\n    estimator.fit(X_train, y_train, **fit_params)\n  File \"C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_28092\\1190018890.py\", line 157, in fit\n    train_model(self.model_, loader, val_loader, criterion, optimizer, max_epochs=self.epochs)\n  File \"C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_28092\\1190018890.py\", line 77, in train_model\n    loss = criterion(outputs, labels)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\ASUS\\anaconda3\\envs\\Env2\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1773, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\ASUS\\anaconda3\\envs\\Env2\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1784, in _call_impl\n    return forward_call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\ASUS\\anaconda3\\envs\\Env2\\Lib\\site-packages\\torch\\nn\\modules\\loss.py\", line 1310, in forward\n    return F.cross_entropy(\n           ^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\ASUS\\anaconda3\\envs\\Env2\\Lib\\site-packages\\torch\\nn\\functional.py\", line 3462, in cross_entropy\n    return torch._C._nn.cross_entropy_loss(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nIndexError: Target 47 is out of bounds.\n\n--------------------------------------------------------------------------------\n5 fits failed with the following error:\nTraceback (most recent call last):\n  File \"C:\\Users\\ASUS\\anaconda3\\envs\\Env2\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 859, in _fit_and_score\n    estimator.fit(X_train, y_train, **fit_params)\n  File \"C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_28092\\1190018890.py\", line 157, in fit\n    train_model(self.model_, loader, val_loader, criterion, optimizer, max_epochs=self.epochs)\n  File \"C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_28092\\1190018890.py\", line 77, in train_model\n    loss = criterion(outputs, labels)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\ASUS\\anaconda3\\envs\\Env2\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1773, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\ASUS\\anaconda3\\envs\\Env2\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1784, in _call_impl\n    return forward_call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\ASUS\\anaconda3\\envs\\Env2\\Lib\\site-packages\\torch\\nn\\modules\\loss.py\", line 1310, in forward\n    return F.cross_entropy(\n           ^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\ASUS\\anaconda3\\envs\\Env2\\Lib\\site-packages\\torch\\nn\\functional.py\", line 3462, in cross_entropy\n    return torch._C._nn.cross_entropy_loss(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nIndexError: Target 48 is out of bounds.\n\n--------------------------------------------------------------------------------\n5 fits failed with the following error:\nTraceback (most recent call last):\n  File \"C:\\Users\\ASUS\\anaconda3\\envs\\Env2\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 859, in _fit_and_score\n    estimator.fit(X_train, y_train, **fit_params)\n  File \"C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_28092\\1190018890.py\", line 157, in fit\n    train_model(self.model_, loader, val_loader, criterion, optimizer, max_epochs=self.epochs)\n  File \"C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_28092\\1190018890.py\", line 77, in train_model\n    loss = criterion(outputs, labels)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\ASUS\\anaconda3\\envs\\Env2\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1773, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\ASUS\\anaconda3\\envs\\Env2\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1784, in _call_impl\n    return forward_call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\ASUS\\anaconda3\\envs\\Env2\\Lib\\site-packages\\torch\\nn\\modules\\loss.py\", line 1310, in forward\n    return F.cross_entropy(\n           ^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\ASUS\\anaconda3\\envs\\Env2\\Lib\\site-packages\\torch\\nn\\functional.py\", line 3462, in cross_entropy\n    return torch._C._nn.cross_entropy_loss(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nIndexError: Target 46 is out of bounds.\n\n--------------------------------------------------------------------------------\n5 fits failed with the following error:\nTraceback (most recent call last):\n  File \"C:\\Users\\ASUS\\anaconda3\\envs\\Env2\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 859, in _fit_and_score\n    estimator.fit(X_train, y_train, **fit_params)\n  File \"C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_28092\\1190018890.py\", line 157, in fit\n    train_model(self.model_, loader, val_loader, criterion, optimizer, max_epochs=self.epochs)\n  File \"C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_28092\\1190018890.py\", line 77, in train_model\n    loss = criterion(outputs, labels)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\ASUS\\anaconda3\\envs\\Env2\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1773, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\ASUS\\anaconda3\\envs\\Env2\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1784, in _call_impl\n    return forward_call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\ASUS\\anaconda3\\envs\\Env2\\Lib\\site-packages\\torch\\nn\\modules\\loss.py\", line 1310, in forward\n    return F.cross_entropy(\n           ^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\ASUS\\anaconda3\\envs\\Env2\\Lib\\site-packages\\torch\\nn\\functional.py\", line 3462, in cross_entropy\n    return torch._C._nn.cross_entropy_loss(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nIndexError: Target 44 is out of bounds.\n\n--------------------------------------------------------------------------------\n8 fits failed with the following error:\nTraceback (most recent call last):\n  File \"C:\\Users\\ASUS\\anaconda3\\envs\\Env2\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 859, in _fit_and_score\n    estimator.fit(X_train, y_train, **fit_params)\n  File \"C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_28092\\1190018890.py\", line 157, in fit\n    train_model(self.model_, loader, val_loader, criterion, optimizer, max_epochs=self.epochs)\n  File \"C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_28092\\1190018890.py\", line 77, in train_model\n    loss = criterion(outputs, labels)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\ASUS\\anaconda3\\envs\\Env2\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1773, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\ASUS\\anaconda3\\envs\\Env2\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1784, in _call_impl\n    return forward_call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\ASUS\\anaconda3\\envs\\Env2\\Lib\\site-packages\\torch\\nn\\modules\\loss.py\", line 1310, in forward\n    return F.cross_entropy(\n           ^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\ASUS\\anaconda3\\envs\\Env2\\Lib\\site-packages\\torch\\nn\\functional.py\", line 3462, in cross_entropy\n    return torch._C._nn.cross_entropy_loss(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nIndexError: Target 39 is out of bounds.\n\n--------------------------------------------------------------------------------\n4 fits failed with the following error:\nTraceback (most recent call last):\n  File \"C:\\Users\\ASUS\\anaconda3\\envs\\Env2\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 859, in _fit_and_score\n    estimator.fit(X_train, y_train, **fit_params)\n  File \"C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_28092\\1190018890.py\", line 157, in fit\n    train_model(self.model_, loader, val_loader, criterion, optimizer, max_epochs=self.epochs)\n  File \"C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_28092\\1190018890.py\", line 77, in train_model\n    loss = criterion(outputs, labels)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\ASUS\\anaconda3\\envs\\Env2\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1773, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\ASUS\\anaconda3\\envs\\Env2\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1784, in _call_impl\n    return forward_call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\ASUS\\anaconda3\\envs\\Env2\\Lib\\site-packages\\torch\\nn\\modules\\loss.py\", line 1310, in forward\n    return F.cross_entropy(\n           ^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\ASUS\\anaconda3\\envs\\Env2\\Lib\\site-packages\\torch\\nn\\functional.py\", line 3462, in cross_entropy\n    return torch._C._nn.cross_entropy_loss(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nIndexError: Target 42 is out of bounds.\n\n--------------------------------------------------------------------------------\n5 fits failed with the following error:\nTraceback (most recent call last):\n  File \"C:\\Users\\ASUS\\anaconda3\\envs\\Env2\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 859, in _fit_and_score\n    estimator.fit(X_train, y_train, **fit_params)\n  File \"C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_28092\\1190018890.py\", line 157, in fit\n    train_model(self.model_, loader, val_loader, criterion, optimizer, max_epochs=self.epochs)\n  File \"C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_28092\\1190018890.py\", line 77, in train_model\n    loss = criterion(outputs, labels)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\ASUS\\anaconda3\\envs\\Env2\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1773, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\ASUS\\anaconda3\\envs\\Env2\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1784, in _call_impl\n    return forward_call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\ASUS\\anaconda3\\envs\\Env2\\Lib\\site-packages\\torch\\nn\\modules\\loss.py\", line 1310, in forward\n    return F.cross_entropy(\n           ^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\ASUS\\anaconda3\\envs\\Env2\\Lib\\site-packages\\torch\\nn\\functional.py\", line 3462, in cross_entropy\n    return torch._C._nn.cross_entropy_loss(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nIndexError: Target 43 is out of bounds.\n\n--------------------------------------------------------------------------------\n2 fits failed with the following error:\nTraceback (most recent call last):\n  File \"C:\\Users\\ASUS\\anaconda3\\envs\\Env2\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 859, in _fit_and_score\n    estimator.fit(X_train, y_train, **fit_params)\n  File \"C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_28092\\1190018890.py\", line 157, in fit\n    train_model(self.model_, loader, val_loader, criterion, optimizer, max_epochs=self.epochs)\n  File \"C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_28092\\1190018890.py\", line 77, in train_model\n    loss = criterion(outputs, labels)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\ASUS\\anaconda3\\envs\\Env2\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1773, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\ASUS\\anaconda3\\envs\\Env2\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1784, in _call_impl\n    return forward_call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\ASUS\\anaconda3\\envs\\Env2\\Lib\\site-packages\\torch\\nn\\modules\\loss.py\", line 1310, in forward\n    return F.cross_entropy(\n           ^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\ASUS\\anaconda3\\envs\\Env2\\Lib\\site-packages\\torch\\nn\\functional.py\", line 3462, in cross_entropy\n    return torch._C._nn.cross_entropy_loss(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nIndexError: Target 45 is out of bounds.\n\n--------------------------------------------------------------------------------\n1 fits failed with the following error:\nTraceback (most recent call last):\n  File \"C:\\Users\\ASUS\\anaconda3\\envs\\Env2\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 859, in _fit_and_score\n    estimator.fit(X_train, y_train, **fit_params)\n  File \"C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_28092\\1190018890.py\", line 157, in fit\n    train_model(self.model_, loader, val_loader, criterion, optimizer, max_epochs=self.epochs)\n  File \"C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_28092\\1190018890.py\", line 77, in train_model\n    loss = criterion(outputs, labels)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\ASUS\\anaconda3\\envs\\Env2\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1773, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\ASUS\\anaconda3\\envs\\Env2\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1784, in _call_impl\n    return forward_call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\ASUS\\anaconda3\\envs\\Env2\\Lib\\site-packages\\torch\\nn\\modules\\loss.py\", line 1310, in forward\n    return F.cross_entropy(\n           ^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\ASUS\\anaconda3\\envs\\Env2\\Lib\\site-packages\\torch\\nn\\functional.py\", line 3462, in cross_entropy\n    return torch._C._nn.cross_entropy_loss(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nIndexError: Target 41 is out of bounds.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[19]\u001b[39m\u001b[32m, line 179\u001b[39m\n\u001b[32m    169\u001b[39m param_grid = {\n\u001b[32m    170\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mhidden1\u001b[39m\u001b[33m'\u001b[39m: [\u001b[32m64\u001b[39m, \u001b[32m128\u001b[39m],\n\u001b[32m    171\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mhidden2\u001b[39m\u001b[33m'\u001b[39m: [\u001b[32m32\u001b[39m, \u001b[32m64\u001b[39m],\n\u001b[32m   (...)\u001b[39m\u001b[32m    175\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mepochs\u001b[39m\u001b[33m'\u001b[39m: [\u001b[32m50\u001b[39m]\n\u001b[32m    176\u001b[39m }\n\u001b[32m    178\u001b[39m grid_model = GridSearchCV(SklearnTorchWrapper(), param_grid, cv=\u001b[32m3\u001b[39m, verbose=\u001b[32m2\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m179\u001b[39m \u001b[43mgrid_model\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    180\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mGridSearchCV Best Params:\u001b[39m\u001b[33m\"\u001b[39m, grid_model.best_params_)\n\u001b[32m    182\u001b[39m \u001b[38;5;66;03m# ----------------------------------------------------------\u001b[39;00m\n\u001b[32m    183\u001b[39m \u001b[38;5;66;03m# 6. FINAL MODEL TRAINING USING BEST PARAMS (Optuna)\u001b[39;00m\n\u001b[32m    184\u001b[39m \u001b[38;5;66;03m# ----------------------------------------------------------\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\Env2\\Lib\\site-packages\\sklearn\\base.py:1365\u001b[39m, in \u001b[36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(estimator, *args, **kwargs)\u001b[39m\n\u001b[32m   1358\u001b[39m     estimator._validate_params()\n\u001b[32m   1360\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[32m   1361\u001b[39m     skip_parameter_validation=(\n\u001b[32m   1362\u001b[39m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[32m   1363\u001b[39m     )\n\u001b[32m   1364\u001b[39m ):\n\u001b[32m-> \u001b[39m\u001b[32m1365\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\Env2\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:1051\u001b[39m, in \u001b[36mBaseSearchCV.fit\u001b[39m\u001b[34m(self, X, y, **params)\u001b[39m\n\u001b[32m   1045\u001b[39m     results = \u001b[38;5;28mself\u001b[39m._format_results(\n\u001b[32m   1046\u001b[39m         all_candidate_params, n_splits, all_out, all_more_results\n\u001b[32m   1047\u001b[39m     )\n\u001b[32m   1049\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m results\n\u001b[32m-> \u001b[39m\u001b[32m1051\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_run_search\u001b[49m\u001b[43m(\u001b[49m\u001b[43mevaluate_candidates\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1053\u001b[39m \u001b[38;5;66;03m# multimetric is determined here because in the case of a callable\u001b[39;00m\n\u001b[32m   1054\u001b[39m \u001b[38;5;66;03m# self.scoring the return type is only known after calling\u001b[39;00m\n\u001b[32m   1055\u001b[39m first_test_score = all_out[\u001b[32m0\u001b[39m][\u001b[33m\"\u001b[39m\u001b[33mtest_scores\u001b[39m\u001b[33m\"\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\Env2\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:1605\u001b[39m, in \u001b[36mGridSearchCV._run_search\u001b[39m\u001b[34m(self, evaluate_candidates)\u001b[39m\n\u001b[32m   1603\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_run_search\u001b[39m(\u001b[38;5;28mself\u001b[39m, evaluate_candidates):\n\u001b[32m   1604\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Search all candidates in param_grid\"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1605\u001b[39m     \u001b[43mevaluate_candidates\u001b[49m\u001b[43m(\u001b[49m\u001b[43mParameterGrid\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mparam_grid\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\Env2\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:1028\u001b[39m, in \u001b[36mBaseSearchCV.fit.<locals>.evaluate_candidates\u001b[39m\u001b[34m(candidate_params, cv, more_results)\u001b[39m\n\u001b[32m   1021\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(out) != n_candidates * n_splits:\n\u001b[32m   1022\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   1023\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mcv.split and cv.get_n_splits returned \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1024\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33minconsistent results. Expected \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[33m \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1025\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33msplits, got \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[33m\"\u001b[39m.format(n_splits, \u001b[38;5;28mlen\u001b[39m(out) // n_candidates)\n\u001b[32m   1026\u001b[39m     )\n\u001b[32m-> \u001b[39m\u001b[32m1028\u001b[39m \u001b[43m_warn_or_raise_about_fit_failures\u001b[49m\u001b[43m(\u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43merror_score\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1030\u001b[39m \u001b[38;5;66;03m# For callable self.scoring, the return type is only know after\u001b[39;00m\n\u001b[32m   1031\u001b[39m \u001b[38;5;66;03m# calling. If the return type is a dictionary, the error scores\u001b[39;00m\n\u001b[32m   1032\u001b[39m \u001b[38;5;66;03m# can now be inserted with the correct key. The type checking\u001b[39;00m\n\u001b[32m   1033\u001b[39m \u001b[38;5;66;03m# of out will be done in `_insert_error_scores`.\u001b[39;00m\n\u001b[32m   1034\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(\u001b[38;5;28mself\u001b[39m.scoring):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\Env2\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py:505\u001b[39m, in \u001b[36m_warn_or_raise_about_fit_failures\u001b[39m\u001b[34m(results, error_score)\u001b[39m\n\u001b[32m    498\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m num_failed_fits == num_fits:\n\u001b[32m    499\u001b[39m     all_fits_failed_message = (\n\u001b[32m    500\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mAll the \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_fits\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m fits failed.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    501\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mIt is very likely that your model is misconfigured.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    502\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mYou can try to debug the error by setting error_score=\u001b[39m\u001b[33m'\u001b[39m\u001b[33mraise\u001b[39m\u001b[33m'\u001b[39m\u001b[33m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    503\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mBelow are more details about the failures:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mfit_errors_summary\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    504\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m505\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(all_fits_failed_message)\n\u001b[32m    507\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    508\u001b[39m     some_fits_failed_message = (\n\u001b[32m    509\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mnum_failed_fits\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m fits failed out of a total of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_fits\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    510\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mThe score on these train-test partitions for these parameters\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   (...)\u001b[39m\u001b[32m    514\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mBelow are more details about the failures:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mfit_errors_summary\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    515\u001b[39m     )\n",
      "\u001b[31mValueError\u001b[39m: \nAll the 96 fits failed.\nIt is very likely that your model is misconfigured.\nYou can try to debug the error by setting error_score='raise'.\n\nBelow are more details about the failures:\n--------------------------------------------------------------------------------\n5 fits failed with the following error:\nTraceback (most recent call last):\n  File \"C:\\Users\\ASUS\\anaconda3\\envs\\Env2\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 859, in _fit_and_score\n    estimator.fit(X_train, y_train, **fit_params)\n  File \"C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_28092\\1190018890.py\", line 157, in fit\n    train_model(self.model_, loader, val_loader, criterion, optimizer, max_epochs=self.epochs)\n  File \"C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_28092\\1190018890.py\", line 77, in train_model\n    loss = criterion(outputs, labels)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\ASUS\\anaconda3\\envs\\Env2\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1773, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\ASUS\\anaconda3\\envs\\Env2\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1784, in _call_impl\n    return forward_call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\ASUS\\anaconda3\\envs\\Env2\\Lib\\site-packages\\torch\\nn\\modules\\loss.py\", line 1310, in forward\n    return F.cross_entropy(\n           ^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\ASUS\\anaconda3\\envs\\Env2\\Lib\\site-packages\\torch\\nn\\functional.py\", line 3462, in cross_entropy\n    return torch._C._nn.cross_entropy_loss(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nIndexError: Target 49 is out of bounds.\n\n--------------------------------------------------------------------------------\n15 fits failed with the following error:\nTraceback (most recent call last):\n  File \"C:\\Users\\ASUS\\anaconda3\\envs\\Env2\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 859, in _fit_and_score\n    estimator.fit(X_train, y_train, **fit_params)\n  File \"C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_28092\\1190018890.py\", line 157, in fit\n    train_model(self.model_, loader, val_loader, criterion, optimizer, max_epochs=self.epochs)\n  File \"C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_28092\\1190018890.py\", line 77, in train_model\n    loss = criterion(outputs, labels)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\ASUS\\anaconda3\\envs\\Env2\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1773, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\ASUS\\anaconda3\\envs\\Env2\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1784, in _call_impl\n    return forward_call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\ASUS\\anaconda3\\envs\\Env2\\Lib\\site-packages\\torch\\nn\\modules\\loss.py\", line 1310, in forward\n    return F.cross_entropy(\n           ^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\ASUS\\anaconda3\\envs\\Env2\\Lib\\site-packages\\torch\\nn\\functional.py\", line 3462, in cross_entropy\n    return torch._C._nn.cross_entropy_loss(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nIndexError: Target 40 is out of bounds.\n\n--------------------------------------------------------------------------------\n41 fits failed with the following error:\nTraceback (most recent call last):\n  File \"C:\\Users\\ASUS\\anaconda3\\envs\\Env2\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 859, in _fit_and_score\n    estimator.fit(X_train, y_train, **fit_params)\n  File \"C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_28092\\1190018890.py\", line 157, in fit\n    train_model(self.model_, loader, val_loader, criterion, optimizer, max_epochs=self.epochs)\n  File \"C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_28092\\1190018890.py\", line 77, in train_model\n    loss = criterion(outputs, labels)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\ASUS\\anaconda3\\envs\\Env2\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1773, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\ASUS\\anaconda3\\envs\\Env2\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1784, in _call_impl\n    return forward_call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\ASUS\\anaconda3\\envs\\Env2\\Lib\\site-packages\\torch\\nn\\modules\\loss.py\", line 1310, in forward\n    return F.cross_entropy(\n           ^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\ASUS\\anaconda3\\envs\\Env2\\Lib\\site-packages\\torch\\nn\\functional.py\", line 3462, in cross_entropy\n    return torch._C._nn.cross_entropy_loss(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nIndexError: Target 47 is out of bounds.\n\n--------------------------------------------------------------------------------\n5 fits failed with the following error:\nTraceback (most recent call last):\n  File \"C:\\Users\\ASUS\\anaconda3\\envs\\Env2\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 859, in _fit_and_score\n    estimator.fit(X_train, y_train, **fit_params)\n  File \"C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_28092\\1190018890.py\", line 157, in fit\n    train_model(self.model_, loader, val_loader, criterion, optimizer, max_epochs=self.epochs)\n  File \"C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_28092\\1190018890.py\", line 77, in train_model\n    loss = criterion(outputs, labels)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\ASUS\\anaconda3\\envs\\Env2\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1773, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\ASUS\\anaconda3\\envs\\Env2\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1784, in _call_impl\n    return forward_call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\ASUS\\anaconda3\\envs\\Env2\\Lib\\site-packages\\torch\\nn\\modules\\loss.py\", line 1310, in forward\n    return F.cross_entropy(\n           ^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\ASUS\\anaconda3\\envs\\Env2\\Lib\\site-packages\\torch\\nn\\functional.py\", line 3462, in cross_entropy\n    return torch._C._nn.cross_entropy_loss(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nIndexError: Target 48 is out of bounds.\n\n--------------------------------------------------------------------------------\n5 fits failed with the following error:\nTraceback (most recent call last):\n  File \"C:\\Users\\ASUS\\anaconda3\\envs\\Env2\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 859, in _fit_and_score\n    estimator.fit(X_train, y_train, **fit_params)\n  File \"C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_28092\\1190018890.py\", line 157, in fit\n    train_model(self.model_, loader, val_loader, criterion, optimizer, max_epochs=self.epochs)\n  File \"C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_28092\\1190018890.py\", line 77, in train_model\n    loss = criterion(outputs, labels)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\ASUS\\anaconda3\\envs\\Env2\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1773, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\ASUS\\anaconda3\\envs\\Env2\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1784, in _call_impl\n    return forward_call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\ASUS\\anaconda3\\envs\\Env2\\Lib\\site-packages\\torch\\nn\\modules\\loss.py\", line 1310, in forward\n    return F.cross_entropy(\n           ^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\ASUS\\anaconda3\\envs\\Env2\\Lib\\site-packages\\torch\\nn\\functional.py\", line 3462, in cross_entropy\n    return torch._C._nn.cross_entropy_loss(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nIndexError: Target 46 is out of bounds.\n\n--------------------------------------------------------------------------------\n5 fits failed with the following error:\nTraceback (most recent call last):\n  File \"C:\\Users\\ASUS\\anaconda3\\envs\\Env2\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 859, in _fit_and_score\n    estimator.fit(X_train, y_train, **fit_params)\n  File \"C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_28092\\1190018890.py\", line 157, in fit\n    train_model(self.model_, loader, val_loader, criterion, optimizer, max_epochs=self.epochs)\n  File \"C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_28092\\1190018890.py\", line 77, in train_model\n    loss = criterion(outputs, labels)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\ASUS\\anaconda3\\envs\\Env2\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1773, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\ASUS\\anaconda3\\envs\\Env2\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1784, in _call_impl\n    return forward_call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\ASUS\\anaconda3\\envs\\Env2\\Lib\\site-packages\\torch\\nn\\modules\\loss.py\", line 1310, in forward\n    return F.cross_entropy(\n           ^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\ASUS\\anaconda3\\envs\\Env2\\Lib\\site-packages\\torch\\nn\\functional.py\", line 3462, in cross_entropy\n    return torch._C._nn.cross_entropy_loss(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nIndexError: Target 44 is out of bounds.\n\n--------------------------------------------------------------------------------\n8 fits failed with the following error:\nTraceback (most recent call last):\n  File \"C:\\Users\\ASUS\\anaconda3\\envs\\Env2\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 859, in _fit_and_score\n    estimator.fit(X_train, y_train, **fit_params)\n  File \"C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_28092\\1190018890.py\", line 157, in fit\n    train_model(self.model_, loader, val_loader, criterion, optimizer, max_epochs=self.epochs)\n  File \"C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_28092\\1190018890.py\", line 77, in train_model\n    loss = criterion(outputs, labels)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\ASUS\\anaconda3\\envs\\Env2\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1773, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\ASUS\\anaconda3\\envs\\Env2\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1784, in _call_impl\n    return forward_call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\ASUS\\anaconda3\\envs\\Env2\\Lib\\site-packages\\torch\\nn\\modules\\loss.py\", line 1310, in forward\n    return F.cross_entropy(\n           ^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\ASUS\\anaconda3\\envs\\Env2\\Lib\\site-packages\\torch\\nn\\functional.py\", line 3462, in cross_entropy\n    return torch._C._nn.cross_entropy_loss(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nIndexError: Target 39 is out of bounds.\n\n--------------------------------------------------------------------------------\n4 fits failed with the following error:\nTraceback (most recent call last):\n  File \"C:\\Users\\ASUS\\anaconda3\\envs\\Env2\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 859, in _fit_and_score\n    estimator.fit(X_train, y_train, **fit_params)\n  File \"C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_28092\\1190018890.py\", line 157, in fit\n    train_model(self.model_, loader, val_loader, criterion, optimizer, max_epochs=self.epochs)\n  File \"C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_28092\\1190018890.py\", line 77, in train_model\n    loss = criterion(outputs, labels)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\ASUS\\anaconda3\\envs\\Env2\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1773, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\ASUS\\anaconda3\\envs\\Env2\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1784, in _call_impl\n    return forward_call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\ASUS\\anaconda3\\envs\\Env2\\Lib\\site-packages\\torch\\nn\\modules\\loss.py\", line 1310, in forward\n    return F.cross_entropy(\n           ^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\ASUS\\anaconda3\\envs\\Env2\\Lib\\site-packages\\torch\\nn\\functional.py\", line 3462, in cross_entropy\n    return torch._C._nn.cross_entropy_loss(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nIndexError: Target 42 is out of bounds.\n\n--------------------------------------------------------------------------------\n5 fits failed with the following error:\nTraceback (most recent call last):\n  File \"C:\\Users\\ASUS\\anaconda3\\envs\\Env2\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 859, in _fit_and_score\n    estimator.fit(X_train, y_train, **fit_params)\n  File \"C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_28092\\1190018890.py\", line 157, in fit\n    train_model(self.model_, loader, val_loader, criterion, optimizer, max_epochs=self.epochs)\n  File \"C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_28092\\1190018890.py\", line 77, in train_model\n    loss = criterion(outputs, labels)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\ASUS\\anaconda3\\envs\\Env2\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1773, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\ASUS\\anaconda3\\envs\\Env2\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1784, in _call_impl\n    return forward_call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\ASUS\\anaconda3\\envs\\Env2\\Lib\\site-packages\\torch\\nn\\modules\\loss.py\", line 1310, in forward\n    return F.cross_entropy(\n           ^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\ASUS\\anaconda3\\envs\\Env2\\Lib\\site-packages\\torch\\nn\\functional.py\", line 3462, in cross_entropy\n    return torch._C._nn.cross_entropy_loss(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nIndexError: Target 43 is out of bounds.\n\n--------------------------------------------------------------------------------\n2 fits failed with the following error:\nTraceback (most recent call last):\n  File \"C:\\Users\\ASUS\\anaconda3\\envs\\Env2\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 859, in _fit_and_score\n    estimator.fit(X_train, y_train, **fit_params)\n  File \"C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_28092\\1190018890.py\", line 157, in fit\n    train_model(self.model_, loader, val_loader, criterion, optimizer, max_epochs=self.epochs)\n  File \"C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_28092\\1190018890.py\", line 77, in train_model\n    loss = criterion(outputs, labels)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\ASUS\\anaconda3\\envs\\Env2\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1773, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\ASUS\\anaconda3\\envs\\Env2\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1784, in _call_impl\n    return forward_call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\ASUS\\anaconda3\\envs\\Env2\\Lib\\site-packages\\torch\\nn\\modules\\loss.py\", line 1310, in forward\n    return F.cross_entropy(\n           ^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\ASUS\\anaconda3\\envs\\Env2\\Lib\\site-packages\\torch\\nn\\functional.py\", line 3462, in cross_entropy\n    return torch._C._nn.cross_entropy_loss(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nIndexError: Target 45 is out of bounds.\n\n--------------------------------------------------------------------------------\n1 fits failed with the following error:\nTraceback (most recent call last):\n  File \"C:\\Users\\ASUS\\anaconda3\\envs\\Env2\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 859, in _fit_and_score\n    estimator.fit(X_train, y_train, **fit_params)\n  File \"C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_28092\\1190018890.py\", line 157, in fit\n    train_model(self.model_, loader, val_loader, criterion, optimizer, max_epochs=self.epochs)\n  File \"C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_28092\\1190018890.py\", line 77, in train_model\n    loss = criterion(outputs, labels)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\ASUS\\anaconda3\\envs\\Env2\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1773, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\ASUS\\anaconda3\\envs\\Env2\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1784, in _call_impl\n    return forward_call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\ASUS\\anaconda3\\envs\\Env2\\Lib\\site-packages\\torch\\nn\\modules\\loss.py\", line 1310, in forward\n    return F.cross_entropy(\n           ^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\ASUS\\anaconda3\\envs\\Env2\\Lib\\site-packages\\torch\\nn\\functional.py\", line 3462, in cross_entropy\n    return torch._C._nn.cross_entropy_loss(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nIndexError: Target 41 is out of bounds.\n"
     ]
    }
   ],
   "source": [
    "# ----------------------------------------------------------\n",
    "# IMPORTS\n",
    "# ----------------------------------------------------------\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import optuna\n",
    "\n",
    "# ----------------------------------------------------------\n",
    "# 1. LOAD & PREPROCESS DATA\n",
    "# ----------------------------------------------------------\n",
    "df = pd.read_csv(\"weather.csv\")\n",
    "df = df.drop('Date/Time', axis=1)\n",
    "X = df.drop(\"Weather\", axis=1)\n",
    "y = df[\"Weather\"]\n",
    "\n",
    "le = LabelEncoder()\n",
    "y = le.fit_transform(y)\n",
    "\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_valid = scaler.transform(X_valid)\n",
    "\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.long)\n",
    "X_valid_tensor = torch.tensor(X_valid, dtype=torch.float32)\n",
    "y_valid_tensor = torch.tensor(y_valid, dtype=torch.long)\n",
    "\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "valid_dataset = TensorDataset(X_valid_tensor, y_valid_tensor)\n",
    "\n",
    "input_dim = X_train.shape[1]\n",
    "num_classes = len(np.unique(y))\n",
    "\n",
    "# ----------------------------------------------------------\n",
    "# 2. DEFINE PYTORCH MODEL\n",
    "# ----------------------------------------------------------\n",
    "class WeatherNN(nn.Module):\n",
    "    def __init__(self, input_dim, num_classes, hidden1=128, hidden2=64, dropout=0.3):\n",
    "        super(WeatherNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden1)\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        self.fc2 = nn.Linear(hidden1, hidden2)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "        self.fc3 = nn.Linear(hidden2, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.dropout1(x)\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = self.dropout2(x)\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "# ----------------------------------------------------------\n",
    "# 3. TRAINING FUNCTION\n",
    "# ----------------------------------------------------------\n",
    "def train_model(model, train_loader, valid_loader, criterion, optimizer, patience=10, max_epochs=200):\n",
    "    best_val_loss = float('inf')\n",
    "    patience_counter = 0\n",
    "    best_model_state = model.state_dict()  # initialize\n",
    "\n",
    "    for epoch in range(max_epochs):\n",
    "        model.train()\n",
    "        for inputs, labels in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        correct, total = 0, 0\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in valid_loader:\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                val_loss += loss.item()\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "\n",
    "        avg_val_loss = val_loss / len(valid_loader)\n",
    "        if avg_val_loss < best_val_loss:\n",
    "            best_val_loss = avg_val_loss\n",
    "            patience_counter = 0\n",
    "            best_model_state = model.state_dict()\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= patience:\n",
    "                break\n",
    "\n",
    "    model.load_state_dict(best_model_state)\n",
    "    val_acc = 100 * correct / total\n",
    "    return best_val_loss, val_acc\n",
    "\n",
    "# ----------------------------------------------------------\n",
    "# 4. OPTUNA HYPERPARAMETER TUNING\n",
    "# ----------------------------------------------------------\n",
    "def objective(trial):\n",
    "    hidden1 = trial.suggest_int(\"hidden1\", 64, 256)\n",
    "    hidden2 = trial.suggest_int(\"hidden2\", 32, 128)\n",
    "    dropout = trial.suggest_float(\"dropout\", 0.2, 0.6)\n",
    "    lr = trial.suggest_float(\"lr\", 1e-4, 1e-2, log=True)\n",
    "    batch_size = trial.suggest_categorical(\"batch_size\", [16, 32, 64])\n",
    "\n",
    "    model = WeatherNN(input_dim, num_classes, hidden1, hidden2, dropout)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    valid_loader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    val_loss, val_acc = train_model(model, train_loader, valid_loader, criterion, optimizer)\n",
    "    return val_loss\n",
    "\n",
    "study = optuna.create_study(direction=\"minimize\")\n",
    "study.optimize(objective, n_trials=25)\n",
    "print(\"Optuna Best Params:\", study.best_params)\n",
    "\n",
    "# ----------------------------------------------------------\n",
    "# 5. GRIDSEARCHCV WITH SKLEARN WRAPPER\n",
    "# ----------------------------------------------------------\n",
    "class SklearnTorchWrapper(BaseEstimator, ClassifierMixin):\n",
    "    def __init__(self, hidden1=128, hidden2=64, dropout=0.3, lr=0.001, batch_size=32, epochs=50):\n",
    "        self.hidden1 = hidden1\n",
    "        self.hidden2 = hidden2\n",
    "        self.dropout = dropout\n",
    "        self.lr = lr\n",
    "        self.batch_size = batch_size\n",
    "        self.epochs = epochs\n",
    "        self.model_ = None\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        X_tensor = torch.tensor(X, dtype=torch.float32)\n",
    "        y_tensor = torch.tensor(y, dtype=torch.long)\n",
    "        dataset = TensorDataset(X_tensor, y_tensor)\n",
    "        loader = DataLoader(dataset, batch_size=self.batch_size, shuffle=True)\n",
    "\n",
    "        self.model_ = WeatherNN(X.shape[1], len(np.unique(y)), self.hidden1, self.hidden2, self.dropout)\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        optimizer = optim.Adam(self.model_.parameters(), lr=self.lr)\n",
    "\n",
    "        # Dummy validation loader\n",
    "        val_loader = DataLoader(dataset, batch_size=self.batch_size, shuffle=False)\n",
    "        train_model(self.model_, loader, val_loader, criterion, optimizer, max_epochs=self.epochs)\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        self.model_.eval()\n",
    "        X_tensor = torch.tensor(X, dtype=torch.float32)\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model_(X_tensor)\n",
    "            _, y_pred = torch.max(outputs, 1)\n",
    "        return y_pred.numpy()\n",
    "\n",
    "# Define parameter grid for GridSearchCV\n",
    "param_grid = {\n",
    "    'hidden1': [64, 128],\n",
    "    'hidden2': [32, 64],\n",
    "    'dropout': [0.2, 0.4],\n",
    "    'lr': [0.001, 0.005],\n",
    "    'batch_size': [16, 32],\n",
    "    'epochs': [50]\n",
    "}\n",
    "\n",
    "grid_model = GridSearchCV(SklearnTorchWrapper(), param_grid, cv=3, verbose=2)\n",
    "grid_model.fit(X_train, y_train)\n",
    "print(\"GridSearchCV Best Params:\", grid_model.best_params_)\n",
    "\n",
    "# ----------------------------------------------------------\n",
    "# 6. FINAL MODEL TRAINING USING BEST PARAMS (Optuna)\n",
    "# ----------------------------------------------------------\n",
    "best_params = study.best_params\n",
    "model = WeatherNN(input_dim, num_classes,\n",
    "                  hidden1=best_params['hidden1'],\n",
    "                  hidden2=best_params['hidden2'],\n",
    "                  dropout=best_params['dropout'])\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=best_params['lr'])\n",
    "train_loader = DataLoader(train_dataset, batch_size=best_params['batch_size'], shuffle=True)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=best_params['batch_size'], shuffle=False)\n",
    "\n",
    "val_loss, val_acc = train_model(model, train_loader, valid_loader, criterion, optimizer)\n",
    "print(f\"Final Model - Validation Accuracy: {val_acc:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc1d4937-0277-4c33-875f-f01cf54c7ae5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a5d249f7-95e8-4af0-b939-f3ba7cc36754",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"['target_column'] not found in axis\"",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[23]\u001b[39m\u001b[32m, line 16\u001b[39m\n\u001b[32m     13\u001b[39m \u001b[38;5;66;03m# ------------------- 1. LOAD DATA -------------------\u001b[39;00m\n\u001b[32m     14\u001b[39m df = pd.read_csv(\u001b[33m\"\u001b[39m\u001b[33mweather.csv\u001b[39m\u001b[33m\"\u001b[39m)  \u001b[38;5;66;03m# replace with your dataset path\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m16\u001b[39m X = \u001b[43mdf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdrop\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtarget_column\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m.values  \u001b[38;5;66;03m# replace with your target column\u001b[39;00m\n\u001b[32m     17\u001b[39m y = df[\u001b[33m\"\u001b[39m\u001b[33mtarget_column\u001b[39m\u001b[33m\"\u001b[39m].values\n\u001b[32m     19\u001b[39m \u001b[38;5;66;03m# ------------------- 2. LABEL ENCODING -------------------\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\Env2\\Lib\\site-packages\\pandas\\core\\frame.py:5603\u001b[39m, in \u001b[36mDataFrame.drop\u001b[39m\u001b[34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[39m\n\u001b[32m   5455\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdrop\u001b[39m(\n\u001b[32m   5456\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   5457\u001b[39m     labels: IndexLabel | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   5464\u001b[39m     errors: IgnoreRaise = \u001b[33m\"\u001b[39m\u001b[33mraise\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   5465\u001b[39m ) -> DataFrame | \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   5466\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   5467\u001b[39m \u001b[33;03m    Drop specified labels from rows or columns.\u001b[39;00m\n\u001b[32m   5468\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m   5601\u001b[39m \u001b[33;03m            weight  1.0     0.8\u001b[39;00m\n\u001b[32m   5602\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m5603\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdrop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   5604\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5605\u001b[39m \u001b[43m        \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m=\u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5606\u001b[39m \u001b[43m        \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m=\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5607\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5608\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5609\u001b[39m \u001b[43m        \u001b[49m\u001b[43minplace\u001b[49m\u001b[43m=\u001b[49m\u001b[43minplace\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5610\u001b[39m \u001b[43m        \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5611\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\Env2\\Lib\\site-packages\\pandas\\core\\generic.py:4810\u001b[39m, in \u001b[36mNDFrame.drop\u001b[39m\u001b[34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[39m\n\u001b[32m   4808\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m axis, labels \u001b[38;5;129;01min\u001b[39;00m axes.items():\n\u001b[32m   4809\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m labels \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m4810\u001b[39m         obj = \u001b[43mobj\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_drop_axis\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[43merrors\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4812\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m inplace:\n\u001b[32m   4813\u001b[39m     \u001b[38;5;28mself\u001b[39m._update_inplace(obj)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\Env2\\Lib\\site-packages\\pandas\\core\\generic.py:4852\u001b[39m, in \u001b[36mNDFrame._drop_axis\u001b[39m\u001b[34m(self, labels, axis, level, errors, only_slice)\u001b[39m\n\u001b[32m   4850\u001b[39m         new_axis = axis.drop(labels, level=level, errors=errors)\n\u001b[32m   4851\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m4852\u001b[39m         new_axis = \u001b[43maxis\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdrop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[43merrors\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4853\u001b[39m     indexer = axis.get_indexer(new_axis)\n\u001b[32m   4855\u001b[39m \u001b[38;5;66;03m# Case for non-unique axis\u001b[39;00m\n\u001b[32m   4856\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\Env2\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:7136\u001b[39m, in \u001b[36mIndex.drop\u001b[39m\u001b[34m(self, labels, errors)\u001b[39m\n\u001b[32m   7134\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m mask.any():\n\u001b[32m   7135\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m errors != \u001b[33m\"\u001b[39m\u001b[33mignore\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m-> \u001b[39m\u001b[32m7136\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlabels[mask].tolist()\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m not found in axis\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   7137\u001b[39m     indexer = indexer[~mask]\n\u001b[32m   7138\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.delete(indexer)\n",
      "\u001b[31mKeyError\u001b[39m: \"['target_column'] not found in axis\""
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebd3acc7-ae06-4f55-9d31-16a7bccc7fa2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de952f60-f05e-49eb-ab83-6958cc00e3ac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4940beb6-d291-4b92-9268-d92cc06e17b2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f67f9f5e-2e84-4645-a89d-a6de6de7bc5c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
